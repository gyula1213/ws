<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Build and manage Red Hat Device Edge images with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/09/build-and-manage-red-hat-device-edge-images-ansible" /><author><name>Ricardo Noriega De Soto, James Harmison</name></author><id>a25423b2-f6d4-4f7f-860d-083ea58a8c53</id><updated>2023-05-09T07:00:00Z</updated><published>2023-05-09T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/device-edge"&gt;Red Hat Device Edge&lt;/a&gt; is a new solution that delivers a lightweight enterprise-ready &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; distribution called MicroShift combined with an &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt;-optimized operating system built from &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL).&lt;/p&gt; &lt;p&gt;This article will guide you through the process of building your own customized Red Hat Device Edge images, from setting up the necessary building infrastructure to deploying the image on a device or virtual machine.&lt;/p&gt; &lt;p&gt;This tutorial will also show you a way to manage edge devices using supported tooling such as &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; in a pull-based model. This approach simplifies management and more accurately emulates real-life edge device connectivity.&lt;/p&gt; &lt;h2&gt;Building Red Hat Device Edge images&lt;/h2&gt; &lt;p&gt;Before you start to learn the process of creating and customizing Red Hat Device Edge images, we have pre-built one for you. Jump ahead to the section &lt;strong&gt;Test a pre-built Red Hat Device Edge image&lt;/strong&gt; so you can download the image and start testing. If you are interested in the process, keep reading.&lt;/p&gt; &lt;p&gt;The diagram in Figure 1 shows the architecture of what we are going to build in this tutorial.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhde_aap_blog_diagrams_-_imagebuilder.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhde_aap_blog_diagrams_-_imagebuilder.png?itok=BMOih1hB" width="600" height="339" alt="Image Builder architecture diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The required components to build and manage a Red Hat Device Edge image. You can build the image using image builder from a RHEL machine or use the pre-built image provided in this article. Once this image is flashed into your edge device, it is ready to be managed by Ansible and can be updated via an OSTree repository.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;First of all, we will need to set up our building machine. This is basically a Red Hat Enterprise Linux system where we are going to install a tool called &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/composing_a_customized_rhel_system_image/composer-description_composing-a-customized-rhel-system-image"&gt;image builder&lt;/a&gt;. Image builder allows you to build customized RHEL system images prepared for deployment on cloud platforms or on bare metal machines. Image builder automatically handles the setup details for each output type and therefore is easier to use and faster than other manual methods of image creation.&lt;/p&gt; &lt;h3&gt;Install image build tools&lt;/h3&gt; &lt;p&gt;Assuming there is already a RHEL 8 or RHEL 9 system with an active subscription you can work on (we recommend a disk size of 85 G to allow room for multiple images), let’s go ahead and install a set of packages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo dnf update -y sudo dnf install -y git osbuild-composer composer-cli cockpit-composer bash-completion lorax sudo systemctl enable --now osbuild-composer.socket &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This installs the required tools to build our customized images. Image builder works with a construct called &lt;strong&gt;sources&lt;/strong&gt;, which basically are RPM repositories where the tool is going to find extra packages not provided by the standard sources. MicroShift is shipped as part of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; repository, and it needs as a dependency, the Open vSwitch package shipped as part of the Fast Datapath repository.&lt;/p&gt; &lt;h3&gt;Add image builder sources&lt;/h3&gt; &lt;p&gt;In the next step, we will add these two repositories as image builder sources:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ARCH=$(uname -i) cat &lt;&lt; EOFRHOCP &gt; rhocp-source.toml check_gpg = true check_ssl = true id = "rhocp-4.12" name = "rhocp-4.12" rhsm = true system = false type = "yum-baseurl" url = "https://cdn.redhat.com/content/dist/layered/rhel8/$ARCH/rhocp/4.12/os" EOFRHOCP cat &lt;&lt; EOFFDP &gt; fdp-source.toml check_gpg = true check_ssl = true id = "fast-datapath" name = "fast-datapath" rhsm = true system = false type = "yum-baseurl" url = "https://cdn.redhat.com/content/dist/layered/rhel8/$ARCH/fast-datapath/os" EOFFDP composer-cli sources add rhocp-source.toml composer-cli sources add fdp-source.toml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After adding the sources, now you list them with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli sources list&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And see the following four sources:&lt;/p&gt; &lt;pre&gt; appstream baseos fast-datapath rhocp-4.12 &lt;/pre&gt; &lt;h3&gt;Create a blueprint for the Device Edge image&lt;/h3&gt; &lt;p&gt;Image builder has the concept of blueprint, a definition of the image to be built, where you can specify a set of packages, versions, and several customization options. &lt;a href="https://www.osbuild.org/guides/blueprint-reference/blueprint-reference.html"&gt;Refer to the upstream documentation.&lt;/a&gt; Now, we will create the blueprint for our Red Hat Device Edge image and push it to the image-builder service.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt; EOF &gt; microshift-container.toml name = "microshift-container" description = "" version = "0.0.1" distro = "rhel-8" modules = [] groups = [] # MicroShift and dependencies [[packages]] name = "microshift" version = "*" [[packages]] name = "openshift-clients" version = "*" [customizations] hostname = "edge" [customizations.services] enabled = ["microshift"] EOF composer-cli blueprints push microshift-container.toml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;depsolve&lt;/code&gt; sub-command will check if all packages are reachable from the build host and dependencies will be fulfilled, ensuring a better result during the build process. It will show a list of all packages included in the resulting image.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli blueprints depsolve microshift-container&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a rpm-ostree based RHEL image&lt;/h3&gt; &lt;p&gt;Red Hat Device Edge makes use of the benefits of certain technologies that are suitable for &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge computing&lt;/a&gt; scenarios. &lt;a href="https://coreos.github.io/rpm-ostree/"&gt;rpm-ostree&lt;/a&gt; is a technology that allows fully managed and reprovisionable operating system images with transactional upgrades and rollbacks. &lt;/p&gt; &lt;p&gt;Image builder provides a set of image types (such as qcow2, gce, ami, etc.) that includes rpm-ostree based images types, like edge-container, edge-commit, etc. If you want to find out more about these image types, read the &lt;a href="https://www.osbuild.org/guides/user-guide/building-an-image-from-cli.html"&gt;documentation&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;For this purpose, we are going to create an OSTree-based Red Hat Enterprise Linux 8.7 image with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli compose start-ostree microshift-container --ref rhel/8/$ARCH/edge edge-container&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the status of the build with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ID=$(composer-cli compose list | grep "RUNNING" | awk '{print $1}') watch composer-cli compose list &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once it’s finished you can cancel the &lt;code&gt;watch&lt;/code&gt; with Ctrl+C. Download the image with the following commands: &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli compose image $ID sudo chown $(id -u):$(id -g) $ID-container.tar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file is ready to be loaded into &lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman&lt;/a&gt; and run as a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt;. The container will expose the OSTree commit for image builder to pull and embed it into the installer image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman load -i $ID-container.tar IMAGE_ID=$(podman images | awk '/&lt;none&gt;/{print $3}') podman tag $IMAGE_ID localhost/microshift-container podman run -d --name=edge-container -p 8080:8080 localhost/microshift-container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following steps will create an empty blueprint that will basically pull the rpm-ostree commit from the container we have just created, and embed it into the installer image. The resulting device will not point at the exposed rpm-ostree, but the local copy for the sake of simplicity and demonstration. In following steps, we will show you how to configure the device to point at the endpoint where the rpm-ostree is going to look for updates.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF &gt; microshift-installer.toml name = "microshift-installer" description = "MicroShift Installer blueprint" version = "0.0.1" EOF composer-cli blueprints push microshift-installer.toml composer-cli blueprints depsolve microshift-installer composer-cli compose start-ostree microshift-installer edge-installer --ref rhel/8/$ARCH/edge --url http://localhost:8080/repo/ ID=$(composer-cli compose list | grep "RUNNING" | awk '{print $1}') watch -n 5 composer-cli compose list&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the image is ready, the status will show as &lt;code&gt;FINISHED&lt;/code&gt;. Cancel this &lt;code&gt;watch&lt;/code&gt; with Ctrl+C as well and download the resulting image with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli compose image $ID&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The installer ISO image has been created.&lt;/p&gt; &lt;h3&gt;Inject and configure the kickstart file&lt;/h3&gt; &lt;p&gt;In order to have a fully automated installation experience, we will inject a kickstart file that configures several aspects such as LVM partitioning and firewall rules. Read through the kickstart file to understand what it is doing and you will find a couple of sections you might want to uncomment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt; EOF &gt; kickstart-microshift.ks lang en_US.UTF-8 keyboard us timezone UTC text reboot # Configure network to use DHCP and activate on boot network --bootproto=dhcp --device=link --activate --onboot=on # Partition disk with a 1GB boot XFS partition and an LVM volume containing a 10GB+ system root # The remainder of the volume will be used by the CSI driver for storing data # # For example, a 20GB disk would be partitioned in the following way: # # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT # sda 8:0 0 20G 0 disk # ├─sda1 8:1 0 200M 0 part /boot/efi # ├─sda1 8:1 0 800M 0 part /boot # └─sda2 8:2 0 19G 0 part # └─rhel-root 253:0 0 10G 0 lvm /sysroot # zerombr clearpart --all --initlabel part /boot/efi --fstype=efi --size=200 part /boot --fstype=xfs --asprimary --size=800 part pv.01 --grow volgroup rhel pv.01 logvol / --vgname=rhel --fstype=xfs --size=10240 --name=root # Configure ostree ostreesetup --nogpg --osname=rhel --remote=edge --url=file:///run/install/repo/ostree/repo --ref=rhel/8/$ARCH/edge %post --log=/var/log/anaconda/post-install.log --erroronfail # Uncomment the following line and replace the rpm-ostree server name variable # in case you want to expose updates remotely at a certain URL # echo -e 'url=http://REPLACE_OSTREE_SERVER_NAME/repo/' &gt;&gt; /etc/ostree/remotes.d/edge.conf # The pull secret is mandatory for MicroShift builds on top of OpenShift, but not OKD # The /etc/crio/crio.conf.d/microshift.conf references the /etc/crio/openshift-pull-secret file cat &gt; /etc/crio/openshift-pull-secret &lt;&lt; EOFPS REPLACE_OCP_PULL_SECRET_CONTENTS EOFPS chmod 600 /etc/crio/openshift-pull-secret # Create a default redhat user, allowing it to run sudo commands without password useradd -m -d /home/redhat -p $(openssl passwd -6 redhat | sed 's/\$/\\$/g') -G wheel redhat echo -e 'redhat\tALL=(ALL)\tNOPASSWD: ALL' &gt;&gt; /etc/sudoers # Uncomment the following lines if you want to inject your ssh public key # mkdir -m 700 /home/redhat/.ssh # cat &gt; /home/redhat/.ssh/authorized_keys &lt;&lt; EOFK # REPLACE_REDHAT_AUTHORIZED_KEYS_CONTENTS # EOFK # chmod 600 /home/redhat/.ssh/authorized_keys # Make sure redhat user directory contents ownership is correct chown -R redhat:redhat /home/redhat/ # Configure the firewall (rules reload is not necessary here) firewall-offline-cmd --zone=trusted --add-source=10.42.0.0/16 firewall-offline-cmd --zone=trusted --add-source=169.254.169.1 # Make the KUBECONFIG from MicroShift directly available for the root user echo -e 'export KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig' &gt;&gt; /root/.profile %end EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This kickstart file performs a set of steps to configure the host during the installation in a way required by MicroShift.&lt;/p&gt; &lt;p&gt;The device will need to have the OpenShift pull secret from the user injected so container images can be downloaded from the Red Hat registry. Follow this &lt;a href="https://console.redhat.com/openshift/install/pull-secret"&gt;link&lt;/a&gt;, copy your pull secret, and paste it in the following variable:&lt;/p&gt; &lt;pre&gt; PULL_SECRET=’’ # Don’t forget the single quotes to keep all characters from the pull secret&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sed -i "s/REPLACE_OCP_PULL_SECRET_CONTENTS/$PULL_SECRET/g" kickstart-microshift.ks sudo mkksiso kickstart-microshift.ks $ID-installer.iso redhat-device-edge-installer-$ARCH.iso&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deploy the image&lt;/h3&gt; &lt;p&gt;Now you are ready to deploy the ISO you have customized into your device (or virtual machine). It should be located in &lt;code&gt;redhat-device-edge-installer-x86_64.iso&lt;/code&gt; in your working directory on the RHEL machine you've been working on. Recover it if necessary and flash it to a USB drive using &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/sect-making-usb-media#sect-making-usb-media-linux"&gt;this documentation&lt;/a&gt; or your own preferred method. The installation process is completely automated, and whenever the process is finished, you can log into your device using &lt;code&gt;redhat:redhat&lt;/code&gt; credentials. &lt;/p&gt; &lt;p&gt;MicroShift should be up and running, and the OpenShift client is ready to use. You can copy the kubeconfig file into the default location of your user:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir ~/.kube sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig &gt; ~/.kube/config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or log in as root (&lt;code&gt;sudo -i&lt;/code&gt;) and it will be ready for you.&lt;/p&gt; &lt;pre&gt; $ oc get pods -A  NAMESPACE NAME READY STATUS RESTARTS AGE openshift-dns dns-default-87gk8 2/2 Running 0 111s openshift-dns node-resolver-tdpdc 1/1 Running 0 111s openshift-ingress router-default-54765bcdf7-qbvz7 1/1 Running 0 106s openshift-ovn-kubernetes ovnkube-master-l6xgk 4/4 Running 0 111s openshift-ovn-kubernetes ovnkube-node-gdwhj 1/1 Running 0 111s openshift-service-ca service-ca-79dbd484cf-kr549 1/1 Running 0 106s openshift-storage topolvm-controller-5c9ccfcf45-jzkdd 4/4 Running 0 112s openshift-storage topolvm-node-j8zbj 4/4 Running 0 112s &lt;/pre&gt; &lt;h2&gt;Test a pre-built Red Hat Device Edge image&lt;/h2&gt; &lt;p&gt;If you want to test a Red Hat Device Image right away, we've provided &lt;a href="https://developers.redhat.com/content-gateway/file/v1/Red-Hat-device-edge-x86-installer.iso"&gt;a pre-built image for you to download here&lt;/a&gt;. This image contains all the necessary components, except an OpenShift pull secret that you must insert once the device is up and running. Log into your device using &lt;code&gt;redhat:redhat&lt;/code&gt; credentials. &lt;/p&gt; &lt;p&gt;Copy your pull secret to the following location:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;/etc/crio/openshift-pull-secret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, enable and start MicroShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo systemctl enable --now microshift.service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your device is now ready to be managed!&lt;/p&gt; &lt;h2&gt;Managing Red Hat Device Edge&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is an &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; platform beloved by administrators in the data center and cloud for its simplicity (to learn and to use), flexibility, and advanced capabilities. Ansible Automation Platform is capable of managing any endpoint with a remote management interface, and includes out-of-the-box support for anything that can be managed over SSH, WinRM, and many API-driven environments (such as cloud providers and indeed, the Kubernetes API).&lt;/p&gt; &lt;p&gt;Ansible Automation Platform is designed to support a push-based, agentless management model. This flexibility means that managing new devices is as simple as updating a centralized inventory file with connection and authentication information, which is great for IT administrators looking to manage a large fleet of devices. The agentless architecture also gives us the flexibility to manage devices with a remote management interface that lack the capacity to install an agent, such as the common "appliance" form-factor devices in enterprise IT environments.&lt;/p&gt; &lt;h3&gt;Managing workloads on resource-constrained edge devices&lt;/h3&gt; &lt;p&gt;Red Hat Device Edge, being designed for very small form factor devices, finds its niche in the field rather than the data center or cloud. The OSTree deployment brings some nice features for that data center environment, but the features clearly target the edge—and the minimal-footprint Kubernetes at the heart of the Red Hat build of MicroShift clearly targets constrained resource environments who sacrifice features for size, weight, and power (SWAP).&lt;/p&gt; &lt;p&gt;This reality means that in some cases, we won’t control the network environment our Red Hat Device Edge nodes work in. Sometimes they’ll be on a constrained network hosted by a mobile provider; sometimes they’ll be mobile and connecting to WiFi networks opportunistically; sometimes they'll simply have very intermittent network connectivity and a push-based model might imply that a device is unhealthy when it’s simply not reachable.&lt;/p&gt; &lt;h3&gt;A more flexible approach&lt;/h3&gt; &lt;p&gt;Ansible does have a viable solution for these kinds of devices called &lt;a href="https://docs.ansible.com/ansible/latest/cli/ansible-pull.html"&gt;ansible-pull&lt;/a&gt;, but ansible-pull is only designed for situations where we want a device to manage only itself—and it strictly controls how Ansible inventories may be applied to the node running the command. Because MicroShift’s Kubernetes API is not the same management interface as Red Hat Device Edge’s SSH-based remote access at the OS layer, we might find ourselves in a situation where an application needs host configuration alongside the application deployment. We want a system that works like ansible-pull (rather than the traditional push model of Ansible Automation Platform), but gives us more flexibility.&lt;/p&gt; &lt;p&gt;Luckily, there are supported components of Ansible Automation Platform that make this kind of management model possible. It should be noted that this deployment architecture is not currently explicitly supported by Red Hat in production, but problems with the supported pieces we’re using should be able to be reproduced in a fully supported environment. If you’re a Red Hat customer interested in solving for these constraints then you should work with your account team at Red Hat to express interest.&lt;/p&gt; &lt;h3&gt;Architecture diagram&lt;/h3&gt; &lt;p&gt;The diagram in Figure 2 shows all the components we are going to use to manage edge devices with Ansible Automation Platform.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhde_aap_blog_diagrams_-_ansible.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhde_aap_blog_diagrams_-_ansible.png?itok=VIRfRf_8" width="600" height="254" alt="AAP for edge architecture diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: How Ansible Automation Platform manages the Red Hat Device Edge life cycle. A Git repository represents the source of truth for application and operating system configurations in code. Ansible will pull that content and operate on to the device to get to the desired state. Ansible can run as a Kubernetes CronJob or as a systemd service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure the example repository&lt;/h3&gt; &lt;p&gt;With all of that stage-setting out of the way, let’s get started. Because we’re driving management of our Red Hat Device Edge node in a &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;-based pull model, we need a repository to declare our configurations in.&lt;/p&gt; &lt;p&gt;It’s simplest to start from a working example, so head to &lt;a href="https://github.com/redhat-na-ssa/microshift-ansible-pull"&gt;https://github.com/redhat-na-ssa/microshift-ansible-pull&lt;/a&gt; and click the &lt;strong&gt;Fork&lt;/strong&gt; button in the top right to fork the repository into your own account or organization; then click &lt;strong&gt;Create fork&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;When GitHub finishes making your copy, clone the fork down to the RHEL 8 instance you used as the image builder in the preceding section using whatever method you prefer (GitHub CLI, SSH, HTTPS, your own IDE) and change into the repository root. If you have another RHEL device with valid entitlements and network access to your Red Hat Device Edge node, then it’s safe to run from that as well, but you won’t be able to run these commands from your Device Edge node.&lt;/p&gt; &lt;p&gt;This monorepo is designed to help you manage a single application deployed to a single MicroShift instance right now, but it should be extensible enough that once you get the idea of the workflow contained within you can extend it to fit your edge management use case. Let’s get to using the components included out of the box. Ensure that you have enabled the repositories for supported Ansible Automation Platform components:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo subscription-manager repos --enable ansible-automation-platform-2.3-for-rhel-$(rpm -qi redhat-release | awk '/Version/{print $3}' | cut -d. -f1)-x86_64-rpms&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install the packages we’ll need:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo dnf -y install ansible-navigator openshift-clients&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Recover your kubeconfig from the MicroShift node. First, identify the IP address of the node from its own console, your DHCP server’s status page, or some other means—and set that IP address to a variable for easy consumption in later steps.&lt;/p&gt; &lt;pre&gt; RHDE_NODE=192.168.1.213 # MAKE SURE YOU SET THIS TO THE ACTUAL IP &lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir -p ~/.kube ssh redhat@$RHDE_NODE sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig &gt; ~/.kube/config sed -i 's/127\.0\.0\.1/'"$RHDE_NODE/" ~/.kube/config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In order to be able to reach the MicroShift API, we’re going to temporarily open the firewall on the Red Hat Device Edge node to port 6443. You should understand, for production systems, which management method is best for your edge endpoints. You can enable SSH and enforce strict security, or you can drop packets to SSH and open the path to the API endpoint (which doesn’t have any authentication/users, simply the built-in administrator who is identified by an X509 certificate), or you can choose to disable both once we get the pull-based management bootstrapped.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ssh redhat@$RHDE_NODE sudo firewall-cmd --add-port=6443/tcp oc get nodes # to verify that you have connectivity to the endpoint &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ensure that your terminal is in the repository root that you cloned down:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;ls &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;ansible.cfg  ansible-navigator.yml  app  deploy  inventory  LICENSE  playbooks  README.md&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Edit the inventory to point to your Red Hat Device Edge node:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sed -i 's/10\.1\.1\.11/'"$RHDE_NODE/" inventory/hosts &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ensure that you can locally pull the supported execution environment using the &lt;code&gt;PULL_SECRET&lt;/code&gt; you saved earlier by putting it in one of the paths that Podman would find it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir -p ~/.docker echo "$PULL_SECRET" &gt; ~/.docker/config.json &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Make sure that the &lt;a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html"&gt;Ansible Vault&lt;/a&gt; password for your secrets is saved in the location the repository expects it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;vault_pass="definitely a secure password" # This is actually the password encrypting the vault in the repository you forked mkdir -p /tmp/secrets echo "$vault_pass" &gt; /tmp/secrets/.vault &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you changed the password for the &lt;strong&gt;redhat&lt;/strong&gt; user on the Red Hat Device Edge node from what was included in the kickstart, you’ll need to edit that. You might also want to change the vault password for the symmetrically encrypted Ansible Vault in the repository. You can do those with the following (optional) steps:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-vault () { podman run --rm -it --entrypoint ansible-vault -v ./:/repo -v /tmp/secrets/.vault:/tmp/secrets/.vault -e ANSIBLE_VAULT_PASSWORD_FILE=/tmp/secrets/.vault --security-opt=label=disable --privileged --workdir /repo registry.redhat.io/ansible-automation-platform-23/ee-supported-rhel8:1.0.0 "${@}" ; } ansible-vault decrypt inventory/group_vars/node/vault.yml ${EDITOR:-nano} inventory/group_vars/node/vault.yml # and update the password vault_pass="a totally new password" echo "$vault_pass" &gt; /tmp/secrets/.vault ansible-vault encrypt inventory/group_vars/node/vault.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our last little bit of configuration will be to point management to our own fork of the repo, instead of the default. It’s simplest right now to stick with HTTPS repository access for the Red Hat Device Edge node, but you could manage SSH deploy keys as secrets and mount them in place to access a repository over SSH, including a private repository, if it makes sense. Edit the configuration that defines where to look for updates:&lt;/p&gt; &lt;pre&gt; MY_GITHUB_ORG=redhat-fan-42 # set this to your actual GitHub org or user who forked the repository&lt;/pre&gt; &lt;pre&gt; &lt;code&gt;sed -i "s/redhat-na-ssa/$MY_GITHUB_ORG/" deploy/cronjob.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let’s push our changes to the repository up to GitHub:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git status -u # make sure the changed files include only your vault (if applicable), inventory hosts, and the CronJob git diff # make sure the edits are what you expect (vault re-encrypted, hosts pointing to your IP, and the CronJob pointing to your fork git add . git commit -m 'Update for my environment' git push &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To ensure that we can push the bootstrapping configuration to the Red Hat Device Edge node, run the following command to use an Ansible “&lt;a href="https://docs.ansible.com/ansible/latest/collections/ansible/builtin/ping_module.html"&gt;ping&lt;/a&gt;” (which is much more than a simple ICMP ping):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-navigator exec --eev /tmp/secrets:/tmp/secrets:Z -- ansible microshift_node -m ping&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; microshift_node | SUCCESS =&gt; { "ansible_facts": { "discovered_interpreter_python": "/usr/libexec/platform-python" }, "changed": false, "ping": "pong" } &lt;/pre&gt; &lt;p&gt;With that, we’re ready to kick off our Red Hat Device Edge configuration bootstrapping!&lt;/p&gt; &lt;p&gt;The Ansible Automation Platform configuration could be embedded into the ISO or by using a provisioning system of your choice as well, instead of doing these manual steps. &lt;/p&gt; &lt;h3&gt;Bootstrap the MicroShift configuration&lt;/h3&gt; &lt;p&gt;Load the Ansible Vault secret manually into the MicroShift API:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create namespace microshift-config oc create secret generic --from-literal=.vault="$vault_pass" vault-key -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And run the playbooks locally, in push mode, using Ansible Navigator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;for playbook in playbooks/*.yml; do ansible-navigator run $playbook -l microshift,microshift_node --eev ~/.kube/config:/home/runner/.kube/config:Z --eev /tmp/secrets/.vault:/tmp/secrets/.vault:Z; done&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Explore the &lt;code&gt;microshift-config&lt;/code&gt; namespace to see what we’ve bootstrapped into there:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get cm -n microshift-config&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME DATA AGE kube-root-ca.crt 1 2m22s microshift-ansible-pull 1 67s microshift-config-env 5 67s openshift-service-ca.crt 1 2m22s&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get cronjob -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE microshift-config 15 * * * * False 0 76s&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pvc -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE microshift-config-data Pending topolvm-provisioner 83s runner-home Pending topolvm-provisioner 83s&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pod -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; No resources found in microshift-config namespace&lt;/pre&gt; &lt;p&gt;Some ConfigMaps, a CronJob (that hasn’t run yet), and two PVCs that will bind when the CronJob does finally run. Feel free to explore those ConfigMaps to better understand them, or dig into the README in the repository you cloned for a detailed explanation of what each of the pieces of this repository does.&lt;/p&gt; &lt;p&gt;Let’s look at the playbooks, now that everything’s set up:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat playbooks/cluster.yml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; --- - hosts: cluster tasks: - name: Create the namespace redhat.openshift.k8s: state: present api_version: v1 kind: Namespace name: hello - name: Deploy the application redhat.openshift.k8s: state: present namespace: hello src: '{{ playbook_dir }}/../app/{{ item }}' loop: - 00-deployment.yml - 05-svc.yml - 10-route.yml - name: Update the CronJob for the next run redhat.openshift.k8s: state: present src: '{{ playbook_dir }}/../deploy/cronjob.yml'&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat playbooks/node.yml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; --- - hosts: node become: true tasks: - name: Open firewall ports for the OpenShift Router ansible.posix.firewalld: state: enabled service: '{{ item }}' permanent: true immediate: true loop: - http - https&lt;/pre&gt; &lt;p&gt;These are pretty straightforward and even if you’ve never written a line of Ansible in your life, they should make sense pretty quickly. We’re going to target our MicroShift cluster with a playbook that configures a namespace and deploys an application from some manifests, and update our CronJob definition. Then we’re going to configure the firewall on the node to expose HTTP and HTTPS ports for the OpenShift Router to expose our web-based application.&lt;/p&gt; &lt;p&gt;Because we used ansible-navigator in a more traditional push management model, our changes are reflected right away. We have a CronJob that reaches out to Git for updates before applying the playbooks, so it can be reconciled without any external connectivity to its SSH or Kubernetes API ports.&lt;/p&gt; &lt;p&gt;It is important to highlight that the Ansible Automation Platform execution environment can be executed by a systemd service and a timer using Podman rather than a CronJob, if that's appropriate for your use case.&lt;/p&gt; &lt;p&gt;Let’s look at the route that was created. From your machine with the repository cloned (and the kubeconfig set up), run the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat app/10-route.yml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-world spec: port: targetPort: 8000 to: kind: Service name: hello-world weight: 100 wildcardPolicy: None tls: insecureEdgeTerminationPolicy: Redirect termination: edge &lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get route -n hello hello-world &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME HOST ADMITTED SERVICE TLS hello-world hello-world-hello.apps.example.com True hello-world &lt;/pre&gt; &lt;p&gt;Our MicroShift default configuration set the cluster’s router to be at &lt;code&gt;apps.example.com&lt;/code&gt;, so our route has a fake DNS name. Let’s add that to &lt;code&gt;/etc/hosts&lt;/code&gt; so we don’t have to update the cluster base domain or set up DNS on the network:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;HELLO_ROUTE=$(oc get route -n hello hello-world -ojsonpath='{.status.ingress[0].host}') echo "$RHDE_NODE $HELLO_ROUTE" | sudo tee -a /etc/hosts 10.1.1.11 hello-world-hello.apps.example.com &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then let’s curl that Route, accepting the default self-signed TLS certificate:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -k https://hello-world-hello.apps.example.com&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; Hello, world, from hello-world-b8747f4d7-qc7v8!&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; oc get pods -n hello&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME READY STATUS RESTARTS AGE hello-world-b8747f4d7-j4bnd 1/1 Running 0 11h hello-world-b8747f4d7-qc7v8 1/1 Running 0 11h hello-world-b8747f4d7-qrpx6 1/1 Running 0 11h &lt;/pre&gt; &lt;p&gt;In my example here, it was the second replica that ended up answering the request. Let’s try some &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; management, changing something pretty straightforward about our application. Set the number of replicas to &lt;code&gt;1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sed -i 's/replicas: 3/replicas: 1/' app/00-deployment.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Commit the changes and push to your remote repository without running ansible-navigator imperatively:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;git status -u git diff git add . git commit -m 'Changed to single replica' git push&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The output of the previous commands should look like this:&lt;/p&gt; &lt;pre&gt; $ git status -u On branch main Your branch is up to date with 'origin/main'. Changes not staged for commit: (use "git add ..." to update what will be committed) (use "git restore ..." to discard changes in working directory) modified: app/00-deployment.yml no changes added to commit (use "git add" and/or "git commit -a") $ git diff diff --git a/app/00-deployment.yml b/app/00-deployment.yml index 2564981..3568124 100644 --- a/app/00-deployment.yml +++ b/app/00-deployment.yml @@ -6,7 +6,7 @@ metadata: labels: app: hello-world spec: - replicas: 3 + replicas: 1 selector: matchLabels: app: hello-world $ git add . $ git commit -m 'Changed to single replica' [main 4e96db7] Changed to single replica 1 file changed, 1 insertion(+), 1 deletion(-) $ git push Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 32 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 810 bytes | 810.00 KiB/s, done. Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:solacelost/microshift-ansible-pull.git 9c271fd..4e96db7 main -&gt; main&lt;/pre&gt; &lt;p&gt;Trigger a manual CronJob run, without waiting for the timer to fire off, with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;MANUAL_TIME=$(date +%s) oc create job --from=cronjob/microshift-config -n microshift-config microshift-config-manual-$MANUAL_TIME oc get pods -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At the time of this writing, you might be required to disable CSI Storage Capacity tracking to have the pod successfully create and bind the volumes. (Remember, MicroShift is still in Developer Preview!) To do that easily, you can run the following if your pod fails to schedule:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc patch csidriver topolvm.io -p '{"spec":{"storageCapacity":false}}' oc get pods -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once your manual Job invocation has finished, you can check the logs to see what’s happened:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc logs -n microshift-config job/microshift-config-manual-$MANUAL_TIME &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; [...] PLAY RECAP ********************************************************************* microshift : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [...] PLAY RECAP ********************************************************************* microshift_node : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/pre&gt; &lt;p&gt;Note that on the cluster playbook, one thing changed. Nothing changed on the node. Let’s double-check our application now:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods -n hello &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME READY STATUS RESTARTS AGE hello-world-b8747f4d7-qc7v8 1/1 Running 1 12h&lt;/pre&gt; &lt;pre&gt; &lt;code&gt;curl -k https://hello-world-hello.apps.example.com&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; Hello, world, from hello-world-b8747f4d7-qc7v8! &lt;/pre&gt; &lt;p&gt;And, just like that, we’ve enabled asynchronous, GitOps-based replication of our Red Hat Device Edge node, including the MicroShift API layer.&lt;/p&gt; &lt;p&gt;Now we’re ready to begin adding more nodes to our inventory and expanding the method we use to apply our CronJob in the playbook to support this flexibility. Let’s look at the existing inventory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat inventory/hosts &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; [cluster] microshift ansible_connection=local ansible_python_interpreter=python3 some_other_microshift ansible_connection=local ansible_python_interpreter=python3 [node] microshift_node ansible_host=10.1.1.11 ansible_connection=ssh ansible_user=redhat some_other_microshift_node ansible_host=86.75.30.9 ansible_connection=ssh ansible_user=redhat&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;And seeing that we’re managing &lt;code&gt;some_other_microshift&lt;/code&gt; in this repository, despite not having it available to us, we should probably consider how we’d apply the same methodology to another edge cluster. Check out the certified collection documentation for the &lt;a href="https://console.redhat.com/ansible/automation-hub/repo/published/redhat/openshift/content/module/k8s/"&gt;redhat.openshift.k8s&lt;/a&gt; module, and consider what we might do with the &lt;a href="https://docs.ansible.com/ansible/latest/collections/ansible/builtin/file_lookup.html"&gt;ansible.builtin.file&lt;/a&gt; lookup plug-in combined with the &lt;a href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#searching-strings-with-regular-expressions"&gt;regex_replace&lt;/a&gt; filter, alongside the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/special_variables.html"&gt;inventory_hostname&lt;/a&gt; special variable, available as part of our bootstrapping. A declarative inventory of many edge nodes running the same, or similar, applications are just a few more commits away with this framework.&lt;/p&gt; &lt;h2&gt;The bottom line&lt;/h2&gt; &lt;p&gt;Ansible Automation Platform is a very capable framework, able to meet your needs for robust and flexible management of endpoints with or without Ansible Controller. The depths of what you can accomplish, declaratively, with Ansible Automation Platform are limited only by your imagination—and, of course, the capabilities and reliability of the collection content at your disposal. If you need more robust workload management across your edge fleet, you could parameterize the shell script a bit further to prefer a branch named after the system UUID, but fall back to main, if desired.&lt;/p&gt; &lt;p&gt;Combining the power of Ansible Automation Platform with the full Kubernetes API available in MicroShift means that we have a wide array of tooling at our disposal for log aggregation, metrics, extensions via Kubernetes Operators, and more—in a fully edge-ready architecture.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/09/build-and-manage-red-hat-device-edge-images-ansible" title="Build and manage Red Hat Device Edge images with Ansible"&gt;Build and manage Red Hat Device Edge images with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ricardo Noriega De Soto, James Harmison</dc:creator><dc:date>2023-05-09T07:00:00Z</dc:date></entry><entry><title type="html">MEF and Sofis use Quarkus as core component of a new innovative architecture</title><link rel="alternate" href="https://quarkus.io/blog/ministry-of-economy-finance-uruguay-adopts-quarkus/" /><author><name>Fabricio Gregorio</name></author><id>https://quarkus.io/blog/ministry-of-economy-finance-uruguay-adopts-quarkus/</id><updated>2023-05-09T00:00:00Z</updated><content type="html">About us The Ministry of Economy and Finance (MEF) of Uruguay is a government ministry. Website: https://www.gub.uy/ministerio-economia-finanzas Sofis Solutions is a company with more than 18 years of experience in the digital transformation of Latin American organizations and technological inclusion. Focuses mainly on projects for the development of digital government...</content><dc:creator>Fabricio Gregorio</dc:creator></entry><entry><title>How to create execution environments using ansible-builder</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/08/how-create-execution-environments-using-ansible-builder" /><author><name>Tathagata Paul</name></author><id>7453d38e-90f7-4d30-9fba-9db9e244053d</id><updated>2023-05-08T07:00:00Z</updated><published>2023-05-08T07:00:00Z</published><summary type="html">&lt;p&gt;The execution environment builder (aka Ansible Builder) is a part of &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. It is a command-line interface (CLI) tool for building and creating custom execution environments. The Ansible Builder project enables users to automate and accelerate the process of creating execution environments. This article will show you how to install and use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.1/html/ansible_builder_guide/index"&gt;execution environment builder&lt;/a&gt; CLI tool.&lt;/p&gt; &lt;h2&gt;Installing the execution environment builder&lt;/h2&gt; &lt;p&gt;The execution environment builder makes it easier for Ansible Automation Platform content creators and administrators to build custom execution environments. They can use dependency information from various &lt;a href="http://ansible.com/products/content-collections"&gt;Ansible Content Collections&lt;/a&gt; and directly from the user.&lt;/p&gt; &lt;h3&gt;Step 1: Install the execution environment builder tool&lt;/h3&gt; &lt;p&gt;Install the execution environment builder tool from the Python Package Index (PyPI) by using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pip install ansible-builder&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 2: Access the ansible-builder subcommands&lt;/h3&gt; &lt;p&gt;To access the subcommands of &lt;code&gt;ansible-builder&lt;/code&gt;, run &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;create&lt;/code&gt; commands to get help output.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;build&lt;/code&gt; subcommand will build the execution environment using the definition file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build –help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It populates the build context and then uses Podman or Docker to create the execution environment image. The help output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;usage: ansible-builder build [-h] [-t TAG] [--container-runtime {podman,docker}] [--build-arg BUILD_ARGS] [-f FILENAME] [-c BUILD_CONTEXT] [--output-filename {Containerfile,Dockerfile}] [-v {0,1,2,3}] Creates a build context (including a Containerfile) from an execution environment spec. The build context will be populated from the execution environment spec. After that, the specified container runtime podman/docker will be invoked to build an image from that definition. After building the image, it can be used locally or published using the supplied tag. optional arguments: -h, --help show this help message and exit -t TAG, --tag TAG The name for the container image being built (default: ansible-execution-env:latest) --container-runtime {podman,docker} Specifies which container runtime to use (default: podman) --build-arg BUILD_ARGS Build-time variables to pass to any podman or docker calls. Internally ansible-builder makes use of ANSIBLE_GALAXY_CLI_COLLECTION_OPTS, EE_BASE_IMAGE, EE_BUILDER_IMAGE. -f FILENAME, --file FILENAME The definition of the execution environment (default: execution-environment.yml) -c BUILD_CONTEXT, --context BUILD_CONTEXT The directory to use for the build context (default: context) --output-filename {Containerfile,Dockerfile} Name of file to write image definition to (default depends on --container-runtime, Containerfile for podman and Dockerfile for docker) -v {0,1,2,3}, --verbosity {0,1,2,3} Increase the output verbosity, for up to three levels of verbosity (invoked via "--verbosity" or "-v" followed by an integer ranging in value from 0 to 3) (default: 2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;create&lt;/code&gt; subcommand works similar to the &lt;code&gt;build&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder create –help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, it will not build the execution environment image as you will see in the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;usage: ansible-builder build [-h] [-t TAG] [--container-runtime {podman,docker}] [--build-arg BUILD_ARGS] [-f FILENAME] [-c BUILD_CONTEXT] [--output-filename {Containerfile,Dockerfile}] [-v {0,1,2,3}] Creates a build context (including a Containerfile) from an execution environment spec. The build context will be populated from the execution environment spec. After that, the specified container runtime podman/docker will be invoked to build an image from that definition. After building the image, it can be used locally or published using the supplied tag. optional arguments: -h, --help show this help message and exit -t TAG, --tag TAG The name for the container image being built (default: ansible-execution-env:latest) --container-runtime {podman,docker} Specifies which container runtime to use (default: podman) --build-arg BUILD_ARGS Build-time variables to pass to any podman or docker calls. Internally ansible-builder makes use of ANSIBLE_GALAXY_CLI_COLLECTION_OPTS, EE_BASE_IMAGE, EE_BUILDER_IMAGE. -f FILENAME, --file FILENAME The definition of the execution environment (default: execution-environment.yml) -c BUILD_CONTEXT, --context BUILD_CONTEXT The directory to use for the build context (default: context) --output-filename {Containerfile,Dockerfile} Name of file to write image definition to (default depends on --container-runtime, Containerfile for podman and Dockerfile for docker) -v {0,1,2,3}, --verbosity {0,1,2,3} Increase the output verbosity, for up to three levels of verbosity (invoked via "--verbosity" or "-v" followed by an integer ranging in value from 0 to 3) (default: 2) &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3: Populate the ansible-builder spec&lt;/h3&gt; &lt;p&gt;Populate the &lt;code&gt;ansible-builder&lt;/code&gt; spec to build the custom execution environment by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir project_directory &amp;&amp; cd project_directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Populate the &lt;code&gt;execution-environment.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; execution-environment.yml --- version: 1 dependencies: galaxy: requirements.yml EOT &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a &lt;code&gt;requirements.yml&lt;/code&gt; file and populate the contents with the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.yml --- collections: - name: servicenow.itsm EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Through the spec and requirements file, we ensure that execution environment builder will download the &lt;strong&gt;servicenow.itsm collection&lt;/strong&gt; while building the execution environment. The default download location is &lt;strong&gt;galaxy.ansible.com&lt;/strong&gt;. You can also point to an automation hub or your own hub instance in the spec file.&lt;/p&gt; &lt;h3&gt;Step 4: Build the execution environment&lt;/h3&gt; &lt;p&gt;Build the execution environment using the previously created files. Run the following command to create a new custom execution environment called &lt;strong&gt;custom-ee:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build -v3 -t custom-ee&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-v3&lt;/code&gt; flag adds verbosity to the CLI run, and &lt;code&gt;-t custom-ee&lt;/code&gt; will tag your image with the name you provided.&lt;/p&gt; &lt;p&gt;The output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Ansible Builder is building your execution environment image, "custom-ee". File context/_build/requirements.yml will be created. Rewriting Containerfile to capture collection requirements Running command: podman build -f context/Containerfile -t custom-ee context [1/3] STEP 1/7: FROM registry.redhat.io/ansible-automation-platform-21/ee-minimal-rhel8:latest AS galaxy [1/3] STEP 2/7: ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= --&gt; 88d9ea223d0 [1/3] STEP 3/7: USER root --&gt; 549f29055c2 [1/3] STEP 4/7: ADD _build /build --&gt; 0d3e9515b12 [1/3] STEP 5/7: WORKDIR /build --&gt; 3b290acf78c [1/3] STEP 6/7: RUN ansible-galaxy role install -r requirements.yml --roles-path /usr/share/ansible/roles Skipping install, no requirements found --&gt; 8af36370e78 [1/3] STEP 7/7: RUN ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path /usr/share/ansible/collections Starting galaxy collection install process Process install dependency map … &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following commands to check the image list:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman images&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;REPOSITORY TAG IMAGE ID CREATED SIZE localhost/custom-ee latest bfe6c40bad52 21 seconds ago 626 MB &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 5: Build a complex execution environment&lt;/h3&gt; &lt;p&gt;To build a complex execution environment, go back into the project directory with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd project_directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit the &lt;code&gt;execution-environment.yml&lt;/code&gt; file and add the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; execution-environment.yml --- version: 1 dependencies: galaxy: requirements.yml python: requirements.txt system: bindep.txt additional_build_steps: prepend: | RUN whoami RUN cat /etc/os-release append: - RUN echo This is a post-install command! - RUN ls -la /etc EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Python requirements were added through the &lt;strong&gt;requirements.txt&lt;/strong&gt; file, which will hold the pip dependencies.&lt;/li&gt; &lt;li&gt;We added a &lt;strong&gt;bindep.txt&lt;/strong&gt;, which will hold the rpm installs.&lt;/li&gt; &lt;li&gt;Additional build steps that will run before (prepend) and after (append) the build steps.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Now create a new file called &lt;code&gt;requirements.yml&lt;/code&gt; and append the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.yml --- collections: - name: servicenow.itsm - name: ansible.utils EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We added a new collection called &lt;strong&gt;ansible.utils&lt;/strong&gt; alongside the &lt;strong&gt;servicenow.itsm&lt;/strong&gt; file.&lt;/p&gt; &lt;p&gt;Create a new file called &lt;code&gt;requirements.txt&lt;/code&gt; and then append the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.txt gcp-cli ncclient netaddr paramiko EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This contains the &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; requirements that need to be installed via pip.&lt;/p&gt; &lt;p&gt;Create a new file called &lt;code&gt;bindep.txt&lt;/code&gt; and then append the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; bindep.txt findutils [compile platform:centos-8 platform:rhel-8] gcc [compile platform:centos-8 platform:rhel-8] make [compile platform:centos-8 platform:rhel-8] python38-devel [compile platform:centos-8 platform:rhel-8] python38-cffi [platform:centos-8 platform:rhel-8] python38-cryptography [platform:centos-8 platform:rhel-8] python38-pycparser [platform:centos-8 platform:rhel-8] EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file contains the rpm requirements needed to be installed using dnf.&lt;/p&gt; &lt;p&gt;Run the following build:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build -v3 -t custom-ee&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Ansible Builder is building your execution environment image, "custom-ee". File context/_build/requirements.yml will be created. File context/_build/requirements.txt will be created. File context/_build/bindep.txt will be created. Rewriting Containerfile to capture collection requirements Running command: podman build -f context/Containerfile -t custom-ee context [1/3] STEP 1/7: FROM registry.redhat.io/ansible-automation-platform-21/ee-minimal-rhel8:latest AS galaxy [1/3] STEP 2/7: ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= --&gt; Using cache 88d9ea223d01bec0d53eb7efcf0e76b5f7da0285a411f2ce0116fe9641cbc3a0 --&gt; 88d9ea223d0 [1/3] STEP 3/7: USER root --&gt; Using cache 549f29055c2f1ba0ef3f7c5dfdc67a40302ff0330af927adb94fbcd7b0b1e7b4 --&gt; 549f29055c2 [1/3] STEP 4/7: ADD _build /build --&gt; 6b9ee91e773 [1/3] STEP 5/7: WORKDIR /build --&gt; 5518e019f2d [1/3] STEP 6/7: RUN ansible-galaxy role install -r requirements.yml --roles-path /usr/share/ansible/roles Skipping install, no requirements found --&gt; 60c1605d66c [1/3] STEP 7/7: RUN ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path /usr/share/ansible/collections Starting galaxy collection install process Process install dependency map&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the context or &lt;code&gt;Containerfile&lt;/code&gt; to see all the steps you took to build the execution environment. You can transfer the context directory to a different server and replicate the image creation via &lt;code&gt;docker&lt;/code&gt; or &lt;code&gt;podman&lt;/code&gt; commands.&lt;/p&gt; &lt;h2&gt;Pushing the execution environment to a private automation hub&lt;/h2&gt; &lt;p&gt;Log in to the &lt;a href="https://www.ansible.com/blog/control-your-content-with-private-automation-hub"&gt;private automation hub&lt;/a&gt; by using the &lt;code&gt;podman&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman login &lt;automation hub url&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then tag the image before pushing it to the hub as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman tag localhost/custom-ee &lt;automation hub url&gt;/developers-bu-aap-builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, push it to the private automation hub as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman push &lt;automation hub url&gt;/developers-bu-aap-builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see the image pushed to the private automation hub in Figure 1:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image_1.png?itok=AxLHdHot" width="1440" height="816" alt="The private automation hub page showing multiple pushed execution environment images." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The private automation hub page showing multiple pushed execution environment images.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your automation journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started with Ansible Automation Platform&lt;/a&gt; by exploring &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;interactive labs&lt;/a&gt;. Check out Red Hat’s hands-on labs for all skill levels to learn more. The wide range of labs include &lt;a href="https://developers.redhat.com/learn/lessons/linux-commands?intcmp=7013a0000026UTXAA2"&gt;useful Linux commands&lt;/a&gt;, &lt;a href="https://developers.redhat.com/learn/installing-software-using-package-managers?intcmp=7013a0000026UTXAA2"&gt;Install software using package managers&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/learn/lessons/deploying-containers-podman?intcmp=7013a0000026UTXAA2"&gt;Deploying containers using container tools [podman]&lt;/a&gt;. Try these labs to see your favorite products in action. Ansible Automation Platform is also available as a managed offering on&lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt; Microsoft Azure&lt;/a&gt; and as a self-managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws"&gt;AWS&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/08/how-create-execution-environments-using-ansible-builder" title="How to create execution environments using ansible-builder"&gt;How to create execution environments using ansible-builder&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tathagata Paul</dc:creator><dc:date>2023-05-08T07:00:00Z</dc:date></entry><entry><title type="html">How to run Spring Boot applications on WildFly</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/spring/spring-boot-hello-world-on-wildfly/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/spring/spring-boot-hello-world-on-wildfly/</id><updated>2023-05-06T07:09:05Z</updated><content type="html">This updated (May 2023) article shows how to deploy Spring Boot 3 / Spring Boot 2 applications on top of WildFly application server as Web application archives (war). We will start by setting up the application with Spring Boot Initializr. Then, we will apply the configuration changes to deploy the application on WildFly. Spring Boot ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">What&amp;#8217;s new in Dashbuilder 0.28.0</title><link rel="alternate" href="https://blog.kie.org/2023/05/whats-new-in-dashbuilder-0-28-0.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/05/whats-new-in-dashbuilder-0-28-0.html</id><updated>2023-05-05T17:39:58Z</updated><content type="html">Kie Tools 0.28.0 is out and with it we have multiple improvements to Dashbuilder. Let’s explore it! INSTALLATION The new editor is live in and the VSCode extension is already available on the . For users just update the dashbuilder-client dependency version to  0.28.0. DASHBUILDER SAMPLES We organized Dashbuilder samples in the repository. Notice you can also access examples directly from . DASHBUILDER CLEANUP AND BACKEND REMOVAL Dashbuilder Authoring and Dashbuilder Runtime App were removed on this version. The reason is that Dashbuilder Runtime is now focused on YAML development and users who need a backend to produce datasets can make use of . As the consequence we removed 238k lines of code and made Dashbuilder faster and smaller: * Java classes from 3088 to 1546 * Dashbuilder client bundle from 21mb to 18mb * Main Javascript reduction from 1.8mb to 1.3mb (with gzip) * Main Load time reduction from ~1.1s to ~900ms ERROR MESSAGES AND USER FEEDBACK IMPROVEMENTS Continuing Dashbuilder user feedback improvements, my colleague Kumar Aditya made 3 great improvements: * Improve unreachable URLs error message.  * Improve dataset parsing error message * Show a message when a displayer configuration is invalid. Dashbuilder used to ignore the displayer with bad configuration, now it shows the cause for the bad configuration. DATASET IMPROVEMENTS Important improvements were made to Dashbuilder datasets * Dataset columns: Now dashbuilder set the LABEL as the default type for columns and it is able to retrieve columns name from Metrics or CSVs * Accumulate Flag: Datasets now can keep data in the memory if accumulate flag is true. This is especially important when reading metrics without Prometheus and having a displayer with auto update, then the metric values are kept in memory and you can display it any way you want. Learn more about it in the that introduced the accumulate flag * CSV parser: The CSV parser had minor fixes and the only currently known limitation is regarding quoted fields with line break.  CONCLUSION The 0.28.0 release is a great milestone for Dashbuilder! The backend removal was important not only for performance reasons, but also for code maintenance because now we have less classes to maintain and evolve 🙂  The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title type="html">Quarkus 2.16.7.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-16-7-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-16-7-final-released/</id><updated>2023-05-05T00:00:00Z</updated><content type="html">We released Quarkus 2.16.6.Final, the seventh maintenance release of our 2.16 release train. As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 2.16. If you are not already using 2.16, please refer to our migration guide. Full changelog You can get...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Integrate Excel with Drools on OpenShift with Knative and Quarkus!</title><link rel="alternate" href="https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2023/05/integrate-excel-with-drools-on-openshift-with-knative-and-quarkus.html</id><updated>2023-05-04T08:42:59Z</updated><content type="html">In this blog post I want to share the results of a technical exploration in bridging, bringing together and integrating a diverse set of technologies and platforms, ranging from classic spreadsheet applications (Excel) to serverless platforms (Knative on OpenShift) to technical rules executed by our rule engine Drools! INTRODUCTION This content has been inspired by I had the opportunity to read recently (see more below). So I wanted to take on a personal challenge to build a novel example, based on some of the powerful techniques presented in the book, and then add some more, going beyond. Specifically, I wanted to be able to invoke some custom DRL rule evaluation in a serverless way, by connecting Excel with my -based application served by on . As I wanted a use-case with plenty of realistic data for this technical exploration, I decided to focus my attention on the IoT (Internet of Things) which is another factor revolutionizing the way we live. If I think about the diverse ranges of devices available nowadays, from smart homes to connected cars, these IoT devices in my opinion are changing not only the way we interact with our surroundings… when used sapiently, I believe they can really augment and improve our lives. However, IoT is more than just internet-connected devices! To me, it is also about leveraging various technologies and platforms to create intelligent systems that can automate processes, optimize, and improve our decision-making. More specifically, I wanted to try processing the technical data collected through my smart scale and smart watch, collecting into Excel, and then processing it via the intelligent application described above. This will give us the opportunity to highlight some of the benefits of the integration scope mentioned in the preamble, and a perspective on how these techniques can help your organization or benefit your own use-cases! Before wrapping up, I will share my review of the mentioned book. SERVERLESS DROOLS Let’s dive into the DRL rules: rule R1 when $r : MeasRecord( morning == true, bpm &lt; 60 || bpm &gt; 100 ) then insert(new Advice("abnormal Blood Pressure in the morning", 100)); end rule R2 when $r : MeasRecord( weight &lt; weight_m3, weight_m3 &lt; weight_m5 ) then insert(new Advice("downward trend in weight")); end rule R3 when $r : MeasRecord( weight &gt; weight_m3, weight_m3 &gt; weight_m5 ) then insert(new Advice("upward trend in weight")); end Here, I want to define some rules which will advise me if specific data measurement is observed. These rules in my opinion are very naturally readable in spite of the technical nature of DRL: I want to emit an advice in case of abnormal bpm, or when there is a specific trend in weight compared to T-3D or T-5D (I take these measurements once each day). Similarly, you could think of analogous DRL rules for your IoT use-case, reacting to events and measurement signals from your sensors or devices! In order to make this intelligent application efficiently consumable as a serverless decision service, I decided to experiment with a number of capabilities of Drools v8 and Quarkus, starting by making use of the Drools v8 . Further, in order for the REST API in my Quarkus application to be easily consumable from external, JavaScript-based services and applications, I needed to enable CORS. A word of warning is important here with regards to the CORS “origin”, that should be tailored to your production use case (as ); if you decide to build on this example, you might want to consider for your allow-list to be specific to the expected origin of your clients (in my case Swagger UI from OpenShift and Excel ScriptLabs, but you might want to extend to the servers of your Office Add-In, etc): quarkus.http.cors=true # note: check settings for PROD: quarkus.http.cors.origins=/.*\\.azureedge\\.net/,/.*\\.openshiftapps\\.com/ quarkus.swagger-ui.always-include=true quarkus.kubernetes.deployment-target=knative quarkus.container-image.registry=quay.io quarkus.container-image.group=mmortari quarkus.container-image.builder=jib In addition to the CORS configuration, it’s pretty easy to influence the behavior of the final resulting Quarkus application, specifically: * I want the Swagger UI to be included in the deployed artifact * it will be a Knative Service, so to allow the serveless use-case, including auto-scale to zero * I find easier to publish my container images on Quay.io, to be picked up by my OpenShift instance * to build the container image, I typically use JIB These configuration steps are similar to what described in , showcasing how it’s really easy to build a Serverless application with Drools and Quarkus! Be sure to if you missed it.😉 EXCEL INTEGRATION Here comes the very unusual part, at least for me, where I wanted to apply some of the techniques from the book and then explore even further.🙂  First, I collected all the data from my IoT devices; personally I own a couple of smart devices from , as I appreciate they allow you to easily export an archive of your data, in CSV format: perfect for Excel! Similarly, you might consider expanding on this example by directly interacting instead . The archive exports a ZIP of a collection of CSV files; for my challenge I indeed decided to focus on bpm and weight measurements, which are actually in 2 separate files. To combine this data into a single table I’ve used a Power Query, one of the capabilities presented in the book, in order to connect to the CSV files as data sources and merge them seamlessly. The merge result is something similar to: Merging 2 CSV files in Excel Then, I have defined a custom function in Excel; you can find more information about this capability on as it is one of the most powerful mechanisms available to extend Excel with custom behavior. I should highlight that in the book, you will find many, many other mechanisms to perform an invocation from your Excel sheets to a remote Drools application running on OpenShift; personally, I opted to develop a custom function in order to try something new but also sophisticated, which could be bundled later as a fully-fledged Office Add-In; but the book indeed guides you through many more (and often easier) mechanisms! One of the reasons I loved that read so much, is that it offered a wide portfolio of options to choose from when it comes to integrating Excel with Drools. My final custom Excel function looks like this: /** @CustomFunction */ function advices(isodate: string, bpm: number, weight: number, weight_m3: number, weight_m5: number): Promise&lt;string&gt; { return new Promise(function (resolve, reject) { const baseUrl = "https://(...).openshiftapps.com"; const payload = JSON.stringify( { "ts": isodate, "bpm": bpm &gt; 0 ? bpm : null, "weight": weight, "weight_m3": weight_m3, "weight_m5": weight_m5 } ); fetch(baseUrl + "/advices", { method: "POST", body: payload, headers: { "Content-type": "application/json; charset=UTF-8" } }) .then((response) =&gt; response.json()) .then((json) =&gt; resolve(json)) .catch((error) =&gt; reject("unable to connect to Drools")) }); } …and it works like a charm! The custom function is invoked by a very simple Excel formula, as one would easily expect: Excel custom function to invoke Drools! It is also to be noted, again as expected, that when the formulas has been computed for the entire worksheet, the backend Knative service will automatically scale back to zero: a Knative based serverless backend serving the Excel formulas! This is super helpful only to consume computing resources when needed, in this case when some Excel worksheet needs to (re-)calculate its formulas!  As the final and most important result, we can appreciate the rules processing the data and producing the advice in our Excel file, as defined in the DRL. I believe combining Excel custom and extended behaviors with a serverless backend is truly a powerful combination! Thankfully integrating Quarkus and Drools and deploying our app on OpenShift with Knative is super easy as we’ve seen in this post. I hope this atypical blog post tickles your curiosity on how to integrate Excel or similarly other spreadsheet platforms; if you are interested to know more, I warmly invite you to check out this book… BOOK REVIEW: BUSINESS RULE ENGINES AND AI FOR EXCEL POWER USERS Title: Business Rule Engines and AI for Excel Power Users: Capture and scale your business knowledge into the cloud – with Microsoft 365, Decision Models, and AI tools from IBM and Red Hat Author: Paul Browne ISBN: 9781804619544 (ISBN10: 180461954X) I believe this book is an excellent guide for both software developers and business analysts seeking to scale the automation of their business knowledge into the cloud. It provides an in-depth analysis of how decision models and semantic rules can be combined with other AI models, to solve some of the inherent limitations of Excel –which is an omnipresent tool in every business and industry sector. The book introduces readers to industry-standard open source Drools rule engine and Kogito, and how these can be linked with many of Microsoft’s tools. Paul presents very easy-to-follow examples to teach readers how to author sophisticated decision models, how to develop decision services in order to solve current business challenges using AI (both ML and symbolic AI), and how to combine rules with workflows to deploy a cloud-based solution. The book also covers advanced modeling using the Decision Model and Notation (DMN) open standard and related testing tools. As a reader of this blog, I assume you are already familiar with some of the KIE projects, so you might be tempted to jump straight into reading from Chapter 6 onwards; but my recommendation would be to make sure to revisit the initial chapters nevertheless, especially Chapters 1-2, since they will equip you with important considerations when evaluating the adoption of the powerful techniques presented in this book in your organization. It is also to be noted that while specific to Microsoft tools, the techniques presented in this book (and this inspired blog post) can very likely be analogously applied using other software provider platforms and other hyperscalers! CONCLUSION I hope this blog post intrigued you to check out this new book and to explore more integration opportunities of Drools with other platforms and tools! How are you planning to integrate Drools for your next use-case? Let us know in the comments section below! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title>New C features in GCC 13</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/04/new-c-features-gcc-13" /><author><name>Marek Polacek</name></author><id>bbc67a05-6b6c-4d0c-a0eb-392635912965</id><updated>2023-05-04T07:00:00Z</updated><published>2023-05-04T07:00:00Z</published><summary type="html">&lt;p&gt;The latest major version of the&lt;a href="https://gcc.gnu.org/"&gt; GNU Compiler Collection&lt;/a&gt; (GCC), 13.1, was released in April 2023. Like every major GCC release, this version will bring many&lt;a href="https://gcc.gnu.org/gcc-13/changes.html"&gt; additions, improvements, bug fixes, and new features&lt;/a&gt;. GCC 13 is already the system compiler in&lt;a href="https://fedoraproject.org/wiki/Changes/GNUToolchainF38"&gt; Fedora 38&lt;/a&gt;. &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) users will get GCC 13 in the Red Hat GCC Toolset (RHEL 8 and RHEL 9). It's also possible to try GCC 13 on &lt;a href="https://godbolt.org/"&gt;godbolt.org&lt;/a&gt; and similar pages.&lt;/p&gt; &lt;p&gt;This article describes new features implemented in the C front end; it does not discuss developments in &lt;a href="https://developers.redhat.com/topics/c"&gt;the C language&lt;/a&gt; itself. It also doesn’t cover recent changes in the C library itself. If you’re interested in the C++ language and what's supported in recent GCC releases, check out &lt;a href="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10"&gt;New C++ features in GCC 10&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2022/04/25/new-c-features-gcc-12"&gt;New C++ features in GCC 12&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The default C dialect in GCC 13 is &lt;code&gt;-std=gnu17&lt;/code&gt;. You can use the &lt;code&gt;-std=c2x&lt;/code&gt; or &lt;code&gt;-std=gnu2x&lt;/code&gt; command-line options to enable C2X features. We use C2X to refer to the next major C standard version; it is expected to become C23.&lt;/p&gt; &lt;h2&gt;C2X features&lt;/h2&gt; &lt;p&gt;GCC 13 has implemented a host of C2X proposals. This section describes the most interesting ones.&lt;/p&gt; &lt;h3&gt;nullptr&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;nullptr&lt;/code&gt; constant first appeared in C++11, described in proposal &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2431.pdf"&gt;N2431&lt;/a&gt; from 2007. Its purpose was to alleviate the problems with the definition of &lt;code&gt;NULL&lt;/code&gt;, which can be defined in a variety of ways: &lt;code&gt;(void *)0&lt;/code&gt; (a pointer constant), &lt;code&gt;0&lt;/code&gt; (an integer), and so on. This posed problems for overload resolution, generic programming, etc. While C doesn’t have function overloading, the protean definition of &lt;code&gt;NULL&lt;/code&gt; still causes headaches. Consider the interaction of &lt;code&gt;_Generic&lt;/code&gt; with &lt;code&gt;NULL&lt;/code&gt;: it’s not clear which function will be called because it depends on the definition of &lt;code&gt;NULL&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; _Generic (NULL, void *: handle_ptr (), int: crash (), default: nop ()); &lt;/pre&gt; &lt;p&gt;Unfortunately, there are less contrived problems in practice. For instance, issues occur with conditional operators or when passing &lt;code&gt;NULL&lt;/code&gt; to a variadic function (taking &lt;code&gt;...&lt;/code&gt;): in such a case, applying &lt;code&gt;va_arg&lt;/code&gt; to the null argument may crash the program if an unexpected definition of &lt;code&gt;NULL&lt;/code&gt; is encountered. GCC 13 implements &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3042.htm"&gt;N3042&lt;/a&gt;, which brings &lt;code&gt;nullptr&lt;/code&gt; to C. Its type is &lt;code&gt;nullptr_t&lt;/code&gt; and is defined in &lt;code&gt;&lt;stddef.h&gt;&lt;/code&gt;. In C2X, the following assert therefore passes:&lt;/p&gt; &lt;pre&gt; static_assert (_Generic (nullptr, nullptr_t: 1, void *: 2, default: 0) == 1, "nullptr_t was selected"); &lt;/pre&gt; &lt;h3&gt;Enhanced enumerations&lt;/h3&gt; &lt;p&gt;Enhanced enumerations is another feature that first appeared in C++11 via &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2347.pdf"&gt;N2347&lt;/a&gt;. In C, the underlying type of an enum was not specified in the standard. In practice, the type would be determined based on the values of the enumerators. Typically, the type would be &lt;code&gt;unsigned int&lt;/code&gt;, or, if any of the values is negative, &lt;code&gt;int&lt;/code&gt;. In any case, the selected type must be capable of holding all of the values of the enum. Given this lacuna in the specification, enums have portability issues. To close this gap, C adopted &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2963.htm"&gt;N2963&lt;/a&gt;, adopting the C++ syntax:&lt;/p&gt; &lt;pre&gt; enum E : long long { R, G, B } e; static_assert (_Generic (e, long long: 1, default: 0) == 1, "E type");&lt;/pre&gt; &lt;p&gt;It seems worth mentioning, however, that specifying the wrong underlying type may lead to subtle problems.  Consider the following:&lt;/p&gt; &lt;pre&gt; enum F : int { A = 0x8000 } f; &lt;/pre&gt; &lt;p&gt;On most platforms, this code will work as expected. The precision of &lt;code&gt;int&lt;/code&gt; isn’t guaranteed to be at least 32 bits, however; it can validly be 16 bits, in which case the previous example will not compile. Thus a better variant would be to use one of the types defined in &lt;code&gt;&lt;stdint.h&gt;&lt;/code&gt;, for example:&lt;/p&gt; &lt;pre&gt; enum F : int_least32_t { A = 0x8000 } f; &lt;/pre&gt; &lt;h3&gt;(...) function prototypes&lt;/h3&gt; &lt;p&gt;C, prior to C2X, required that a variable-argument function has a named argument before the ellipsis (&lt;code&gt;...&lt;/code&gt;). This requirement was the result of historical baggage and is no longer necessary, so &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2975.pdf"&gt;N2975&lt;/a&gt; did away with the requirement. (C++ has always allowed &lt;code&gt;foo(...)&lt;/code&gt;.)&lt;/p&gt; &lt;pre&gt; void f(int, ...); // OK void g(...); // OK in C2X &lt;/pre&gt; &lt;p&gt;Note, however, that &lt;code&gt;fn(...)&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; an unprototyped function, so it is possible to use the &lt;code&gt;va_start&lt;/code&gt; and &lt;code&gt;va_arg&lt;/code&gt; mechanism to access its arguments. An unprototyped function has the form &lt;code&gt;void u();&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Such functions were removed in C2X (see below).&lt;/p&gt; &lt;h3&gt;Type inference with auto&lt;/h3&gt; &lt;p&gt;Type deduction is another feature that first appeared in C++11 via &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1984.pdf"&gt;N1984&lt;/a&gt;. It is a convenient feature that allows the programmer to use the placeholder &lt;code&gt;auto&lt;/code&gt; as the type in a declaration. The compiler will then deduce the variable’s type from the initializer:&lt;/p&gt; &lt;pre&gt; auto i = 42; &lt;/pre&gt; &lt;p&gt;It is, however, more than just a convenience feature to save typing a few more characters. Consider:&lt;/p&gt; &lt;pre&gt; auto x = foo (y); &lt;/pre&gt; &lt;p&gt;Here, the type of &lt;code&gt;foo (y)&lt;/code&gt; may depend on &lt;code&gt;y&lt;/code&gt; (&lt;code&gt;foo&lt;/code&gt; could be a macro using &lt;code&gt;_Generic&lt;/code&gt;), so changing &lt;code&gt;y&lt;/code&gt; implies changing the type of &lt;code&gt;x&lt;/code&gt;. Using &lt;code&gt;auto&lt;/code&gt; in the example above means that the programmer doesn’t have to change the rest of the codebase when the type of &lt;code&gt;y&lt;/code&gt; is updated. GCC has offered &lt;code&gt;__auto_type&lt;/code&gt; since &lt;a href="https://gcc.gnu.org/gcc-4.9/changes.html"&gt;GCC 4.9&lt;/a&gt;, whose semantics is fairly close to C2X &lt;code&gt;auto&lt;/code&gt;, though not exactly the same, and appears to have been used mostly in standard headers. Unlike C++, &lt;code&gt;auto&lt;/code&gt; must be used plain: it cannot be combined with &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;[]&lt;/code&gt; and similar. Moreover, &lt;code&gt;auto&lt;/code&gt; also doesn’t support braces around the initializer. The &lt;code&gt;auto&lt;/code&gt; feature is only enabled in C2X mode. In older modes, &lt;code&gt;auto&lt;/code&gt; is a redundant storage class specifier which can only be used at block scope.&lt;/p&gt; &lt;h3&gt;The constexpr specifier&lt;/h3&gt; &lt;p&gt;Yet another feature that first appeared in C++ is the &lt;code&gt;constexpr&lt;/code&gt; specifier (see for instance &lt;a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2235.pdf"&gt;N2235&lt;/a&gt;, though C++ &lt;code&gt;constexpr&lt;/code&gt; has been greatly expanded since). C &lt;code&gt;constexpr&lt;/code&gt; was introduced in &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3018.htm"&gt;N3018&lt;/a&gt;, with much more limited functionality. Declaring a variable as &lt;code&gt;constexpr&lt;/code&gt; guarantees that the variable can be used in various constant-expression contexts. C requires that objects with static storage duration are initialized with constant expressions. It follows that &lt;code&gt;constexpr&lt;/code&gt; variables can be used to initialize objects with static storage duration. Another great advantage of &lt;code&gt;constexpr&lt;/code&gt; is that various semantic constraints are checked at compile time. Let’s demonstrate both points with an example (note that you must specify &lt;code&gt;-std=c2x&lt;/code&gt; or &lt;code&gt;-std=gnu2x&lt;/code&gt; to be able to use &lt;code&gt;constexpr&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; constexpr int i = 12; static_assert (i == 12); struct X { int bf : i; }; struct S { long l; }; constexpr struct S s = { 1L }; static_assert (s.l == 1L); constexpr unsigned char q = 0xff + i; // initializer not representable in type of object&lt;/pre&gt; &lt;h3&gt;Storage-class specifiers in compound literals&lt;/h3&gt; &lt;p&gt;A compound literal is a way to create unnamed objects that typically have automatic storage duration. Because they are lvalues, it is permitted to take their address:&lt;/p&gt; &lt;pre&gt; int *p = (int []){2, 4}; // p points to the first element of an array of two ints const int *q = &amp;(const int){1}; &lt;/pre&gt; &lt;p&gt;Paper &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3038.htm"&gt;N3038&lt;/a&gt; allows using certain storage-class specifiers (things like &lt;code&gt;constexpr&lt;/code&gt;, &lt;code&gt;static&lt;/code&gt;, &lt;code&gt;thread_local&lt;/code&gt;) in compound literals in C2X mode. This is useful to change the lifetime of the compound literal, or to make it a &lt;em&gt;compound literal constant&lt;/em&gt; with the &lt;code&gt;constexpr&lt;/code&gt; keyword:&lt;/p&gt; &lt;pre&gt; struct S { int i; }; void f (void) { static struct S s = (constexpr struct S){ 42 }; } int * g (void) { return &amp;(static int){ 42 }; } &lt;/pre&gt; &lt;p&gt;Note that even though &lt;code&gt;typedef&lt;/code&gt;, &lt;code&gt;extern&lt;/code&gt;, and &lt;code&gt;auto&lt;/code&gt; are storage-class specifiers, they are not allowed in compound literals.&lt;/p&gt; &lt;h3&gt;C2X typeof&lt;/h3&gt; &lt;p&gt;C2X standardized &lt;code&gt;typeof&lt;/code&gt;, a feature that has been supported as a GNU extension for many years which allows the programmer to get the type of an expression as described &lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Typeof.html"&gt;here&lt;/a&gt;. Along with &lt;code&gt;typeof&lt;/code&gt;, C2X also adds &lt;code&gt;typeof_unqual&lt;/code&gt;, which additionally removes all qualifiers and &lt;code&gt;_Atomic&lt;/code&gt; from the resulting type:&lt;/p&gt; &lt;pre&gt; int i; volatile int vi; extern typeof (vi) vi; // OK, no conflict extern typeof_unqual (vi) i; // OK, no conflict &lt;/pre&gt; &lt;p&gt;A minor difference between the GNU version and the standard version is the treatment of the noreturn property of a function: the GNU variant of &lt;code&gt;typeof&lt;/code&gt; takes noreturn as part of the type of a pointer to function, but the standard version does not.&lt;/p&gt; &lt;p&gt;Note that C++11 standardized a similar feature under the name &lt;code&gt;decltype&lt;/code&gt;, so sadly we wound up with two names for a nearly identical feature.&lt;/p&gt; &lt;h3&gt;New keywords&lt;/h3&gt; &lt;p&gt;This proposal harmonizes C and C++ further by making &lt;code&gt;alignas&lt;/code&gt;, &lt;code&gt;alignof&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;, &lt;code&gt;static_assert&lt;/code&gt;, &lt;code&gt;thread_local&lt;/code&gt;, and &lt;code&gt;true&lt;/code&gt; ordinary keywords in C2X mode. Therefore this translation unit will compile OK in C2X mode:&lt;/p&gt; &lt;pre&gt; static_assert (true, ""); &lt;/pre&gt; &lt;p&gt;This change can break existing code, for example&lt;/p&gt; &lt;pre&gt; int alignof = 42; &lt;/pre&gt; &lt;p&gt;will not compile in C2X mode.&lt;/p&gt; &lt;h3&gt;The noreturn attribute&lt;/h3&gt; &lt;p&gt;A further compatibility tweak to bring C and C++ closer together. C11 added the &lt;code&gt;_Noreturn&lt;/code&gt; function specifier to signal to the compiler that a function never returns to its caller, but &lt;code&gt;_Noreturn&lt;/code&gt; works in C only, so C2X &lt;a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2764.pdf"&gt;N2764&lt;/a&gt; added a standard &lt;code&gt;[[noreturn]]&lt;/code&gt; attribute while simultaneously marking &lt;code&gt;_Noreturn&lt;/code&gt; as obsolescent.&lt;/p&gt; &lt;pre&gt; [[noreturn]] void exit (int); &lt;/pre&gt; &lt;h3&gt;Empty initializer braces&lt;/h3&gt; &lt;p&gt;C2X standardized empty initializer braces (&lt;code&gt;{}&lt;/code&gt;) and GCC 13 implements this proposal. Some cases were already supported as a GNU extension (e.g., initializing an array or a structure), but newly it’s possible to use &lt;code&gt;{}&lt;/code&gt; to initialize a scalar variable or a variable-length array as well:&lt;/p&gt; &lt;pre&gt; int i = {}; int arr[10] = {}; struct S { int i; }; struct S s = {}; void g (void) { int n = 10; int vla[n] = {}; } &lt;/pre&gt; &lt;h3&gt;unreachable macro&lt;/h3&gt; &lt;p&gt;C2X brings the &lt;code&gt;unreachable()&lt;/code&gt; macro, defined in &lt;code&gt;&lt;stddef.h&gt;&lt;/code&gt;, which is a convenient shorthand for the GCC built-in function &lt;code&gt;__builtin_unreachable()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; #include &lt;stddef.h&gt; int foo (int x) { if (x &lt; 0) unreachable (); return x &amp; 1; } &lt;/pre&gt; &lt;h3&gt;Unprototyped functions removed&lt;/h3&gt; &lt;p&gt;Unprototyped functions in C were of the form &lt;code&gt;int foo()&lt;/code&gt;, which is a function &lt;code&gt;foo&lt;/code&gt; that returns an integer which takes an unspecified number of arguments of unspecified types. This is very dangerous because the compiler can’t perform any checking when such a function is used.&lt;/p&gt; &lt;p&gt;In C2X, &lt;code&gt;int foo()&lt;/code&gt; is equivalent to &lt;code&gt;int foo(void)&lt;/code&gt;, which is a function &lt;code&gt;foo&lt;/code&gt; that returns an integer and takes no arguments.&lt;/p&gt; &lt;h2&gt;New warnings&lt;/h2&gt; &lt;p&gt;The C front end has gained some new warnings in GCC 13. For instance, &lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wxor-used-as-pow"&gt;-Wxor-used-as-pow&lt;/a&gt;, which was described in the C++ part of the GCC 13 blog post. There’s a new warning specific for the C front end.&lt;/p&gt; &lt;h3&gt;-Wenum-int-mismatch&lt;/h3&gt; &lt;p&gt;In C, an enumerated type is compatible with char, a signed integer type, or an unsigned integer type, so the following code compiles if the underlying type of &lt;code&gt;enum E&lt;/code&gt; is &lt;code&gt;int&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; enum E { l = -1, z = 0, g = 1 }; int foo(void); enum E foo(void) { return z; } &lt;/pre&gt; &lt;p&gt;However, as I previously noted, the choice of the underlying type of the enum is implementation-defined. Since the code above is likely a mistake and constitutes a portability problem (the code will not compile if a different type than &lt;code&gt;int&lt;/code&gt; is chosen to be the underlying type), GCC 13 implements a new warning which warns about enum/integer type mismatches. For the code above the warning looks like the following:&lt;/p&gt; &lt;pre&gt; q.c:5:10: warning: conflicting types for ‘foo’ due to enum/integer mismatch; have ‘enum E(void)’ [-Wenum-int-mismatch] 5 | enum E foo(void) { return z; } | ^~~ q.c:4:7: note: previous declaration of ‘foo’ with type ‘int(void)’ 4 | int foo(void); | ^~~ &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;GCC 13 implements many C2X proposals. These proposals align the C and C++ languages a little bit closer to each other by entwining certain features, and make programming in C easier and more secure.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/04/new-c-features-gcc-13" title="New C features in GCC 13"&gt;New C features in GCC 13&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Marek Polacek</dc:creator><dc:date>2023-05-04T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.0.2.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-0-2-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-0-2-final-released/</id><updated>2023-05-04T00:00:00Z</updated><content type="html">We released Quarkus 3.0.2.Final, the first maintenance release of our 3.0 release train (as our first public release for 3.0 was 3.0.1.Final). As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 3.0. If you are not already using 3.0, please refer...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>How to debug C++ lambda expressions with GDB</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/03/how-debug-c-lambda-expressions-gdb" /><author><name>Kevin Buettner</name></author><id>f017049c-3b61-4400-b158-a767e759946e</id><updated>2023-05-03T07:00:00Z</updated><published>2023-05-03T07:00:00Z</published><summary type="html">&lt;p&gt;Modern versions of the &lt;a href="https://developers.redhat.com/topics/c"&gt;C++&lt;/a&gt; programming language have a feature known as &lt;a href="https://en.cppreference.com/w/cpp/language/lambda"&gt;lambda expressions&lt;/a&gt;. This article shows how you can debug lambda expressions using GDB, the GNU Project Debugger. Even if you're not interested in debugging lambdas, the techniques presented here are useful for many other debugging situations.&lt;/p&gt; &lt;h2&gt;What is a lambda expression?&lt;/h2&gt; &lt;p&gt;A lambda expression provides the C++ programmer with a way to create an anonymous or unnamed function. Lambda expressions are often used in situations where a callback function is desired. Using a lambda expression often makes writing a callback function significantly easier since various pieces of state that are known in the function from which the callback is passed don't need to be packaged up into a data structure which the callback function will later access. This is due to the fact that a C++ lambda expression provides a way to capture in-scope variables which may be later used when the lambda expression is executed.&lt;/p&gt; &lt;p&gt;The example below shows some simple captures in a few of its lambda expressions.&lt;/p&gt; &lt;h2&gt;Example program&lt;/h2&gt; &lt;p&gt;I'll use the example program below to demonstrate some debugging techniques in addition to showing some of the challenges that might be encountered when debugging lambda expressions. I've included line numbers as comments for some of the lines at which breakpoints might be placed. I've named this program &lt;code&gt;lambda.cc&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; #include &lt;functional&gt; int successor (int i) { return i + 1; } int apply0 (int (*fn)(int), int arg) { return fn (arg); } int apply1 (std::function&lt;int(int)&gt; fn, int arg) { return fn (arg); } std::function&lt;int(int)&gt; make_function(int&amp; x) { return [&amp;] (int i) { return i + x; }; /* Line 17 */ } int main (int argc, char **argv) { int n = 7, m = -28; printf ("Answer 1 is %d\n", apply0 (successor, 3)); /* Line 24 */ printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ printf ("Answer 3 is %d\n", apply0 ([] (int i) { return i + 1; }, 1)); /* Line 28 */ printf ("Answer 4 is %d\n", apply1 ([] (int i) { return i + 1; }, 2)); /* Line 30 */ printf ("Answer 5 is %d\n", apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ auto lf2 = make_function (n); /* Line 35 */ printf ("Answer 6 is %d\n", apply1 (lf2, 1)); /* Line 36 */ auto lf3 = make_function (m); printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lines 17, 28, 30, and 33 all contain lambda expressions. The lambda expression on line 33 is:&lt;/p&gt; &lt;p&gt;&lt;code&gt;[n] (int i) { return i + n; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Lambda expressions start with a left square bracket. In this particular lambda expression the left square bracket is followed by &lt;code&gt;n&lt;/code&gt;, which is a variable that is being &lt;em&gt;captured&lt;/em&gt; for use in the body of the lambda. Note that &lt;code&gt;n&lt;/code&gt; is a local variable in the function &lt;code&gt;main&lt;/code&gt;. Parentheses enclose a list of formal parameters to the anonymous function being defined; in this case there is just one parameter named &lt;code&gt;i&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Finally, the body of the lambda expression is placed between curly braces, just like a normal function. The body consists of zero or more executable statements. In this case, there is just one executable statement, a return statement. But do note that the expression to return refers to both the captured variable &lt;code&gt;n&lt;/code&gt; and the parameter &lt;code&gt;i&lt;/code&gt;. Note that there is no function name—that's what makes it anonymous. Lambda expressions are sometimes assigned to variables; this is done indirectly on lines 35 and 38. There are other optional syntactic components as well as many nuances of lambda expressions which are not described here.&lt;/p&gt; &lt;h2&gt;Building and debugging the example program&lt;/h2&gt; &lt;p&gt;You can use the GNU C++ compiler to compile and link the example program by using this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;g++ -Wall -g -o lambda lambda.cc&lt;/code&gt;&lt;/p&gt; &lt;p&gt;A similar command can be used to build an executable using the LLVM C++ compiler (clang++):&lt;/p&gt; &lt;p&gt;&lt;code&gt;clang++ -Wall -g -o lambda-clang lambda.cc &lt;/code&gt;&lt;/p&gt; &lt;p&gt;GDB interactions shown below were performed using GDB 13.1 on a Fedora 37 machine. Except where noted, the example program was compiled using GCC (g++) 12.2.1. When I did use LLVM (clang++) to check compatibility, I used Clang version 15.0.7.&lt;/p&gt; &lt;p&gt;We can begin debugging the example program using GDB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ gdb -q lambda Reading symbols from lambda... (gdb) start Temporary breakpoint 1 at 0x401275: file lambda.cc, line 22. Starting program: /home/kev/examples/lambda This GDB supports auto-downloading debuginfo from the following URLs: &lt;https://debuginfod.fedoraproject.org/&gt; Enable debuginfod for this session? (y or [n]) y Debuginfod has been enabled. To make this setting permanent, add 'set debuginfod enabled on' to .gdbinit. [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". Temporary breakpoint 1, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:22 22 int n = 7, m = -28; (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example shows how GDB is invoked on the executable named &lt;code&gt;lambda&lt;/code&gt;. The &lt;code&gt;-q&lt;/code&gt; switch causes GDB to not display copyright, warranty and information about how to obtain documentation and help. Once in GDB, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Starting.html#index-start"&gt;&lt;code&gt;start&lt;/code&gt; command&lt;/a&gt; is used to run to the first executable line in the &lt;code&gt;main&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;Note, too, that I answered &lt;code&gt;y&lt;/code&gt; to enable &lt;a href="https://developers.redhat.com/blog/2019/10/14/introducing-debuginfod-the-elfutils-debuginfo-server"&gt;debuginfod&lt;/a&gt; for the session. When debuginfod is enabled, debugging information associated with libraries used by the program may be downloaded for use by GDB.&lt;/p&gt; &lt;p&gt;In the following examples, I won't show (again) the initial step of compiling the program nor the initial steps of debugging the program with GDB. That said, if you want to follow along, I've organized the examples (up to the section &lt;a href="#Debugging an LLVM compiled program"&gt;Debugging an LLVM compiled program&lt;/a&gt;) so that you won't need to restart GDB—you should be able to obtain similar output by simply typing the commands shown here to GDB.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; if the version of GDB or version of the compiler used to build the example program differ significantly form the versions noted earlier, it's possible that you'll see output different from what I show here.&lt;/p&gt; &lt;h2&gt;Debugging apply0 and successor&lt;/h2&gt; &lt;p&gt;Before looking at debugging a lambda expression, let's first look at using GDB to step into the &lt;code&gt;apply0&lt;/code&gt; call at line 24. Note that &lt;code&gt;successor&lt;/code&gt; is passed to &lt;code&gt;apply0&lt;/code&gt;; the &lt;code&gt;successor&lt;/code&gt; function adds one to its &lt;code&gt;int&lt;/code&gt; argument and returns this value.  &lt;code&gt;apply0&lt;/code&gt; takes two arguments, the first of which is a pointer to a function with the second being the value to pass to that function.  It simply calls the function with the argument and returns the result. Stepping into &lt;code&gt;apply0&lt;/code&gt; and then &lt;code&gt;successor&lt;/code&gt; is straightforward:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) until 24 main (argc=1, argv=0x7fffffffdd88) at lambda1.cc:24 24 printf ("Answer 0.0 is %d\n", apply0 (successor, 3)); (gdb) step apply0 (fn=0x401156 &lt;successor(int)&gt;, arg=3) at lambda1.cc:8 8 return fn (arg); (gdb) step successor (i=3) at lambda1.cc:4 4 int successor (int i) { return i + 1; } (gdb) print i $1 = 3 (gdb) backtrace #0 successor (i=3) at lambda1.cc:4 #1 0x000000000040117f in apply0 (fn=0x401156 &lt;successor(int)&gt;, arg=3) at lambda1.cc:8 #2 0x0000000000401305 in main (argc=1, argv=0x7fffffffdd88) at lambda1.cc:24 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above example shows GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-until"&gt;&lt;code&gt;until&lt;/code&gt; command&lt;/a&gt;, which, in this case, can be used to advance program execution to the specified line, in this case line 24. (Note: The &lt;code&gt;until&lt;/code&gt; command is used for other purposes too; don't expect it to always advance to the specified line number.)&lt;/p&gt; &lt;p&gt;Next, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-step"&gt;&lt;code&gt;step&lt;/code&gt; command&lt;/a&gt; is used twice, once to step into &lt;code&gt;apply0&lt;/code&gt; and then again into &lt;code&gt;successor&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After that, a &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Data.html#index-print"&gt;&lt;code&gt;print&lt;/code&gt; command&lt;/a&gt; is used to show the value of the argument &lt;code&gt;i&lt;/code&gt; which is in the &lt;code&gt;successor&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;Finally, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Backtrace.html#index-backtrace"&gt;&lt;code&gt;backtrace&lt;/code&gt; command&lt;/a&gt; is used to show the stack trace.&lt;/p&gt; &lt;p&gt;These commands are straightforward and should be unsurprising to anyone accustomed to using GDB. Ideally, we'd like the debugging of lambda expressions to be just as straightforward, though we'll soon see that this is not the case.&lt;/p&gt; &lt;h2&gt;Function objects: Debugging apply1 and successor&lt;/h2&gt; &lt;p&gt;The first gotcha occurs when using a function object. To demonstrate this, I've defined &lt;code&gt;apply1&lt;/code&gt; to be very similar to &lt;code&gt;apply0&lt;/code&gt;, with the only difference being that instead of taking a function pointer as the first argument (as is done in &lt;code&gt;apply0&lt;/code&gt;), the first argument of &lt;code&gt;apply1&lt;/code&gt; is a function object which takes an int argument and returns an int. As a reminder, this is what &lt;code&gt;apply1&lt;/code&gt; looks like:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;int apply1 (std::function&lt;int(int)&gt; fn, int arg) { return fn (arg); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In GDB, we'll first advance to line 25 by first using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-tbreak"&gt;&lt;code&gt;tbreak&lt;/code&gt; command&lt;/a&gt; to place a temporary breakpoint, after which we use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-c-_0028continue_0029"&gt;&lt;code&gt;continue&lt;/code&gt; command&lt;/a&gt; to advance to that breakpoint:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) tbreak 25 Temporary breakpoint 2 at 0x4012a9: file lambda.cc, line 25. (gdb) continue Continuing. Answer 1 is 4 Temporary breakpoint 2, main (argc=1, argv=0x7fffffffdd98) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before attempting to step into &lt;code&gt;apply1&lt;/code&gt;, let's create a checkpoint to which we'll come back later:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 1: fork returned pid 4178814. &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Checkpoint_002fRestart.html#index-checkpoint-1"&gt;&lt;code&gt;checkpoint&lt;/code&gt; command&lt;/a&gt; has several limitations: it doesn't work for multi-threaded programs and it also only works on GNU/Linux. But when it can be used, it's very handy for situations where you might wish to return to an earlier program state.&lt;/p&gt; &lt;p&gt;Now, let's see what happens when we try to step into &lt;code&gt;apply1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step std::function&lt;int (int)&gt;::function&lt;int (&amp;)(int), void&gt;(int (&amp;)(int)) ( this=0x7fffffffdb90, __f=@0x401156: {int (int)} 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/std_function.h:437 437 : _Function_base() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What has happened here is that we've stepped into code which is constructing the function object, which is not what we wanted—we wanted to step into &lt;code&gt;apply1&lt;/code&gt;. To get past this, we can use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-fin-_0028finish_0029"&gt;&lt;code&gt;finish&lt;/code&gt; command&lt;/a&gt; (which returns us to &lt;code&gt;main&lt;/code&gt; at line 25) and then &lt;code&gt;step&lt;/code&gt; again, which will get us into &lt;code&gt;apply1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) finish Run till exit from #0 std::function&lt;int (int)&gt;::function&lt;int (&amp;)(int), void&gt;(int (&amp;)(int)) (this=0x7fffffffdb90, __f=@0x401156: {int (int)} 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/std_function.h:437 0x00000000004012bd in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ (gdb) step apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A more complicated call might require the GDB user to issue multiple &lt;code&gt;finish&lt;/code&gt; / &lt;code&gt;step&lt;/code&gt; commands in order to end up in the desired function.&lt;/p&gt; &lt;h2&gt;Using GDB's skip command to avoid stepping into function object constructors&lt;/h2&gt; &lt;p&gt;Instead of using &lt;code&gt;finish&lt;/code&gt; and then &lt;code&gt;step&lt;/code&gt; as shown above, let's look at a way that GDB can more directly step into apply1. First, we'll return to the checkpoint that we created earlier—this is done by using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Checkpoint_002fRestart.html#index-restart-checkpoint_002did"&gt;&lt;code&gt;restart&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 1 Switching to Thread 0x7ffff7a89400 (LWP 4178814) #0 main (argc=1, argv=0x7fffffffdd98) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we'll use GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-skip"&gt;&lt;code&gt;skip&lt;/code&gt; command&lt;/a&gt;.  The &lt;code&gt;skip&lt;/code&gt; command shown here will cause GDB to skip, while stepping, any calls to any method in the std::function class, which includes the constructor that we ran into earlier. This particular &lt;code&gt;skip&lt;/code&gt; command uses the &lt;code&gt;-rfunction&lt;/code&gt; option with the regular expression &lt;code&gt;^std::function.*&lt;/code&gt;. The &lt;code&gt;-rfunction&lt;/code&gt; option indicates that the GDB &lt;em&gt;skip&lt;/em&gt; machinery should attempt to match functions at which it might stop against the specified regular expression.&lt;/p&gt; &lt;p&gt;In this case, the regular expression is &lt;code&gt;^std::function.*&lt;/code&gt;. The carat (&lt;code&gt;^&lt;/code&gt;) matches the beginning of the string being matched, followed by the literal &lt;code&gt;std::function&lt;/code&gt;. Finally, the&lt;code&gt;.*&lt;/code&gt; matches the rest of the string. (If you want to know more about regular expressions, I wholeheartedly recommend Jeffrey Friedl's book, &lt;a href="https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/"&gt;Mastering Regular Expressions&lt;/a&gt;.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) skip -rfunction ^std::function.* Function(s) ^std::function.* will be skipped when stepping. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, with the &lt;code&gt;skip&lt;/code&gt; in place, let's try the &lt;code&gt;step&lt;/code&gt; again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg); (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This time, due to the use of the &lt;code&gt;skip&lt;/code&gt; command, shown above, we're able to &lt;code&gt;step&lt;/code&gt; into &lt;code&gt;apply1&lt;/code&gt; in a similar fashion as shown for &lt;code&gt;apply0&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Attempting to step into the function call in apply1&lt;/h2&gt; &lt;p&gt;In attempting to step into the function call on line 13 (shown above), we'll encounter another "gotcha." So let's create another checkpoint:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 2: fork returned pid 4193641. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's see what happens when we step:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step 14 }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This didn't step into the successor function as expected.  It turns out that the skip command that we used earlier to avoid seeing the call to the function object constructor is now working against us.  When, while stepping, GDB finds a function to skip, it also skips all functions called by the function in question, even if some descendant call is to a function which wouldn't have been otherwise skipped. To better see what's gone wrong, let's restart at the most recent checkpoint and then disable the skip using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-skip-disable"&gt;&lt;code&gt;skip disable&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 2 Switching to Thread 0x7ffff7a89400 (LWP 4193641) #0 apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg); (gdb) info skip Num Enb Glob File RE Function 1 y n &lt;none&gt; y ^std::function.* (gdb) skip disable 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also shown above, is the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-info-skip"&gt;&lt;code&gt;info skip&lt;/code&gt; command&lt;/a&gt;, which is used to display a list of the things to be skipped while stepping.&lt;/p&gt; &lt;p&gt;Now, with the &lt;code&gt;skip&lt;/code&gt; on methods in &lt;code&gt;std::function&lt;/code&gt; disabled, let's try stepping again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step std::function&lt;int (int)&gt;::operator()(int) const (this=0x7fffffffdb90, __args#0=4) at /usr/include/c++/12/bits/std_function.h:589 589 if (_M_empty())&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We've ended up in the implementation of &lt;code&gt;operator()(int)&lt;/code&gt; in &lt;code&gt;std::function&lt;/code&gt;. While it is possible to end up in &lt;code&gt;successor()&lt;/code&gt; after stepping some more, this is tedious. It is better to instead use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-break"&gt;&lt;code&gt;break&lt;/code&gt; command&lt;/a&gt; to set a breakpoint in &lt;code&gt;successor&lt;/code&gt; and continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break successor Breakpoint 3 at 0x40115d: file lambda.cc, line 4. (gdb) continue Continuing. Breakpoint 3, successor (i=4) at lambda.cc:4 4 int successor (int i) { return i + 1; } (gdb) print i $2 = 4 (gdb) backtrace #0 successor (i=4) at lambda.cc:4 #1 0x00000000004026d8 in std::__invoke_impl&lt;int, int (*&amp;)(int), int&gt; ( __f=@0x7fffffffdb90: 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/invoke.h:61 #2 0x00000000004025ac in std::__invoke_r&lt;int, int (*&amp;)(int), int&gt; ( __fn=@0x7fffffffdb90: 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/invoke.h:114 #3 0x0000000000402452 in std::_Function_handler&lt;int (int), int (*)(int)&gt;::_M_invoke(std::_Any_data const&amp;, int&amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: 4) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdb90, __args#0=4) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 #6 0x00000000004012d1 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:25&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note how much more complicated this backtrace is compared to that of &lt;code&gt;apply0&lt;/code&gt; and &lt;code&gt;successor&lt;/code&gt;. This is due to the fact that frames corresponding to calls to C++ support functions for invoking a function object are still on the stack.&lt;/p&gt; &lt;h2&gt;Stopping in a lambda expression&lt;/h2&gt; &lt;p&gt;Thus far, we've encountered difficulties when stepping into a function to which a function object is passed. That problem is easily avoided by either using &lt;code&gt;finish&lt;/code&gt; followed by (another) &lt;code&gt;step&lt;/code&gt; or by using GDB's &lt;code&gt;skip&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;We've also seen that stepping into a call of a function object is problematic - instead of stepping directly into the function object, we instead step into some C++ implementation details related to making such a function call. Assuming we know the function being called, we can simply set a breakpoint on it and then continue.&lt;/p&gt; &lt;p&gt;This is the approach that should be taken for debugging a C++ lambda expression: set a breakpoint on it. Due to its anonymous nature, you don't know its name, so, instead, you must set the breakpoint via its line number. But this too, might be somewhat surprising. Let's take a closer look by placing a breakpoint on the lambda expression on line 33. As a reminder, the code looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; printf ("Answer 5 is %d\n", apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is what happens when we set a breakpoint on line 33:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 33 Breakpoint 4 at 0x40124f: /home/kev/examples/lambda.cc:33. (2 locations) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the message indicates that breakpoint 4 has been set at 2 locations. We'll use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-info-breakpoints"&gt;&lt;code&gt;info breakpoints&lt;/code&gt; command&lt;/a&gt; (abbreviated to &lt;code&gt;info break&lt;/code&gt;) to find out more:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) info break 4 Num Type Disp Enb Address What 4 breakpoint keep y &lt;MULTIPLE&gt; 4.1 y 0x000000000040124f in operator()(int) const at lambda.cc:33 4.2 y 0x000000000040136b in main(int, char**) at lambda.cc:33 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This shows the two locations upon which the breakpoint has been set. Breakpoint number 4.2 is in &lt;code&gt;main&lt;/code&gt; and will be the breakpoint that is hit first. The other breakpoint, indicated by 4.1, is the breakpoint for the lambda expression. Let's see what happens when we continue first to 4.2 and then to 4.1, and then look at some program state.  Note that we'll need to &lt;code&gt;continue&lt;/code&gt; twice, stopping once in &lt;code&gt;main&lt;/code&gt;, and the second time in the lambda function.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 2 is 5 Answer 3 is 2 Answer 4 is 3 Breakpoint 4.2, main (argc=1, argv=0x7fffffffdd98) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ (gdb) continue Continuing. Breakpoint 4.1, operator() (__closure=0x7fffffffdc00, i=4) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ (gdb) print i $2 = 4 (gdb) print n $3 = 7 (gdb) backtrace #0 operator() (__closure=0x7fffffffdc00, i=4) at lambda.cc:33 #1 0x0000000000401ff0 in std::__invoke_impl&lt;int, main(int, char**)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401d3e in std::__invoke_r&lt;int, main(int, char**)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x0000000000401954 in std::_Function_handler&lt;int(int), main(int, char**)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdaf4: 4) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc00, __args#0=4) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 #6 0x0000000000401398 in main (argc=1, argv=0x7fffffffdd98) at lambda.cc:32 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that there are four stack frames in between &lt;code&gt;apply1&lt;/code&gt; and the lambda function at frame #0. This is the C++ support code responsible for invoking the lambda expression. Also note that GDB is able print both the argument &lt;code&gt;i&lt;/code&gt; and the captured variable &lt;code&gt;n&lt;/code&gt; which was declared in &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Stopping in a particular invocation of a lambda expression&lt;/h2&gt; &lt;p&gt;The example program defines a function which returns a function object representing a lambda expression:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;std::function&lt;int(int)&gt; make_function(int&amp; x) { return [&amp;] (int i) { return i + x; }; /* Line 17 */ } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's used twice as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; auto lf2 = make_function (n); printf ("Answer 6 is %d\n", apply1 (lf2, 1)); /* Line 36 */ auto lf3 = make_function (m); printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Suppose that we wish to only stop in the lambda expression invoked on line 39. The lambda expression is actually on line 17, so we'll have to place a breakpoint on that line, but, as we'll see, the code for the lambda expression is also invoked via the call to apply1 on line 36. We'll arrive at the solution in stages.&lt;/p&gt; &lt;p&gt;We can start out by placing a breakpoint on line 17, after first making another checkpoint. (I'm making another checkpoint so that we can easily back up to an earlier execution point without having to start the program from scratch.) Also, we'll take a look at the locations at which GDB has placed breakpoints:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 3: fork returned pid 2030341. (gdb) break 17 Breakpoint 5 at 0x4011af: /home/kev/examples/lambda.cc:17. (2 locations) (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 y 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info breakpoint 5&lt;/code&gt; command shows that breakpoint 5.2 is in &lt;code&gt;make_function&lt;/code&gt; and that breakpoint 5.1 is in &lt;code&gt;operator()(int)&lt;/code&gt;, which is the lambda expression. Our goal is to stop only in the lambda expression, so let's disable breakpoint 5.2.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5.2 (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's see what happens when we &lt;code&gt;continue&lt;/code&gt;.  It'll take two &lt;code&gt;continue&lt;/code&gt; commands to get to the lambda invoked by the &lt;code&gt;apply1&lt;/code&gt; call at line 39:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 5 is 11 Breakpoint 5.1, operator() (__closure=0x7fffffffdc20, i=1) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) continue Continuing. Answer 6 is 8 Breakpoint 5.1, operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) backtrace #0 operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 #1 0x0000000000401e70 in std::__invoke_impl&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401a79 in std::__invoke_r&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x000000000040174e in std::_Function_handler&lt;int(int), make_function(int&amp;)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: -14) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc40, __args#0=-14) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=-14) at lambda.cc:13 #6 0x0000000000401452 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;backtrace&lt;/code&gt; shows that the second &lt;code&gt;continue&lt;/code&gt; command brought us to the lambda expression invoked by the the call to &lt;code&gt;apply1&lt;/code&gt; at line 39 in &lt;code&gt;main&lt;/code&gt;. For this simple program, it's not onerous to continue twice, but in real application code, it might happen that hundreds or thousands of &lt;code&gt;continue&lt;/code&gt; commands might be needed to stop at the point of interest.&lt;/p&gt; &lt;p&gt;To show how we can stop at a breakpoint which might be hit after some other line of code has been executed, let's first restart from checkpoint 3:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 3 Switching to Thread 0x7ffff7a89400 (LWP 2030341) #0 operator() (__closure=0x7fffffffdbf0, i=4) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This causes GDB to switch to the fork created in the previous checkpoint, but it doesn't undo any of the breakpoints that we set up after creating the checkpoint. To see this, let's look again at breakpoint 5:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; breakpoint already hit 2 times 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is pretty close to the state of breakpoint 5 when we last looked at it. One difference is that it tells us that breakpoint 5 has (overall) been hit 2 times, whereas that message was missing we we looked at it earlier.&lt;/p&gt; &lt;p&gt;Now, just so it's clear where we are in the program, we'll set a temporary breakpoint at line 35 and then continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) tbreak 35 Temporary breakpoint 6 at 0x4013b5: file lambda.cc, line 35. (gdb) continue Continuing. Answer 5 is 11 Temporary breakpoint 6, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:35 35 auto lf2 = make_function (n);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB has stopped on the first call to &lt;code&gt;make_function&lt;/code&gt;. The following line, 36, will call &lt;code&gt;apply1&lt;/code&gt; using the function object held in &lt;code&gt;lf2&lt;/code&gt;. Recall that it's our goal to stop in the lambda expression invoked by calling &lt;code&gt;apply1&lt;/code&gt; on line 39. In order to realize that goal, let's first disable breakpoint 5 and then look at what &lt;code&gt;info breakpoint&lt;/code&gt; says about it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5 (gdb) info break 5 Num Type Disp Enb Address What 5 breakpoint keep n &lt;MULTIPLE&gt; breakpoint already hit 2 times 5.1 y- 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Examining the 'Enb' column shows that, overall, breakpoint 5 is disabled, but were it to be enabled, then breakpoint 5.1 would also be enabled, while breakpoint 5.2 would still be disabled. If we were to continue at this point, the program would not stop at either of the breakpoint 5 locations since it is currently disabled.&lt;/p&gt; &lt;p&gt;The next step is to place a breakpoint at line 39 and then add some commands to run when that breakpoint is hit.  Specifically, we'll enable breakpoint 5 as one of the commands. The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Break-Commands.html#index-commands"&gt;&lt;code&gt;commands&lt;/code&gt; command&lt;/a&gt; is used to associate some GDB commands with a breakpoint; the &lt;code&gt;commands&lt;/code&gt; command can be given an argument specifying the breakpoint number (to which to associate some GDB commands), but when no breakpoint number is specified, as is shown below, it simply attaches commands to the most recently created breakpoint.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 39 Breakpoint 7 at 0x40142b: file lambda.cc, line 39. (gdb) commands Type commands for breakpoint(s) 7, one per line. End with a line saying just "end". &gt;enable 5 &gt;continue &gt;end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After enabling breakpoint 5 (via the command &lt;code&gt;enable 5&lt;/code&gt;), a &lt;code&gt;continue&lt;/code&gt; command will be issued. Thus, this breakpoint won't stop for user interaction, but will continue execution after first enabling breakpoint #5, which is for the lambda expression that we want to stop in. This is what happens when we continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 6 is 8 Breakpoint 7, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39 39 printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ Breakpoint 5.1, operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) print x $5 = (int &amp;) @0x7fffffffdb88: -28 (gdb) print i $6 = -14 (gdb) backtrace #0 operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 #1 0x0000000000401e70 in std::__invoke_impl&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401a79 in std::__invoke_r&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x000000000040174e in std::_Function_handler&lt;int(int), make_function(int&amp;)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: -14) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc40, __args#0=-14) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=-14) at lambda.cc:13 #6 0x0000000000401452 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that GDB shows that breakpoint 7 was hit, but it doesn't stop at it. If we wanted to cause that message to not be printed, we could have used the special command &lt;code&gt;silent&lt;/code&gt; as the first command in the commands for breakpoint #7. (Note: the &lt;code&gt;silent&lt;/code&gt; command can only be used as a command within the &lt;code&gt;commands&lt;/code&gt; command.)&lt;/p&gt; &lt;p&gt;The above example also prints the value of the captured variable &lt;code&gt;x&lt;/code&gt; and the argument (to the lambda function) &lt;code&gt;i&lt;/code&gt;. Frame #6 of the backtrace shows that the call to &lt;code&gt;apply1&lt;/code&gt; was invoked from line 39 in function &lt;code&gt;main&lt;/code&gt;. Frames #0, #5, and #6 correspond to lines of code in our example program. The remaining frames, #1 thru #4, show calls to functions within the C++ library.&lt;/p&gt; &lt;p&gt;GDB's ability to execute commands associated with a breakpoint is a powerful feature that's useful in many other situations as well.&lt;/p&gt; &lt;h2&gt;&lt;a id="Debugging an LLVM compiled program" name="Debugging an LLVM compiled program"&gt;&lt;/a&gt;Debugging an LLVM-compiled program&lt;/h2&gt; &lt;p&gt;As noted earlier, the interactions with GDB shown above were performed against an executable built with GCC 12.1.&lt;/p&gt; &lt;p&gt;If you're using clang++ instead of the GNU compiler, most of the interactions will be similar if not identical to that shown above. One somewhat important difference is the order in which the locations for a breakpoint on a line containing a lambda expression are shown. When using the LLVM compiler, these locations are reversed from that shown for the GNU compiler. E.g., when setting a breakpoint on line 33, we (might) see this instead when debugging an executable produced by the LLVM compiler:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 33 Breakpoint 4 at 0x4013bb: /home/kev/examples/lambda.cc:33. (2 locations) (gdb) info break 4 Num Type Disp Enb Address What 4 breakpoint keep y &lt;MULTIPLE&gt; 4.1 y 0x00000000004013bb in main(int, char**) at lambda.cc:33 4.2 y 0x000000000040205f in main::$_2::operator()(int) const at lambda.cc:33&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This isn't especially important for breakpoint 4, but it is important for breakpoint 5 which was placed on line 17. Before, when using the GCC executable, we disabled the breakpoint on 5.2 in order to disable the breakpoint on the outer scope. But, when using an LLVM executable, we instead have to disable the breakpoint on 5.1 instead. I.e.:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5.1 (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 n 0x00000000004011f7 in make_function(int&amp;) at lambda.cc:17 5.2 y 0x000000000040195f in make_function(int&amp;)::$_0::operator()(int) const at lambda.cc:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This could also happen with some past or future version of the GNU compiler. You shouldn't assume that you know the order of the locations, but should instead use the &lt;code&gt;info breakpoints&lt;/code&gt; command to figure out which breakpoint location to disable or even delete.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article has discussed several problems that might encountered when using GDB to debug function objects. These problems include inadvertently stepping into constructors as well as the related problem of stepping into function object invocation code. In order to avoid the latter problem, it is best to place a breakpoint in the target function. This can be done by name, assuming we know the name, but in the case of lambda expressions, there is no name, so the breakpoint must be placed via line number.&lt;/p&gt; &lt;p&gt;When placing a breakpoint on a line containing a lambda, it's frequently the case that the breakpoint will be placed at multiple locations. The &lt;code&gt;info breakpoints&lt;/code&gt; command can be used to determine which location is that of the containing function and which is the lambda; once this is determined, GDB's &lt;code&gt;disable&lt;/code&gt; command can be used to cause GDB to not stop at one of those locations.&lt;/p&gt; &lt;p&gt;Finally, a helper breakpoint with associated commands can be used to re-enable a breakpoint that's been disabled. The combination of first disabling a breakpoint on a lambda and then using a helper breakpoint to re-enable the lambda's breakpoint is useful for targeting a specific invocation of a lambda expression or other function object.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/03/how-debug-c-lambda-expressions-gdb" title="How to debug C++ lambda expressions with GDB"&gt;How to debug C++ lambda expressions with GDB&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2023-05-03T07:00:00Z</dc:date></entry></feed>
