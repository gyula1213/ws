<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to set up event-driven microservices using Knative Eventing</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/02/how-set-event-driven-microservices-using-knative-eventing" /><author><name>Matthias Wessendorf</name></author><id>0244c1a3-a6bf-4876-9c57-26b9ab99e820</id><updated>2023-05-02T07:00:00Z</updated><published>2023-05-02T07:00:00Z</published><summary type="html">&lt;p&gt;Many modern application designs are &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt;, aiming to deliver events quickly. This article describes how to orchestrate event-driven &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; using standards like CNCF CloudEvents and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; APIs for Knative to simplify EDA-style application development.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://www.redhat.com/en/topics/integration/what-is-event-driven-architecture"&gt;Event Driven Architecture (EDA)&lt;/a&gt; allows the implementation of loosely coupled applications and services. In this model, event producers do not know for which event consumers are listening, and the event itself does not know the consequences of its occurrence. EDA is a good option for distributed application architectures.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates an example of an event-driven application consisting of three services, producing and consuming different events with an event bus responsible for the orchestration and routing of the events. Note that “Service 3” produces an event indicating the business process finished, but there is no consumer for the event.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-02-27_18-13-39.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-02-27_18-13-39.png?itok=Z1ss7cix" width="600" height="515" alt="Illustration of an event-driven application consisting of three services, producing and consuming different events." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: An event-driven application consisting of three services, producing and consuming different events.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It is simple to implement another consumer for the “Finish Event” with the flexible architecture of event-driven systems.&lt;/p&gt; &lt;h2&gt;Knative Eventing&lt;/h2&gt; &lt;p&gt;How can you build a system on Kubernetes that orchestrates events and routes them to consumers? Luckily there is &lt;a href="https://knative.dev/docs/eventing"&gt;Knative Eventing&lt;/a&gt;, which offers a collection of APIs that enable cloud native developers to use an event-driven architecture within their applications and services. You can use these APIs to create components that route events from event producers to event consumers, known as sinks, that receive events.&lt;/p&gt; &lt;p&gt;Knative Eventing uses standard HTTP requests to send and receive events between event producers and sinks. These events conform to a CNCF industry standard called &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt;, which enables creating, parsing, sending, and receiving events in any programming language. The binding between the HTTP protocol and CloudEvents is standardized in this &lt;a href="https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/http-protocol-binding.md"&gt;specification&lt;/a&gt;. Although the focus of this article is on HTTP, it is worth mentioning that the CloudEvents specification also describes bindings for &lt;a href="https://github.com/cloudevents/spec/tree/v1.0.2/cloudevents/bindings"&gt;other protocols&lt;/a&gt;, such as &lt;a href="http://docs.oasis-open.org/amqp/core/v1.0/os/amqp-core-overview-v1.0-os.html"&gt;AMQP&lt;/a&gt; or &lt;a href="https://datatracker.ietf.org/doc/html/rfc6455"&gt;Websocket&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Event mesh with Knative Broker&lt;/h2&gt; &lt;p&gt;One of the key APIs in Knative Eventing is the &lt;a href="https://knative.dev/docs/eventing/brokers"&gt;Knative Broker API&lt;/a&gt;, which defines an &lt;a href="https://knative.dev/docs/eventing/event-mesh/"&gt;event mesh&lt;/a&gt; aiding the event orchestration and routing. Figure 2 shows a complete process, covering purchases from an online web shop and ends when the order is completely delivered at the customer's door. The process is implemented by a couple of microservice applications that are consuming and producing events. There is no direct communication or invocation between the services. Instead, the applications are loosely coupled and communicate only via events.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%20from%202023-03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%20from%202023-03.png?itok=Uj_tWe6i" width="600" height="272" alt="An illustration of the process flow of an event-driven application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The complete process flow of an event-driven application.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The orchestration of the event exchange is handled by an event mesh. In our case, this is the Knative Broker for &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: eventing.knative.dev/v1 kind: Broker metadata:  annotations:    eventing.knative.dev/broker.class: Kafka  name: order-broker spec:  config:    apiVersion: v1    kind: ConfigMap    name: order-broker-config --- apiVersion: v1 kind: ConfigMap metadata:  name: order-broker-config data:  bootstrap.servers: &lt;url&gt;  auth.secret.ref.name: &lt;optional-secret-name&gt;  default.topic.partitions: "10"  default.topic.replication.factor: "3"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The broker is annotated to pick the Kafka-backed implementation and points to a ConfigMap holding configuration about the Apache Kafka Topic, internally used by the broker. Generally, it is also recommended to configure aspects of delivery guarantees and retries. But we skipped this to keep this article simple. For more details on best practices for Knative Broker configurations, read the article, &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka"&gt;Our advice for configuring Knative Broker for Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After the Broker definition has been applied with &lt;code&gt;oc apply&lt;/code&gt;, you can check for the broker and its status as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get brokers.eventing.knative.dev -n orders NAME           URL                                                                                 AGE   READY order-broker   http://kafka-broker-ingress.knative-eventing.svc.cluster.local/orders/order-broker   46m   True&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Every Knative Broker object exposes an HTTP endpoint acting as the Ingress for CloudEvents. The URL can be found on the status of each broker object. The following is an example of an HTTP POST request that could be sent from the “Online Shop” service to the event mesh, aka the Knative Kafka Broker.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -v -X POST \ -H "content-type: application/json" \ -H "ce-specversion: 1.0" \ -H "ce-source: /online/shop" \ -H "ce-type: order.requested" \ -H "ce-id: 1f1380d4-8ff2-4ab0-b2ba-54811226c21b" \ -d '{"customerId": 20207-19, "orderId": "f8bc3445-b844"}' \ http://kafka-broker-ingress.knative-eventing.svc.cluster.local/orders/order-broker&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But how is the event getting delivered to the “Payment Service”, since there is no direct coupling between the two?&lt;/p&gt; &lt;h2&gt;Event orchestration and routing&lt;/h2&gt; &lt;p&gt;While the Broker API implements an event mesh, it goes hand-in-hand with the trigger API, which the broker is using to route messages based on a given set of rules or criteria to their destination.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: trigger-order-requested spec: broker: order-broker filter: attributes: type: order.requested source: /online/shop subscriber: ref: apiVersion:v1 kind: Service name: payment&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example is a trigger for the “order-broker” which contains two filters, each for different CloudEvent attributes metadata:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Type&lt;/li&gt; &lt;li aria-level="1"&gt;Source&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Both rules are treated as an AND, and the event is only routed to the referenced payment service by the Knative Broker if both rules match.&lt;/p&gt; &lt;p&gt;The routing of the matching CloudEvent to the referenced Kubernetes Service (payment) is done by the Knative Broker using HTTP. This allows a flexible architecture for the implementation of the processing services since simply all that is needed is a Web Server program, regardless of the written language. Besides the Kubernetes Service API, we can also reference a &lt;a href="https://knative.dev/docs/serving/"&gt;Knative Serving Service&lt;/a&gt;, supporting serverless principals.&lt;/p&gt; &lt;p&gt;If the referenced service replies with a CloudEvent in its HTTP response, this event is returned back to the Knative Broker and available for further processing. Using a different trigger with a matching rule can route those events to other service applications.&lt;/p&gt; &lt;h2&gt;CloudEvent processing with Knative Functions&lt;/h2&gt; &lt;p&gt;One simple way to create microservices that are processing standard CloudEvents is to leverage the &lt;a href="https://knative.dev/docs/functions/"&gt;Knative Functions&lt;/a&gt; project. It contains templates for a number of languages and platforms, such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; (&lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt;)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; (&lt;a href="https://developers.redhat.com/node/219015"&gt;Java&lt;/a&gt;)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt; (Java)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following script is the implementation of the “Payment Service” application, which was written based on the &lt;a href="https://quarkus.io/guides/funqy"&gt;Quarkus Funqy&lt;/a&gt; template. Funqy is part of Quarkus’s serverless strategy that provides a portable Java API for developers to write serverless functions and deploy them to heterogeneous serverless runtimes, including AWS Lambda, Azure Functions, Google Cloud, and Knative. With Funqy, developers can easily bind their methods to CloudEvents, using the @Funq annotation. Funqy ensures that the @Funq annotated Java method is invoked with the HTTP request from the Knative Broker, containing the “OrderRequested” CloudEvent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The OrderRequests object is part of the CloudEvent payload in a serialized JSON format, as indicated by the previous cURL example.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@Funq public CloudEvent&lt;PaymentReceived&gt; orderRequest(final CloudEvent&lt;OrderRequested&gt; order) { LOG.debug("Incoming CloudEvent with ID: " + order.id()); try { final PaymentReceived payment = paymentProvider.processPayment(order.data()); return CloudEventBuilder.create() .id(UUID.randomUUID().toString()) .type("payment.received") .source("/payment") .build(payment); } catch (InvalidPaymentException ipe) { // recover from here return CloudEventBuilder.create().build(...); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The CloudEvent payload is deserialized to the “OrderRequest” type, using the &lt;code&gt;data()&lt;/code&gt; method from the CloudEvents API and processed by a payment provider service. Once the payment is approved, the Knative Function code returns a different CloudEvent with type &lt;strong&gt;payment.received&lt;/strong&gt;, indicating the payment has been received.&lt;/p&gt; &lt;p&gt;Let’s have a look at the diagram in Figure 2 where the “Order Service” subscribed to the “payment.received” event. Whenever such an event is available in the event mesh, the Knative Broker will dispatch it to the subscribed “Order Service” and continue the process of the shopping cart application.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In case of a failure, we see the &lt;strong&gt;InvalidPaymentException&lt;/strong&gt; and a different CloudEvent with an error type returned to the broker, indicating that a failure has occurred. For more information on how to configure the Knative Broker for delivery guarantees and retries, please refer to the previously mentioned &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka"&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Using application-specific events&lt;/h3&gt; &lt;p&gt;When working with event-driven microservices, it is highly recommended that every service or function should respond to incoming requests with an outgoing event on its HTTP response. The CloudEvents should be domain-specific and provide context about their state on the CloudEvent metadata attributes. It is very important to not return the same CloudEvent type that goes into a function because this would cause a filter loop on the executing Knative Broker. For successful event processing and domain-specific failures, a service should always return a CloudEvent to the Knative Broker.&lt;/p&gt; &lt;p&gt;To handle network-level failures occurring while the Knative Broker tries to deliver the CloudEvents (such as HTTP 4xx/5xx errors), we recommend configuring a Dead-Letter-Sink for improved delivery guarantees.&lt;/p&gt; &lt;h2&gt;Knative Eventing simplifies event-driven microservices&lt;/h2&gt; &lt;p&gt;The article described how the Knative Eventing Broker and Trigger APIs help to orchestrate event-driven microservices. The architecture is based on standardized Kubernetes APIs for Knative and CNCF CloudEvents. We also discussed how the implementation of EDA-style applications are loosely coupled and how to implement a simple routing approach for events using Knative Eventing. Leveraging industry standards is a good investment for any application architecture. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/02/how-set-event-driven-microservices-using-knative-eventing" title="How to set up event-driven microservices using Knative Eventing"&gt;How to set up event-driven microservices using Knative Eventing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Matthias Wessendorf</dc:creator><dc:date>2023-05-02T07:00:00Z</dc:date></entry><entry><title>Why service mesh and API management are better together</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/01/why-service-mesh-and-api-management-are-better-together" /><author><name>Vamsi Ravula</name></author><id>2319ff1a-48e5-4872-82b7-a2bf8e4b32dc</id><updated>2023-05-01T07:00:00Z</updated><published>2023-05-01T07:00:00Z</published><summary type="html">&lt;p&gt;When it comes to managing &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;, the possibilities are truly endless with &lt;a href="https://developers.redhat.com/topics/service-mesh/"&gt;service mesh&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API management&lt;/a&gt;. You might think service mesh and API management are competing technologies—however, they are actually complementary and can help revolutionize the way you manage and secure your microservices and APIs.&lt;/p&gt; &lt;p&gt;In this article, we will explore the benefits of using service mesh and API management together, using a fictitious travel company as an example. &lt;/p&gt; &lt;h2&gt;Service mesh versus API management&lt;/h2&gt; &lt;p&gt;Service mesh is a dedicated infrastructure layer that gives you the power to manage and control communication between microservices, with features like traffic management, service discovery, load balancing, and security.&lt;/p&gt; &lt;p&gt;API management refers to the process for creating, publishing, and managing APIs that connect applications and data across the enterprise and across clouds. This approach lets you control access to your APIs with authentication, authorization, rate limiting, and monitoring.&lt;/p&gt; &lt;p&gt;Together, service mesh and API management can improve the reliability, scalability, security, and performance of your microservices and APIs, as we'll see in the following scenarios.&lt;/p&gt; &lt;h2&gt;Example scenario&lt;/h2&gt; &lt;p&gt;We will use a fictitious travel company, Travelz, to demonstrate the implementation of both service mesh and API management in different scenarios. The subsequent sections will highlight the company's challenges and goals and demonstrate how we can combine these technologies to achieve those goals.&lt;/p&gt; &lt;h3&gt;About the Travelz application&lt;/h3&gt; &lt;p&gt;The travel booking system has two business units that are responsible for different aspects of the travel experience. First is the travel portal business unit that offers multiple travel shops where you can easily search and book your travel.&lt;/p&gt; &lt;p&gt;The second business unit is the travel agency that hosts a set of services such as flights, cars, hotels, etc., designed to provide the travel quotes. The travel portal business unit makes calls to the travel agency services and APIs to provide the quotes.&lt;/p&gt; &lt;h3&gt;Observability, traffic monitoring, and security&lt;/h3&gt; &lt;p&gt;With the growing number of microservices, the team needed a way to manage, secure, and control how the services talk to each other. To connect, manage, and observe the microservices, and to control traffic between services, Travelz decided to adopt Istio-based &lt;a href="https://developers.redhat.com/topics/service-mesh"&gt;Red Hat OpenShift Service Mesh&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;With OpenShift Service Mesh, the team was able to provide the necessary service-to-service capabilities—traffic monitoring, access control, discovery, security, resiliency, metrics, and more—without requiring changes to the code of any of the app’s microservices. The architecture is shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed-4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed-4.png?itok=7kPVYUh-" width="600" height="286" alt="A diagram of access via portals versus agency." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Travel agency microservices are managed and monitored using Red Hat OpenShift Service Mesh.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Traffic routing and securing the APIs for external consumption&lt;/h3&gt; &lt;p&gt;Travelz tourism becomes super popular, and now other travel portals want to partner with Travelz. The team aims to enforce these principles in their technology:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Allow secure access to internal services to external partners and clients as APIs.&lt;/li&gt; &lt;li aria-level="1"&gt;APIs should be easy to find, understand, integrate with and adopt.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Travelz builds a new version of their travels service (v2), so their travel partners will access v2 while the internal platforms access v1.&lt;/p&gt; &lt;p&gt;Service mesh's intelligent traffic routing capabilities make it extremely easy to divert the traffic based on who is making the call (internal or external). Travelz IT leveraged service mesh's virtual service capability to achieve this. A &lt;strong&gt;virtual service&lt;/strong&gt; defines a set of traffic routing rules and each routing rule defines matching criteria for the traffic. If the traffic is matched, then it is sent to a named destination service (or subset/version of it). Travelz IT built two different dedicated virtual services: one for external partners and the other for internal portals.&lt;/p&gt; &lt;h3&gt;Managing external access&lt;/h3&gt; &lt;p&gt;To manage access by the external partners, Travelz introduces the &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management &lt;/a&gt;platform. They adopt a contract-first approach by creating OpenAPI specifications for their existing and new services before onboarding external clients. With the help of the &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management platform&lt;/a&gt;, Travelz IT now manages partner access to their APIs in such a way that the partners can only access APIs that are protected by a &lt;code&gt;user_key&lt;/code&gt; (see Figure 2). 3scale provides a developer portal out of the box for partner developers to discover, learn, test, and sign up for those APIs.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed-3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed-3.png?itok=zvs83zEB" width="600" height="334" alt="Diagram showing the company manages partner access to their APIs using 3scale." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The company can now securely manage partner access to their APIs using Red Hat 3scale API Management platform.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Before we jump into the next scenario, let’s talk about the concept of domain boundaries. This will help you understand and appreciate the next scenario better.&lt;/p&gt; &lt;h3&gt;Domain boundaries&lt;/h3&gt; &lt;p&gt;As we saw in the earlier two scenarios, traffic direction provides a straightforward guide for when to choose API management or service mesh solutions. It is almost tempting to say you should always choose API management for north-south (external) traffic and service mesh for east-west (internal) traffic.&lt;/p&gt; &lt;p&gt;However, most organizations are not so simple. Typical organizations contain multiple groups that create and manage their own services and interact with other teams and external parties. &lt;strong&gt;Domain boundaries &lt;/strong&gt;can help you divide your organization into smaller, more manageable areas. Much as your enterprise boundary denotes the perimeter of your overall organization, domain boundaries designate the perimeters of groups within your organization. See Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed.png?itok=LByQC6KL" width="600" height="516" alt="Diagram showing enterprise boundaries for a group of applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Using domain boundaries within your organization.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The same north-south and east-west traffic patterns that occur in relation to your enterprise boundary also apply to domain boundaries within your organization. As a result, you should generally choose API management for interdomain traffic and service meshes for intradomain traffic.&lt;/p&gt; &lt;p&gt;Figure 4 will help you better understand the nuances of interdomain versus intradomain traffic patterns.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed%20%281%29.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed%20%281%29.png?itok=CsDbBdMN" width="600" height="230" alt="Diagram of interdomain traffic versus intradomain traffic patterns." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Interdomain versus intradomain traffic patterns.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Interdomain traffic&lt;/strong&gt; crosses domain or enterprise boundaries to connect services with consumers beyond your group or team. Interdomain traffic follows north-south traffic patterns.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Hierarchical 1:N connection structures&lt;/li&gt; &lt;li&gt;Separate service providers and service consumers&lt;/li&gt; &lt;li&gt;Requires authorization and authentication&lt;/li&gt; &lt;li&gt;Formal use contracts needed&lt;/li&gt; &lt;li&gt;Guided service discovery, accessible developer portal, and formal documentation&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Intradomain traffic &lt;/strong&gt;stays within domain and enterprise boundaries to link individual microservices. Intradomain traffic typically follows east-west traffic patterns.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Non-hierarchical 1:1 connection structures&lt;/li&gt; &lt;li&gt;Service providers and consumers within the same team&lt;/li&gt; &lt;li&gt;Authentication required&lt;/li&gt; &lt;li&gt;Implicit or informal contracts, if any&lt;/li&gt; &lt;li&gt;Internal documentation within code&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This is a great e-book if you are interested in more detailed information on this topic: &lt;a href="https://www.redhat.com/en/resources/service-mesh-and-api-management-e-book"&gt;Service mesh or API management? &lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Standardizing API access for internal consumption and collaboration&lt;/h3&gt; &lt;p&gt;With the above context, let’s see how that applies in our example. Travelz IT wants to standardize access of their core agency services for the internal platforms (think interdomain traffic), similar to how they did for external partners. Because all the services are already a part of Red Hat OpenShift Service Mesh, they leverage the seamless integration between &lt;a href="https://developers.redhat.com/topics/service-mesh"&gt;OpenShift Service Mesh&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API management&lt;/a&gt; for this use case.&lt;/p&gt; &lt;p&gt;The WebAssembly extension that enables this is deployed as a sidecar to the microservices, and it communicates directly with the 3scale API manager. It eases the integration of OpenShift Service Mesh and 3scale, and authorizes HTTP requests made to 3scale. It provides a standard way to inject 3scale API Management configurations into OpenShift Service Mesh for execution in a single data plane. In this scenario, service mesh serves as the data plane and 3scale serves as the control plane, eliminating the need for an additional gateway and reducing latency due to the reduced number of hops. See Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed-3_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed-3_0.png?itok=ajyowzGR" width="600" height="334" alt="Diagram of 3scale and service mesh integration to secure internal traffic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Simplified integration between 3scale and OpenShift Service Mesh to secure internal traffic. &lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Try this architecture on your own&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/red-hat-architecture-and-design-patterns"&gt;Red Hat solution patterns&lt;/a&gt; provide a solid starting point for those seeking a specific reference architecture to replicate or use as inspiration for technical decision making. These patterns offer guidance on designing, developing, integrating, automating, and delivering cloud-native applications.&lt;/p&gt; &lt;p&gt;The Travelz example is one &lt;a href="https://redhat-solution-patterns.github.io/solution-pattern-apim-servicemesh/comprehensive-service-architecture/"&gt;such pattern that specifically&lt;/a&gt; examines the relationship between service mesh and API management to help you understand how to use these solutions together to establish a comprehensive service management architecture.&lt;/p&gt; &lt;p&gt;Interested in trying this on your own? Red Hat Developer's &lt;a href="https://redhat-solution-patterns.github.io/solution-pattern-apim-servicemesh/comprehensive-service-architecture/03-demo.html"&gt;solutions patterns topic page&lt;/a&gt; provides necessary scripts and detailed steps to see the architecture in action.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Service mesh and API management are both powerful tools that can bring significant benefits to your microservices architecture. When used together, they can provide a comprehensive solution for managing and securing your microservices and APIs, improving security, performance, observability, and developer experience. By using both, your organization can have a more holistic view of its microservices and API usage, which can help you make better decisions for future scaling and security.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/01/why-service-mesh-and-api-management-are-better-together" title="Why service mesh and API management are better together"&gt;Why service mesh and API management are better together&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Vamsi Ravula</dc:creator><dc:date>2023-05-01T07:00:00Z</dc:date></entry><entry><title type="html">Monitoring Made Easy: How to Use Micrometer API</title><link rel="alternate" href="https://www.mastertheboss.com/eclipse/eclipse-microservices/monitoring-made-easy-how-to-use-micrometer-api/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/eclipse/eclipse-microservices/monitoring-made-easy-how-to-use-micrometer-api/</id><updated>2023-04-28T14:42:58Z</updated><content type="html">Micrometer is a metrics instrumentation library for Java applications. It provides a simple facade over the instrumentation clients for a number of popular monitoring systems. In this tutorial, we will learn how to use Micrometer API in a Jakarta EE application with WildFly or a Quarkus application. Firstly, we will cover the Micrometer configuration using ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Support for TLS/SSL in TCP</title><link rel="alternate" href="http://belaban.blogspot.com/2023/04/support-for-tlsssl-in-tcp.html" /><author><name>Bela Ban</name></author><id>http://belaban.blogspot.com/2023/04/support-for-tlsssl-in-tcp.html</id><updated>2023-04-28T07:37:00Z</updated><content type="html">In version 5.2.15 (to be released soon), TLS can be enabled in TCP via a simple configuration change: &lt;TCP      tls.enabled="true"      tls.client_auth="NEED"      tls.keystore_path="good-server.jks"      tls.keystore_password="password"      tls.keystore_alias="server" ... /&gt; This installs an SSLSocketFactory into TCP, creating SSLSockets instead of Sockets and SSLServerSockets instead of ServerSockets.   This is an alternative to SYM_ENCRYPT.   Details can be found in [1].   Cheers, [1]</content><dc:creator>Bela Ban</dc:creator></entry><entry><title>How to add public Ingress to a PrivateLink ROSA cluster</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/27/how-add-public-ingress-private-link-rosa-cluster" /><author><name>Suresh Gaikwad</name></author><id>fb947412-f1ec-40f9-8910-2b5dec129b87</id><updated>2023-04-27T07:00:00Z</updated><published>2023-04-27T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to expose applications to the internet by deploying in a PrivateLink &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift Service on AWS&lt;/a&gt; (ROSA) cluster within a truly private &lt;a href="https://aws.amazon.com/vpc/"&gt;Virtual Private Cloud &lt;/a&gt;(VPC) that doesn’t have a network address translation (NAT) gateway or an internet gateway attached to it. We will be using a single VPC for Ingress and Egress traffic. However, you might choose to have separate VPCs for Ingress and Egress traffic to provide more security control of this traffic.&lt;/p&gt; &lt;p&gt;The cluster used in this article has been installed as per the architecture diagram detailed in Figure 1 of the previous article, &lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt;. There are &lt;a href="https://github.com/aws-samples/rosa-patterns/blob/main/templates/cloudformation/privatelink/README.md"&gt;cloud formation templates&lt;/a&gt; to automate the cluster deployment as per the architecture discussed in that previous article.&lt;/p&gt; &lt;p&gt;To expose the applications to the internet, we will use a custom domain operator. The Custom Domains Operator sets up a new Ingress controller with a custom certificate as a day-2 operation. It will create an additional private network load balancer in the ROSA private VPC. We will create an additional network load balancer in the egress VPC and then use the network load balancer created in the ROSA VPC as its target. Additionally, we will create CloudFront distribution for content delivery and WAF to protect web applications by filtering and monitoring HTTP traffic between a web application and the internet. We will use the AWS network firewall for fine-grained control over network traffic.&lt;/p&gt; &lt;p&gt;In this example, we will use a single availability zone for the network firewall in the egress VPC (Figure 1). However, it is strongly recommended that a production cluster uses multiple availability zones to minimize the potential for outages.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_0.jpg?itok=cZ6eJ9gu" width="1179" height="642" alt="A diagram of the OpenShift Service on AWS architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: OpenShift Service on AWS architecture to expose applications deployed in truly private VPC to the Internet in a secure way.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The network details used in this architecture are:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Egress/Ingress VPC &lt;strong&gt;(10.0.0.0/16)&lt;/strong&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;Egress public subnet &lt;strong&gt;(10.0.128.0/19)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Egress public subnet &lt;strong&gt;(10.0.0.0/17)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Firewall subnet &lt;strong&gt;(10.0.192.0/27)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;ROSA VPC &lt;strong&gt;(10.1.0.0/16)&lt;/strong&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;ROSA subnet 1 &lt;strong&gt;(10.1.0.0/18)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;ROSA subnet 2 &lt;strong&gt;(10.1.64.0/18)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;ROSA subnet 3 &lt;strong&gt;(10.1.128.0/17)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The AWS network firewall will be created in the firewall subnet which will specify the VPC endpoint in the route table entries shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rt_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rt_0.jpg?itok=1wiRJDvO" width="1030" height="702" alt="Route tables created to route the traffic to the internet." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Route tables created to route the traffic to the Internet.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Log in to the jump host from where you have access to the ROSA cluster.&lt;/p&gt; &lt;h2&gt;Configure custom domain for applications&lt;/h2&gt; &lt;p&gt;To configure a custom domain for applications, get the certificates for your custom domain. If you don’t have one, you can generate the certificates using &lt;code&gt;certbot&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project custom-domain $ EMAIL=testuser@gmail.com $ DOMAIN=sgaikwad.mobb.ninja $ certbot certonly --manual --preferred-challenges=dns --email $EMAIL --server https://acme-v02.api.letsencrypt.org/directory --agree-tos --manual-public-ip-logging-ok -d "*.$DOMAIN"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Follow the instructions to create a TXT record in route53. This will create certs in &lt;code&gt;/etc/letsencrypt/live/sgaikwad.mobb.cloud-*&lt;/code&gt; directory. Specify the exact path as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ CERTS=/etc/letsencrypt/live/sgaikwad.mobb.cloud-* $ oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem $ cat cr.yaml apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: sgaikwad.mobb.cloud scope: Internal loadBalancerType: NLB certificate: name: acme-tls namespace: custom-domain $ oc apply -f cr.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait until customdomain has endpoints.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ watch oc get customdomains&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a network load balancer in the ROSA VPC. Let's create a network load balancer in Egress/Ingress VPC now.&lt;/p&gt; &lt;p&gt;Create a target group as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export ROSA_EGRESS_VPC=&lt;VpcId_for_Egress_VPC&gt; $ export TG_NAME=rosa-tg1 $ export ROSA_EGRESS_VPC=`aws ec2 describe-vpcs --filters Name=tag:Name,Values=rosa-egress-vpc |jq -r.Vpcs[0].VpcId` $ export TG_ARN=$(aws elbv2 create-target-group --name test --protocol TCP --port 80 --vpc-id $ROSA_EGRESS_VPC --target-type ip --ip-address-type ipv4 --query 'TargetGroups[0].TargetGroupArn' --output text)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the &lt;code&gt;target_Id&lt;/code&gt; from your OpenShift Service on AWS cluster, enter the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export NLB_PRIVATE_IP=`nslookup $(oc get customdomains |awk '{print $2}' |tail -1) |grep Address: |grep -v '#' |awk '{print $2}'` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Register targets for your &lt;strong&gt;Target Group&lt;/strong&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws elbv2 register-targets --target-group-arn $TG_ARN --targets AvailabilityZone=all,Id=$NLB_PRIVATE_IP,Port=80 $ aws elbv2 register-targets --target-group-arn $TG_ARN --targets AvailabilityZone=all,Id=$NLB_PRIVATE_IP,Port=443&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create a network load balancer in Egress/Ingress VPC by grabbing the public subnet IDs from the VPC connected to the internet and map it while creating the load balancer as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export SUBNET_ID=`aws ec2 describe-subnets --filters "Name=vpc-id,Values=$ROSA_EGRESS_VPC" --filters "Name=tag:Name,Values=rosa-egress-public-subnet" --query 'Subnets[*].SubnetId' --output text` $ export NLB_ARN=`aws elbv2 create-load-balancer --name sgaikwad-rosa-nlb --type network --scheme internet-facing --subnet-mappings SubnetId=$SUBNET_ID --query 'LoadBalancers[0].LoadBalancerArn' --output text` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's create a listener and default action as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws elbv2 wait load-balancer-available --load-balancer-arns $NLB_ARN &amp;&amp; export NLB_LISTENER=$(aws elbv2 create-listener --load-balancer-arn ${NLB_ARN} --port 80 --protocol TCP --default-actions Type=forward,TargetGroupArn=${TG_ARN} --query 'Listeners[0].ListenerArn' --output text) $ aws elbv2 wait load-balancer-available --load-balancer-arns $NLB_ARN &amp;&amp; export NLB_LISTENER=$(aws elbv2 create-listener --load-balancer-arn ${NLB_ARN} --port 443 --protocol TCP --default-actions Type=forward,TargetGroupArn=${TG_ARN} --query 'Listeners[0].ListenerArn' --output text)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add CNAME record in route53 to point to the newly created internet-facing NLB by grabbing the DNS name for the newly created NLB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export NLB_DNS_NAME=`aws elbv2 describe-load-balancers --load-balancer-arns $NLB_ARN --query 'LoadBalancers[0].DNSName' --output text`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go to route53 and select your domain by following these steps. (In my case, it’s mobb.cloud.)&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on &lt;strong&gt;Create Record&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Under record name, specify your domain:  &lt;code&gt;*.sgaikwad.mobb.cloud&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;CNAME&lt;/strong&gt; for record type.&lt;/li&gt; &lt;li aria-level="1"&gt;In the value field, enter the value: &lt;code&gt;$NLB_DNS_NAME&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Create Record&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;How to implement AWS network firewall&lt;/h2&gt; &lt;p&gt;With the network firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic moving to and from an internet gateway, NAT gateway, or over a VPN or AWS Direct Connect. To create a firewall via the console, you need to create a separate subnet where you want to deploy the AWS network firewall. We recommend creating a separate subnet for the firewall because the AWS network firewall can’t inspect the packets originating and targeting from the same subnet. Hence, if you have any other EC2 instances running in the same subnet, the packets won’t be inspected by the AWS network firewall.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EGRESS_FIREWALL_SUBNET=`aws ec2 create-subnet --vpc-id $ROSA_EGRESS_VPC --cidr-block 10.0.192.0/27 | jq -r.Subnet.SubnetId` $ aws ec2 create-tags --resources $EGRESS_FIREWALL_SUBNET --tags Key=Name,Value=firewall-subnet&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a firewall rule group&lt;/h3&gt; &lt;p&gt;Create a firewall rule group that defines what actions to perform on the packets which will be inspected by the firewall. In this example, we will create a stateless firewall rule group.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Sign in to the AWS Management Console and open the Amazon VPC console at&lt;a href="https://console.aws.amazon.com/vpc/"&gt; https://console.aws.amazon.com/vpc/&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the navigation pane, under &lt;strong&gt;Network Firewall&lt;/strong&gt;, choose &lt;strong&gt;Network Firewall rule groups&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Create Network Firewall rule group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;On the &lt;strong&gt;Create Network Firewall rule group&lt;/strong&gt; page, for the &lt;strong&gt;Rule group type&lt;/strong&gt;, choose &lt;strong&gt;Stateless rule group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name and description for the rule group. You'll use these to identify the rule group when you manage and use it.&lt;/li&gt; &lt;li aria-level="1"&gt;For &lt;strong&gt;Capacity&lt;/strong&gt;, set the maximum capacity you want to allow for the stateless rule group, up to the maximum of 30,000. You can't change this setting after you create the rule group. For information about how to calculate this, refer to:&lt;a href="https://docs.aws.amazon.com/network-firewall/latest/developerguide/rule-group-managing.html#nwfw-rule-group-capacity"&gt; Setting rule group capacity in AWS Network Firewall&lt;/a&gt;. Set the capacity to &lt;strong&gt;10.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Review the rules that you want to add to the stateless rule group. Click on &lt;strong&gt;Add rule&lt;/strong&gt;. We will add a rule to allow SSH traffic from your IP and block it for all others as an example. &lt;ol&gt;&lt;li aria-level="2"&gt;Specify the priority to &lt;strong&gt;1&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Choose &lt;strong&gt;TCP and UDP&lt;/strong&gt; under protocol.&lt;/li&gt; &lt;li aria-level="2"&gt;Under source, select any IPV4 address and enter your public IP address. For source port range, select &lt;strong&gt;Any&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Custom&lt;/strong&gt; in the destination field and specify 0.0.0.0/0 for destination. Specify port 22 in the destination port range.&lt;/li&gt; &lt;li aria-level="2"&gt;Under actions, select &lt;strong&gt;Pass and create the rule&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Repeat the above steps to block SSH traffic to everyone else. Specify priority 2 and 0.0.0.0/0 in the source field and &lt;strong&gt;Drop&lt;/strong&gt; in the actions field.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Review the settings for the rule group, then choose &lt;strong&gt;Create stateless rule group&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create a firewall policy&lt;/h3&gt; &lt;p&gt;Create a firewall policy by following these steps:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Sign in to the AWS Management Console and open the Amazon VPC console at&lt;a href="https://console.aws.amazon.com/vpc/"&gt; https://console.aws.amazon.com/vpc/&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the navigation pane, under &lt;strong&gt;Network Firewall&lt;/strong&gt;, choose &lt;strong&gt;Firewall policies&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Create firewall policy.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a &lt;strong&gt;Name&lt;/strong&gt; to identify this firewall policy.&lt;/li&gt; &lt;li aria-level="1"&gt;For &lt;strong&gt;Stream exception policy&lt;/strong&gt;, choose how the network firewall handles traffic when a network connection breaks midstream. Network connections can break due to disruptions in external networks or within the firewall itself. Choose &lt;strong&gt;Continue&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Next&lt;/strong&gt; to go to the firewall policy's &lt;strong&gt;Add rule groups&lt;/strong&gt; page.&lt;/li&gt; &lt;li aria-level="1"&gt;Keep the settings at default and click on &lt;strong&gt;Add Stateless rule groups&lt;/strong&gt;. &lt;ol&gt;&lt;li aria-level="2"&gt;Select the rule you created earlier and click on &lt;strong&gt;Add rule Group&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Keep all other settings at default and create the firewall policy.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create the network firewall&lt;/h3&gt; &lt;p&gt;Once the firewall policy is created, create the network firewall&lt;strong&gt; &lt;/strong&gt;as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Sign in to the AWS Management Console and open the Amazon VPC console at&lt;a href="https://console.aws.amazon.com/vpc/"&gt; https://console.aws.amazon.com/vpc/&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;In the navigation pane, under &lt;strong&gt;Network Firewall&lt;/strong&gt;, choose &lt;strong&gt;Firewalls&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Create firewall&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name to identify this firewall.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose your VPC from the dropdown list. Here, choose &lt;strong&gt;ROSA Egress VPC&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;For &lt;strong&gt;Firewall subnets&lt;/strong&gt;, choose the availability zones and subnets that you want to use for your firewall endpoints. Select the newly created Firewall subnet from the list.&lt;/li&gt; &lt;li aria-level="1"&gt;For the &lt;strong&gt;Associated firewall policy&lt;/strong&gt; section, choose the firewall policy that you want to associate with the firewall. Select the firewall policy we created in the previous step.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;Create firewall&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Configure VPC route tables&lt;/h3&gt; &lt;p&gt;Now, configure your VPC route tables to send traffic through the firewall endpoints. For more information, refer to&lt;a href="https://docs.aws.amazon.com/network-firewall/latest/developerguide/vpc-config.html#vpc-config-route-tables"&gt; VPC route table configuration for AWS Network Firewall&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Create a new route table to route all the incoming and outgoing traffic through firewall. Under VPC, from the route tables page, click &lt;strong&gt;Create Route Table&lt;/strong&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Specify a name for your route table (i.e., FirewallRT).&lt;/li&gt; &lt;li aria-level="1"&gt;Select the Egress VPC and click on &lt;strong&gt;create Route table&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the newly created subnet and click on subnet associations from the bottom menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Edit subnet association&lt;/strong&gt; and select &lt;strong&gt;Firewall Subnet&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Then, click on the &lt;strong&gt;Routes&lt;/strong&gt; and add two routes as follows: &lt;ol&gt;&lt;li aria-level="2"&gt;For destination 0.0.0.0/0, the target would be the Internet gateway associated with Egress VPC.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.0.0.0/16, the target would be local.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Similarly, create a new route table for the internet gateway. Specify the name InternetGwRT.&lt;/li&gt; &lt;li aria-level="1"&gt;Select Egress VPC and click on &lt;strong&gt;create route table&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the InternetGwRT from the list and click on &lt;strong&gt;Edge Associations&lt;/strong&gt;. &lt;ol&gt;&lt;li aria-level="2"&gt;Attach the internet gateway by clicking on &lt;strong&gt;Edit Edge Associations&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the routes and add the following two routes: &lt;ol&gt;&lt;li aria-level="2"&gt;For destination 10.0.128.0/19, the target would be VPC endpoint which was created after creating the network firewall. Verify the VPC endpoint for the firewall from the endpoints section under VPC.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.0.0.0/16, the target would be local.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Now, select the rosa-egress-public-rt route table and verify the following three routes. If any routes are missing, create those. &lt;ol&gt;&lt;li aria-level="2"&gt;For destination 0.0.0.0/0, the target would be VPC endpoint which was created after creating the Network firewall.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.0.0.0/16, the target would be local.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.1.0.0/16, the target would be transit gateway (rosa-egress-tgw-attachment).&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create WAF and cloudfront distribution&lt;/h3&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create a WAF rule here:&lt;a href="https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2"&gt; https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2&lt;/a&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;Specify the name for your web ACL.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Amazon CloudFront distributions&lt;/strong&gt; in the resource type and click &lt;strong&gt;next&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Click on &lt;strong&gt;Add rules&lt;/strong&gt; and select &lt;strong&gt;Add Managed Rule groups&lt;/strong&gt; from the list.&lt;/li&gt; &lt;li aria-level="2"&gt;Use the core rule set and SQL injection rules from the free rule groups.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Review and Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Add a certificate to ACM: &lt;a href="https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/"&gt;https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/&lt;/a&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;Click on import and paste in the cert, key, certchain from the files which we have generated earlier for the custom domain. Click &lt;strong&gt;Review and Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Log into the&lt;a href="https://us-east-1.console.aws.amazon.com/cloudfront/v3/home#/distributions"&gt; AWS console and Create a Cloud Front distribution&lt;/a&gt; (make sure it's the same region as your cluster). &lt;ol&gt;&lt;li aria-level="2"&gt;Click on &lt;strong&gt;Create distribution&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;In &lt;strong&gt;Origin Domain Name&lt;/strong&gt;, select &lt;strong&gt;sgaikwad-rosa-nlb &lt;/strong&gt;(the network load balancer you created in Egress VPC).&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Origin Protocol Policy: HTTPS only&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;For &lt;strong&gt;Enable Origin Shield&lt;/strong&gt;, click on &lt;strong&gt;yes&lt;/strong&gt;. Specify &lt;strong&gt;ap-south-1&lt;/strong&gt; as origin shield region.&lt;/li&gt; &lt;li aria-level="2"&gt;Set the viewer protocol policy to &lt;strong&gt;Redirect HTTP to HTTPS&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Set the &lt;strong&gt;Allowed HTTP Methods&lt;/strong&gt; to &lt;strong&gt;GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Cache key and origin requests&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;for Cache Policy, select &lt;strong&gt;CachingDisabled&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;For the &lt;strong&gt;Origin request policy&lt;/strong&gt;, click on &lt;strong&gt;Create Policy &lt;/strong&gt;and specify the name for your policy.&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Origin request settings&lt;/strong&gt;, select &lt;strong&gt;Allow viewer headers, All query string and all cookies&lt;/strong&gt;. Click on &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;sgaikwad&lt;/strong&gt; in AWS WAF web ACL (the web ACL which we created in previous step).&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Alternate domain name (CNAME)&lt;/strong&gt;, click on &lt;strong&gt;Add item&lt;/strong&gt; and use &lt;strong&gt;*.sgaikwad.mobb.cloud&lt;/strong&gt; as a domain. Replace this with your domain.&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Custom SSL certificate&lt;/strong&gt;, select the domain you uploaded the certificate in ACM.&lt;/li&gt; &lt;li aria-level="2"&gt;Click on &lt;strong&gt;create distribution&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Under the details page for your distribution, check &lt;strong&gt;Distribution domain name&lt;/strong&gt; and copy it.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Modify the route53 CNAME record to point to the cloud front distribution instead of Egress NLB. &lt;ol&gt;&lt;li aria-level="2"&gt;Under services, go to Route53 hosted zones.&lt;/li&gt; &lt;li aria-level="2"&gt;Select your hosted zone. For me, it’s mobb.cloud.&lt;/li&gt; &lt;li aria-level="2"&gt;Select *.sgaikwad.mobb.cloud CNAME record, click on &lt;strong&gt;Edit&lt;/strong&gt; and change the value to the Cloudfront distribution domain name value that you copied in the previous step.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Deploy the application&lt;/h2&gt; &lt;p&gt;Now that we have configured all the components, we are all set to deploy the sample application in the ROSA cluster and access it from the internet by creating a route with the domain name.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project application $ oc new-app --docker-image=docker.io/openshift/hello-openshift $ oc create route edge --service=hello-openshift hello-openshift-tls --hostname hello-openshift.sgaikwad.mobb.cloud&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, open the browser from your local system and access the application with &lt;a href="https://hello-openshift.sgaikwad.mobb.cloud"&gt;https://hello-openshift.sgaikwad.mobb.cloud&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;If you have questions, please comment below. We welcome your feedback. Visit &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift Services on AWS&lt;/a&gt; (ROSA) for more information and resources.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/27/how-add-public-ingress-private-link-rosa-cluster" title="How to add public Ingress to a PrivateLink ROSA cluster"&gt;How to add public Ingress to a PrivateLink ROSA cluster&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Suresh Gaikwad</dc:creator><dc:date>2023-04-27T07:00:00Z</dc:date></entry><entry><title type="html">Using Dashbuilder with Google Spreadsheets</title><link rel="alternate" href="https://blog.kie.org/2023/04/using-dashbuilder-with-google-spreadsheets.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/04/using-dashbuilder-with-google-spreadsheets.html</id><updated>2023-04-26T17:04:45Z</updated><content type="html">can read from JSON and CSV contents. The only requirement is to make the file accessible to the machine where Dashbuilder is running. The issues you may face are: * : To solve CORS issue you need to use a web proxy to access the service or correctly configure CORS headers; * : To solve this you can have a backend with authentication setup or include headers in the dataset declaration - uuid: my_dataset   url: http://acme.org/myfile.csv       headers:   Authorization: Bearer {token} For Google spreadsheets we don’t have to worry about CORS or authentication as long as the document is published on the internet.  STEPS TO READ A GOOGLE SPREADSHEET  FROM DASHBUILDER * Publish the document on the internet: The document must be public on the internet, so make it sure that you publish it with the option Anyone with the link. Here’s how it looks like in Portuguese: * Now you need the sheet ID. It is what will allow us to get the same sheet output in CSV format. In our example here’s the ID: * Use the ID on this URL template. You can also specify a sheet name if you want In our example here’s the final URL: * Now you can use the sheet from Dashbuilder. Just remember that for CSVs the first row is skipped. Here’s a sample dashboard which you can use to get started: properties: sheet_id: 1XuyPTyrjMFXQ1ey6Bg9AEcrpwZ60CnLQVEs4-DEDrcc datasets:    - uuid: sheet      url: https://docs.google.com/spreadsheets/d/${sheet_id}/gviz/tq?tqx=out:csv pages:    - components:          - settings:                type: BARCHART                lookup:                    uuid: sheet    You can edit this same example using our . CONCLUSION Dashbuilder can easily integrate with Google Spreadsheet and other documents available on the internet. Stay tuned for more Dashbuilder tutorials and articles! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Quarkus 3.0, our new major release, is here!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-final-released/</id><updated>2023-04-26T00:00:00Z</updated><published>2023-04-26T00:00:00Z</published><summary type="html">Quarkus 3.0 is the result of a lot of work and dedication from the community. Quarkus continues to be an Open Source stack to write Java applications offering unparalleled startup time, memory footprint, and developer experience. The development of Quarkus 3 started last year on March 18, 2022, with the...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-04-26T00:00:00Z</dc:date></entry><entry><title>Optimize container images for NGINX and Apache HTTPd</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" /><author><name>Honza Horak</name></author><id>40488572-2eb5-4d73-bbf3-95f8210de674</id><updated>2023-04-25T07:00:00Z</updated><published>2023-04-25T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers/"&gt;Container&lt;/a&gt; image size matters. Let’s look at an experiment that reduced Apache HTTP and NGINX servers to micro container images. This article walks through the process we used to achieve the final result, plus how many megabytes (MB) this approach saved.&lt;/p&gt; &lt;p&gt;For this experiment, we used Fedora RPMs, but a similar approach should work in other operating systems or container images, as you see in the list of available images (there is an Apache HTTP server image that uses CentOS Stream 8 and 9 RPMs).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A shortcut:&lt;/strong&gt; If you are only interested in the result and want to try out the micro variant of Apache HTTP or NGINX server container images, check out the images in the following registries.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/httpd-24-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/nginx-122-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c8s&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c9s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The benefits of smaller container images&lt;/h2&gt; &lt;p&gt;First, let’s explain a bit more about the story behind those micro containers.&lt;/p&gt; &lt;p&gt;Container images include everything that a specific application needs except a &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel. That's the spirit of container technology. Here, our focus is on web servers, so in addition to the Apache HTTPd and NGINX server daemons, the container needs to include also libraries that those daemons use, necessary userspace components, etc.&lt;/p&gt; &lt;p&gt;Even in the year 2023 when Internet speeds are tremendous, the size of such containers is important. For example, it matters in an environment where the Internet speed is still very limited (have you heard about &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; running on satellites somewhere far in space?). It can help make the user experience more delightful (waiting dozens of seconds is not fun) or limit potential attack vectors because of unnecessary pieces of software in the container image that could work without them.&lt;/p&gt; &lt;p&gt;These are just a few reasons why developers want to make the container image as small as practically possible. Now let's review the steps we took to reduce the web server container image size to a minimum. Our experiments used Apache HTTP server 2.4 and NGINX server 1.22.&lt;/p&gt; &lt;h2&gt;Choosing binaries&lt;/h2&gt; &lt;p&gt;For these tech preview container images, we decided to use a Fedora 36 base image and therefore take RPMs from Fedora repositories. There are different attempts to make the container image small by compiling just the necessary pieces directly from the source, which results in a small image, but that’s not always a good idea.&lt;/p&gt; &lt;p&gt;Using packages from a distribution has a clear benefit—they are well-tested, maintained when there is a security issue, interact well with the rest of the operating system, and are proven to work well outside of the container, so we only need to focus on the container specifics.&lt;/p&gt; &lt;p&gt;You might think about removing files once they are installed as RPMs; this might make the image smaller, but it would be rather risky and a container image could crash in some corner cases when some files would be needed, despite it seemed not like that. If our goal is to create a container image good enough for production, we should follow a simple principle: to not remove files from RPMs, so RPM packages are installed fully or not at all.&lt;/p&gt; &lt;p&gt;However, let's first see what we started with. The container images users currently can use for the latest stable web servers are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image        | Compressed size | Uncompressed size | Apache HTTP Server 2.4 | 120 MB          | 376 MB            | Nginx 1.22             | 111 MB          | 348 MB            |&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Minimizing the container image&lt;/h2&gt; &lt;p&gt;The main trick is to use a two-phase building of the container image. That means that we use a parent image only for installing RPMs to an empty directory and then use only the content of this directly as a final result. This way we not only get rid of the package installer (DNF) but also the RPM database and RPM tooling itself. We end up with only the web server RPMs and their direct and indirect dependencies.&lt;/p&gt; &lt;p&gt;This change alone already makes a big difference in size, but it also means installing additional software into such an image is not easy. Extending such an image would either mean copying files directly to the image, or the image would need to be rebuilt from scratch. That’s an acceptable disadvantage because, for many use cases, users do not need to extend images with web servers.&lt;/p&gt; &lt;h3&gt;Analyzing dependencies&lt;/h3&gt; &lt;p&gt;The next step was looking closely at what we actually have in the container image. For example, we see &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; and all of its dependencies. That makes sense when we install the web servers outside of the container image, but in the container? It's likely not needed. So, we worked with Apache HTTPd and NGINX server maintainers, who helped us to get rid of the systemd dependency by installing only &lt;code&gt;httpd-core&lt;/code&gt; and &lt;code&gt;nginx-core&lt;/code&gt; packages. We also avoided installing the Perl module in the case of NGINX, because it pulled in a lot of additional MBs in form of the Perl interpreter and several base libraries.&lt;/p&gt; &lt;p&gt;These changes again helped to squeeze the size significantly. We didn't stop there, though. We analyzed other packages and saw that we installed &lt;code&gt;nss_wrapper&lt;/code&gt; that pulled in the Perl interpreter as well. We also installed &lt;code&gt;gettext&lt;/code&gt; package in order to have &lt;code&gt;envsubst&lt;/code&gt; utility available (for expanding Bash variables in configuration files, as environment variables are common ways to configure container images). In both cases, we worked with the package maintainers, and they allowed us to use only minimal required parts of their tools so we could only install &lt;code&gt;nss_wrapper-libs&lt;/code&gt; and &lt;code&gt;envsubst&lt;/code&gt; packages, which removed additional MBs.&lt;/p&gt; &lt;h3&gt;What we kept in the image&lt;/h3&gt; &lt;p&gt;What we didn't get rid of were several Bash scripts that help the container when starting (starting the daemon, handling the configuration, etc.). These scripts do not take more than a few kilobytes (kB) anyway, so we didn’t touch those.&lt;/p&gt; &lt;p&gt;There are also a couple of other packages that we installed explicitly to make the container images work reasonably (&lt;code&gt;coreutils-single&lt;/code&gt;, &lt;code&gt;glibc-minimal-langpack&lt;/code&gt;), but those were already made as minimal as possible.&lt;/p&gt; &lt;h2&gt;Using the micro web server images&lt;/h2&gt; &lt;p&gt;The container images we worked with are designed to be used either directly via the container command-line interface (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools/"&gt;Podman&lt;/a&gt; or Docker) in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, but they were specifically designed to work well in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Read more about specific usage in the README files available in the GitHub repositories:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/blob/master/2.4-micro/root/usr/share/container-scripts/httpd/README.md"&gt;httpd-container&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/blob/master/1.22-micro/README.md"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The final result&lt;/h2&gt; &lt;p&gt;Did we succeed? Except for the Perl module in the case of the NGINX container image, the tests we have for the images passed fine for the micro container images as well. So, the main use cases should work fine and the micro images should still be pretty useful.&lt;/p&gt; &lt;p&gt;Now we can see how big the micro images are after all those changes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image              | Compressed size | Uncompressed size | Apache HTTP Server 2.4 micro | 16 MB (13%)     | 46 MB (12%)       | Nginx 1.22 micro             | 23 MB (21%)     | 63 MB (18%)       |&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In summary, we were able to decrease to approximately one-fifth of the original size, so the images will be downloaded five times faster and consume less than one-fifth of space.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The price for such a great difference is not large; the most important feature we lose is the ability to install additional software (due to the missing RPM and DNF). If your use case is to serve static content, then micro HTTPd and NGINX images should do the work without trouble. If your use case is beyond this and you want to serve something complicated or install further RPMs, then the original web server images might be a better choice for you. Or you can create your own micro image, based on the principles explained in this article.&lt;/p&gt; &lt;p&gt;Enjoy the micro web servers, and don't forget to let us know what you think by visiting the GitHub projects below. You can also leave a comment here if you simply like this approach and the images work for you.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/issues"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/issues"&gt;httpd-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Looking for more? Explore &lt;a href="https://developers.redhat.com/topics/containers/all"&gt;other container tutorials&lt;/a&gt; from Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" title="Optimize container images for NGINX and Apache HTTPd"&gt;Optimize container images for NGINX and Apache HTTPd&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Honza Horak</dc:creator><dc:date>2023-04-25T07:00:00Z</dc:date></entry><entry><title>How to debug OpenShift operators on a live cluster using dlv</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/24/how-debug-openshift-operators-live-cluster-using-dlv" /><author><name>Swarup Ghosh</name></author><id>1c9b4d81-6214-4d43-99f4-82e55760ed12</id><updated>2023-04-24T18:00:00Z</updated><published>2023-04-24T18:00:00Z</published><summary type="html">&lt;p&gt;Debugging operators can be tricky, especially if the operator needs to be debugged on a live cluster, which is useful for developing &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster operators. Remotely running delve debugger inside the &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; helps in this case. This article is about debugging operators live in an OpenShift cluster on the fly by rebuilding the operator container image and using &lt;code&gt;go dlv&lt;/code&gt; remotely through the &lt;code&gt;oc&lt;/code&gt; port-forward.&lt;/p&gt; &lt;h2&gt;About cluster operators and Delve debugger&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/operators/"&gt;Kubernetes operators&lt;/a&gt; are used to manage the lifecycle of applications within a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. The operator pattern is aimed at simplifying installation, management, and configuration of applications and services. OpenShift is an operator-first platform with its fundamental architecture strongly rooted to various operators. In the OpenShift world, operators help to manage the lifecycle of the running cluster as well as different applications that run on top of it. With each OpenShift cluster installation, there comes a set of default operators known as &lt;a href="https://docs.openshift.com/container-platform/4.12/operators/operator-reference.html" target="_blank"&gt;cluster operators&lt;/a&gt; which help to manage different aspects of the OpenShift cluster. An OpenShift cluster marks cluster creation as complete once all the cluster operators running in the cluster can reach a healthy running state.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openshift/cluster-version-operator" target="_blank"&gt;Cluster Version Operator&lt;/a&gt; (CVO) is one of the important cluster operators that reconciles the resources within the cluster to match them to their desired state while ensuring that other cluster operators remain healthy. Each cluster operator manages specific area of the cluster’s functionality and these operator deployments observe a few set of args in their respective deployments manifests as per the configuration set by the cluster apart from other necessary values.&lt;/p&gt; &lt;p&gt;For the purpose of this example, we will use the &lt;a href="https://github.com/openshift/cluster-kube-apiserver-operator/" target="_blank"&gt;cluster-kube-apiserver-operator&lt;/a&gt; running on an OpenShift cluster and live debug the running operator remotely on a VS Code setup using go dlv debugger.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/go-delve/delve" target="_blank"&gt;Delve&lt;/a&gt; is one of the most widely used debuggers used for &lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;. It has the option to allow debugging a go binary remotely through a connected tcp port with the help of which developers can get debug access to the operator binary running inside the actual cluster.&lt;/p&gt; &lt;h2&gt;Debugging tutorial steps&lt;/h2&gt; &lt;p&gt;The following tutorial is aimed at allowing developers to debug operators running on the cluster.&lt;/p&gt; &lt;p&gt;The first step to modifying any cluster operator running on OpenShift is to disable the CVO this would help prevent the cluster operator deployment manifests to be tweaked without having it be reconciled to to the default image. With the kubeconfig of the running cluster and via &lt;code&gt;oc&lt;/code&gt; command, the following command would disable CVO completely.&lt;/p&gt; &lt;pre&gt; &lt;code class="bash language-bash"&gt;$ oc scale --replicas=0 deploy/cluster-version-operator -n openshift-cluster-version &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, if the cluster operator itself allows the user to set it to an unmanaged state through the ClusterVersion object, for the kube-api-server operator it would be as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash hljs"&gt;$ oc patch clusterversion/version --&lt;span class="hljs-built_in"&gt;type&lt;/span&gt;=&lt;span class="hljs-string"&gt;'merge'&lt;/span&gt; -p &lt;span class="hljs-string"&gt;"&lt;span class="hljs-subst"&gt;$(cat &lt;&lt;- EOF spec: overrides: - group: apps kind: Deployment name: kube-apiserver-operator namespace: openshift-kube-apiserver-operator unmanaged: true EOF )&lt;/span&gt;"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Either methods should work except that the first method completely disables CVO while the second method is specific to allowing deployment changes to kube-api-server operator only. It is noteworthy to mention that these steps are not required if you plan to use this tutorial to debug operators which are not OpenShift cluster operators. In that case, you can start past this point.&lt;/p&gt; &lt;p&gt;The deployment for kube-api-server operator can be displayed as follows:&lt;/p&gt; &lt;pre class="part" data-endline="65" data-position="3499" data-startline="36"&gt; &lt;code class="bash hljs"&gt;$ oc get deployment/kube-apiserver-operator -o yaml -n openshift-kube-apiserver-operator --- name: kube-apiserver-operator namespace: openshift-kube-apiserver-operator ownerReferences: - apiVersion: config.openshift.io/v1 kind: ClusterVersion name: version uid: 4b0f3c33-ade3-4e67-832f-169f8e297639 --- spec: --- spec: automountServiceAccountToken: &lt;span class="hljs-literal"&gt;false&lt;/span&gt; containers: - args: - --config=/var/run/configmaps/config/config.yaml &lt;span class="hljs-built_in"&gt;command&lt;/span&gt;: - cluster-kube-apiserver-operator - operator &lt;span class="hljs-built_in"&gt;env&lt;/span&gt;: - name: IMAGE value: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0314d2c0df2cf572cf8cfd13212c04dff8ef684f1cdbb93e22027c86852f1954 - name: OPERATOR_IMAGE value: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2766c4f3330423b642ad82eaa5df66a5a46893594a685fd0562a6460a2719187 - name: OPERAND_IMAGE_VERSION value: 1.25.2 --- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As one can observe for the proper functioning of this operator, there are various container args plus environment variables required to be set as a part of the container spec. While debugging operators, developers usually start off by running the Golang binaries locally but it can get cumbersome to set these args and environment variables manually. To avoid this, we can rebuild the operator’s image with a debugger-friendly binary, push it to a registry, and use it in the deployment manifest of the operator.&lt;/p&gt; &lt;p&gt;In the case of kube-apiserver-operator, one can obtain the source of the operator by cloning it from &lt;a href="https://github.com/openshift/cluster-kube-apiserver-operator" target="_blank"&gt;the GitHub repository&lt;/a&gt;&lt;span data-position="5171" data-size="154"&gt;, and this step should be similar for any other operator or the developer should have the source files ready on their local at the time of reading this.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Launch a VS Code editor from the local folder containing the source files. Make the following changes to the Dockerfile of the operator. &lt;/p&gt; &lt;p&gt;We create a copy of the Dockerfile and ensure that Go build of the binaries are built with the gcflags as &lt;code&gt;“all=-N -l”&lt;/code&gt; before. It can either be passed at command line using &lt;code&gt;go build -gcflags="all=-N -l"&lt;/code&gt; or by setting environment variable &lt;code&gt;GCFLAGS&lt;/code&gt;. This should be set as a builder stage of the Dockerfile where binaries are compiled from source. For kube-apiserver-operator, the environment variable was set.&lt;/p&gt; &lt;pre class="part" data-endline="84" data-position="5916" data-startline="75"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... AS builder &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; go install -mod=&lt;span class="hljs-built_in"&gt;readonly&lt;/span&gt; github.com/go-delve/delve/cmd/dlv@latest&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; . .&lt;/span&gt; &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GO_PACKAGE github.com/openshift/cluster-kube-apiserver-operator &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GCFLAGS &lt;span class="hljs-string"&gt;"all=-N -l"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; make build --warn-undefined-variables&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same dlv binary needs to be copied over to the final image as well, using the following command:&lt;/p&gt; &lt;pre class="part" data-endline="92" data-position="6256" data-startline="88"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; --from=builder /go/bin/dlv /usr/bin/&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This would ensure that at the time of running the binary in the container as a part of the operator deployment, we can run it using &lt;code&gt;dlv&lt;/code&gt; and bind the debug stub inside the container to a port that can later be port-forwarded.&lt;/p&gt; &lt;p&gt;The final Dockerfile would be as follows:&lt;/p&gt; &lt;pre class="part" data-endline="109" data-position="6613" data-startline="97"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... AS builder &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; go install -mod=&lt;span class="hljs-built_in"&gt;readonly&lt;/span&gt; github.com/go-delve/delve/cmd/dlv@latest&lt;/span&gt; &lt;span class="hljs-keyword"&gt;WORKDIR&lt;/span&gt;&lt;span class="language-bash"&gt; /go/src/github.com/openshift/cluster-kube-apiserver-operator&lt;/span&gt; &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; . .&lt;/span&gt; &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GO_PACKAGE github.com/openshift/cluster-kube-apiserver-operator &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GCFLAGS &lt;span class="hljs-string"&gt;"all=-N -l"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; make build --warn-undefined-variables&lt;/span&gt; &lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; --from=builder /go/bin/dlv /usr/bin/&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same can be done for any other operator including ones which are not cluster operators. The only necessity is to build the Go binary with &lt;code&gt;-l&lt;/code&gt; and &lt;code&gt;-N&lt;/code&gt; to ensure that the linker keeps the symbols for helping to debug later. The size of the debug binary and hence the debug image could be more than the stripped binary we ship in production operators.&lt;/p&gt; &lt;p&gt;After obtaining the newly modified Dockerfile at &lt;code&gt;Dockerfile.debug&lt;/code&gt;, we can build and push it to the registry using:&lt;/p&gt; &lt;pre class="part" data-endline="129" data-position="7468" data-startline="114"&gt; &lt;code class="bash hljs"&gt;$ podman build -t quay.io/&lt;USERNAME&gt;/&lt;REPO&gt;:&lt;ANY_TAG&gt; -f Dockerfile.debug . [1/2] STEP 1/7: ... --- [2/2] COMMIT quay.io/swghosh/cluster-kube-apiserver-operator:debug --&gt; b18f722bd49 Successfully tagged quay.io/swghosh/cluster-kube-apiserver-operator:debug b18f722bd49ad82b8763917800eb0481ef0135b6b1f619973a6fb7c144a09cef $ podman push quay.io/&lt;USERNAME&gt;/&lt;REPO&gt;:&lt;ANY_TAG&gt; --- Copying blob 7c33fa50bff3 &lt;span class="hljs-keyword"&gt;done&lt;/span&gt; Copying config b18f722bd4 &lt;span class="hljs-keyword"&gt;done&lt;/span&gt; Writing manifest to image destination Storing signatures &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next step would be to patch the deployment of the running operator to use the this newly prepared image and alter the container args of the same to run using &lt;code&gt;dlv&lt;/code&gt;.&lt;/p&gt; &lt;pre class="part" data-endline="150" data-position="8151" data-startline="133"&gt; &lt;code class="bash hljs"&gt;$ oc edit deployment kube-apiserver-operator -n openshift-kube-apiserver-operator &lt;span class="hljs-comment"&gt;# Change the spec.template.containers[0].args,command to use dlv&lt;/span&gt; - args: - --listen=:40000 - --headless=&lt;span class="hljs-literal"&gt;true&lt;/span&gt; - --api-version=2 - --accept-multiclient - &lt;span class="hljs-built_in"&gt;exec&lt;/span&gt; - /usr/bin/cluster-kube-apiserver-operator - -- - operator - --config=/var/run/configmaps/config/config.yaml &lt;span class="hljs-built_in"&gt;command&lt;/span&gt;: - /usr/bin/dlv &lt;span class="hljs-comment"&gt;# Change the spec.template.containers[0].image&lt;/span&gt; image: quay.io/swghosh/cluster-kube-apiserver-operator:debug &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The dlv binary needs to be run in the container which would execute the built Golang binary of the operator. For any operator, the patch would finally to run the command: &lt;code&gt;/usr/bin/dlv --listen=:40000 --headless=true --api-version=2 --accept-multiclient exec /usr/bin/&lt;operator_binary&gt; -- &lt;other_operator_args&gt;&lt;/code&gt;. Headless and listen arguments for dlv are required to enable the dlv debugger to run as a stub in headless mode and bind it to a container port which we can access later. Once edited, we can save the deployment and close the editor for the new operator pod to take effect.&lt;/p&gt; &lt;p&gt;Verify that the new operator pod is running after the manifest change as follows:&lt;/p&gt; &lt;pre class="part" data-endline="160" data-position="9410" data-startline="156"&gt; &lt;code class="bash language-bash"&gt;$ oc get pods -n openshift-kube-apiserver-operator NAME READY STATUS RESTARTS AGE kube-apiserver-operator-86c5fc45cd-rr695 1/1 Running 0 29s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the pod is in running state, port-forward 40000 port from the container which is the dlv debug port (used before as a part of &lt;code&gt;--listen&lt;/code&gt;). This would enable the traffic at localhost:40000 to be forwarded to 40000 port bound to dlv process inside the container. Re-run the command if the connection for the port-forward times out.&lt;/p&gt; &lt;pre class="part" data-endline="168" data-position="9962" data-startline="164"&gt; &lt;code class="bash language-bash"&gt;$ oc port-forward pod/kube-apiserver-operator-65bd9656cc-jdvjr 40000 -n openshift-kube-apiserver-operator Forwarding from 127.0.0.1:40000 -&gt; 40000 Forwarding from [::1]:40000 -&gt; 40000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, this operator can be debugged from VS Code or using dlv from the command line and connecting to localhost:40000. To debug it using VS Code, select the remote attach debugger option in &lt;code&gt;Run &gt; Add Configuration &gt; Go: Connect to Server&lt;/code&gt; (as shown in Figure 1). This action will allow remote Go debugging.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_2.png?itok=Y5pzyB2G" width="600" height="215" alt="The VScode debug target window." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: In VS Code, this debug target is selected "Go: Connect to Server" to allow remote Go debugging.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, connect to the localhost (as shown in Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2_5.png?itok=wsv1D5SL" width="600" height="215" alt="Setting the VS code debug target to remote host." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2. In VS Code, set the debug target "localhost" to the remote host.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then, set the port to 40000 (as shown in Figure 3).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3_7.png?itok=M5dQ9d-q" width="600" height="257" alt="The VS code debug window for setting the remote port." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3. In the VS Code window, set the debug port to 40000.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Finally, the &lt;code&gt;launch.json&lt;/code&gt; in the &lt;code&gt;.vscode&lt;/code&gt; directory of the local source directory would contain something similar to what is shown in Figure 4. The contents of &lt;code&gt;launch.json&lt;/code&gt; will be auto-generated with the necessary configurations provided by the remote host and port set in the previous steps.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4_4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4_4.png?itok=rpnmBKjF" width="600" height="257" alt="Contents of the launch.json in the VScode window for remote Go debugging." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4. The contents of the launch.json after setting the debug host and port should be similar.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After the launch.json is setup with the necessary details, you can start the &lt;code&gt;Debug &gt; Connect to server&lt;/code&gt; target (as shown in Figure 5). Start live debugging of the Go binary running through delve inside the cluster.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5_6.png?itok=MfMUQNGk" width="600" height="121" alt="Shows where to start live degugging in the VS code window from the debug explorer." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5. From the debug explorer in VS code, select the "Connect to server" debug target to start live debugging.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now you can use breakpoints, watch, check the current call stack, and do much more with your operator all while it is running live in the cluster.&lt;/p&gt; &lt;h2&gt;Running remote delve simplifies debugging operators&lt;/h2&gt; &lt;p&gt;We have illustrated how to simplify debugging operators by live debugging using dlv remotely. You can say goodbye to eerie print statements. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/24/how-debug-openshift-operators-live-cluster-using-dlv" title="How to debug OpenShift operators on a live cluster using dlv"&gt;How to debug OpenShift operators on a live cluster using dlv&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Swarup Ghosh</dc:creator><dc:date>2023-04-24T18:00:00Z</dc:date></entry><entry><title type="html">Infinispan 15.0.0.Dev01</title><link rel="alternate" href="https://infinispan.org/blog/2023/04/24/infinispan-15dev01" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2023/04/24/infinispan-15dev01</id><updated>2023-04-24T00:00:00Z</updated><content type="html">Infinispan 15.0.0.Dev01 marks the beginning of a new development cycle, and there are a number of notable changes that we are making that deserve a detailed post. BYE BYE, JAVA EE, HELLO JAKARTA EE We’ve made the decision to drop Java EE support completely and focus solely on . For 14.0 we provided artifacts for both, but we believe the time has come to move on. All of the leading application servers (like and ) as well as the most important stacks (like and ) have adopted Jakarta EE, so we’ve decided it was time for us to make the move too. JCACHE CHANGES The choice to drop Java EE directly impacts our JCache provider implementation. The JCache API (aka JSR 107) is essentially in maintenance mode: it was never adopted by Java EE and there are a number of obstacles (mostly legal) that prevent it from moving to Jakarta EE. We understand the need to have an implementation agnostic caching API, so we are going to continue supporting our JCache provider, but we will drop those parts which require Java EE, most notably the javax.cache.annotation package. Hopefully a modern caching API will emerge as part of Jakarta EE. DROPPING SPRING 5 / SPRING BOOT 2 SUPPORT Dropping Java EE also means having to drop Spring 5 and Spring Boot 2 support. QUARKUS We’ve merged our Quarkus components (embedded, CLI and server) into the main repository and have upgraded them to Quarkus 3.0. Our continues to be part of the Quarkus ecosystem. SECURITY MANAGER The Java Security Manager has been . It was designed mostly for sandboxing Java applets, but it was somehow (wrongly) co-opted as a way to implement similar functionality for normal Java applications. It was slow and cumbersome, and we’re glad to see it go. Luckily, the removal of SecurityManager support from Infinispan does not affect its authentication and authorization capabilities. JAVA 17 AND 21 Starting with Infinispan 15, we will baseline on JDK 17 and also offer support for JDK 21, including virtual threads. BUT WHAT IF I RELY ON JAVAEE, SPRING 5, SPRING BOOT 2 AND JCACHE ANNOTATIONS ? Don’t worry, we have you covered for quite some time, as we will continue to support Infinispan 14.0 for quite a while, with security patches, bug fixes and improvements that we can backport. OTHER FEATURES AND TENTATIVE RELEASE DATE What you can expect to see in Infinispan 15: * many improvements to our Redis-compatible endpoint, including many more commands * a completely overhauled endpoint with support for authentication and the binary protocol * improved performance across the board, with particular focus on the remote endpoints and clustering * CDC integration, courtesy of our friends over at * Much more! We hope to have the final release of Infinispan 15 ready for consumption by the end of the year and we will be making regular development releases. RELEASE NOTES SUB-TASK https://issues.redhat.com/browse/ISPN-12152[ISPN-12152] - Remove the Kryo and Protostuff Marshallers https://issues.redhat.com/browse/ISPN-13985[ISPN-13985] - Test AsyncCache new API. https://issues.redhat.com/browse/ISPN-13994[ISPN-13994] - Pass added SerialAllowList to configuration https://issues.redhat.com/browse/ISPN-14016[ISPN-14016] - Implement HotRodMutinyCaches methods FEATURE REQUEST https://issues.redhat.com/browse/ISPN-12081[ISPN-12081] - Weak counter creation https://issues.redhat.com/browse/ISPN-13424[ISPN-13424] - Spring 6 integration https://issues.redhat.com/browse/ISPN-13946[ISPN-13946] - Expose indexes metamodel https://issues.redhat.com/browse/ISPN-14085[ISPN-14085] - Allow to pass new list of indexed entities in update schema API https://issues.redhat.com/browse/ISPN-14233[ISPN-14233] - REST API makes it possible to download any server report https://issues.redhat.com/browse/ISPN-14298[ISPN-14298] - Delete a counter https://issues.redhat.com/browse/ISPN-14300[ISPN-14300] - Add a delta in a value https://issues.redhat.com/browse/ISPN-14303[ISPN-14303] - Reset counter https://issues.redhat.com/browse/ISPN-14309[ISPN-14309] - Improve Counters Table Filtering https://issues.redhat.com/browse/ISPN-14344[ISPN-14344] - StoreMigrator support reading segmented SingleFileStores https://issues.redhat.com/browse/ISPN-14361[ISPN-14361] - Create Hibernate 2LC implementation for Hibernate 6.2.x. https://issues.redhat.com/browse/ISPN-14577[ISPN-14577] - Running tests with alternate JDK https://issues.redhat.com/browse/ISPN-14731[ISPN-14731] - Hot Rod client should support alternate resolution strategies https://issues.redhat.com/browse/ISPN-14758[ISPN-14758] - Add cli command alternative to /rest/v2/caches/{cacheName}/{cacheKey}?extended endpoint BUG https://issues.redhat.com/browse/ISPN-12224[ISPN-12224] - Cluster in a confusing state after restarted from graceful shutdown - no hint for waiting on complete restarted https://issues.redhat.com/browse/ISPN-13877[ISPN-13877] - NullPointerException in MetricsCollector.java https://issues.redhat.com/browse/ISPN-14090[ISPN-14090] - Counters client 14 are incompatible with Server 13 https://issues.redhat.com/browse/ISPN-14112[ISPN-14112] - REST/CLI command will add a schema definition with 'upload' if the given file does not exists https://issues.redhat.com/browse/ISPN-14119[ISPN-14119] - Cache details page: Queries give error in case of running after entries expiration https://issues.redhat.com/browse/ISPN-14210[ISPN-14210] - Stores that do not return the EXPIRATION Characteristic should not allow expiration https://issues.redhat.com/browse/ISPN-14238[ISPN-14238] - Memcached server: corrupted response https://issues.redhat.com/browse/ISPN-14240[ISPN-14240] - Client certificate authentication doesn't work with HTTP/2 https://issues.redhat.com/browse/ISPN-14256[ISPN-14256] - CLI expands `-` filename breaking stdin batch https://issues.redhat.com/browse/ISPN-14264[ISPN-14264] - [Console] Counters page - double click on Strong/Weak select loads all counters https://issues.redhat.com/browse/ISPN-14271[ISPN-14271] - Cache wizard back button not working https://issues.redhat.com/browse/ISPN-14278[ISPN-14278] - [Doc] sql-store examples should be updated https://issues.redhat.com/browse/ISPN-14279[ISPN-14279] - "Divide by 0"-Exception on Cache.getStats() https://issues.redhat.com/browse/ISPN-14280[ISPN-14280] - JSON parser doesn't report error locations correctly https://issues.redhat.com/browse/ISPN-14282[ISPN-14282] - Spring Boot actuator embedded metrics use deprecated code https://issues.redhat.com/browse/ISPN-14290[ISPN-14290] - [Docs] YAML Configuration example with typos https://issues.redhat.com/browse/ISPN-14313[ISPN-14313] - Set different properties and port for cross-site testdriver https://issues.redhat.com/browse/ISPN-14315[ISPN-14315] - Not possible to get server config over REST https://issues.redhat.com/browse/ISPN-14316[ISPN-14316] - '/rest/v2/server/report' throws exception if there are 1+ instances on the host https://issues.redhat.com/browse/ISPN-14317[ISPN-14317] - Not possible to create cluster backup over REST https://issues.redhat.com/browse/ISPN-14319[ISPN-14319] - Use InetSocketAddress.getHostString() instead of getHostName() https://issues.redhat.com/browse/ISPN-14323[ISPN-14323] - Get command blocks in text/plain caches https://issues.redhat.com/browse/ISPN-14329[ISPN-14329] - Availability of caches should be prevented until a cluster is complete after "shutdown cluster" https://issues.redhat.com/browse/ISPN-14348[ISPN-14348] - BytesObjectOutput.writeUTF performance improvements https://issues.redhat.com/browse/ISPN-14356[ISPN-14356] - NPE if not configuring datasource connection pool maxSize https://issues.redhat.com/browse/ISPN-14362[ISPN-14362] - Json escape should use two equality checks instead of HashMap lookup https://issues.redhat.com/browse/ISPN-14364[ISPN-14364] - Yaml parser mishandles lists https://issues.redhat.com/browse/ISPN-14368[ISPN-14368] - CacheInputEntryStream escapes the value for every byte in the value https://issues.redhat.com/browse/ISPN-14376[ISPN-14376] - Web console crashes when cache name has a dot https://issues.redhat.com/browse/ISPN-14377[ISPN-14377] - Stats command inaccurate results https://issues.redhat.com/browse/ISPN-14390[ISPN-14390] - When reading entries from read only store apply expiration https://issues.redhat.com/browse/ISPN-14399[ISPN-14399] - Do not allow read only and passivation to be configured together https://issues.redhat.com/browse/ISPN-14406[ISPN-14406] - Stage returned from reindexing on explicit keys completes earlier than expected https://issues.redhat.com/browse/ISPN-14416[ISPN-14416] - Data Distribution chart size issue https://issues.redhat.com/browse/ISPN-14417[ISPN-14417] - The metrics for misses and retrievals are updated after page reload https://issues.redhat.com/browse/ISPN-14421[ISPN-14421] - CVE-2022-41881 codec-haproxy: HAProxyMessageDecoder Stack Exhaustion DoS [jdg-8] https://issues.redhat.com/browse/ISPN-14435[ISPN-14435] - Backwards compatibility broken with InvalidMagicIdException https://issues.redhat.com/browse/ISPN-14440[ISPN-14440] - calling AsyncCache#keys or MutinyCache#keys throws an exception because the ToEmptyBytesKeyValueFilterConverter cannot be found. https://issues.redhat.com/browse/ISPN-14453[ISPN-14453] - Ickl Queries should support BigInteger and BigDecimal https://issues.redhat.com/browse/ISPN-14461[ISPN-14461] - Add missing licence: MPL-1.1 https://issues.redhat.com/browse/ISPN-14466[ISPN-14466] - Cache configuration update failure cause not returned in http body https://issues.redhat.com/browse/ISPN-14468[ISPN-14468] - REST: return error if failed to create counter https://issues.redhat.com/browse/ISPN-14470[ISPN-14470] - REST cache configuration comparison returns 204 for different caches https://issues.redhat.com/browse/ISPN-14477[ISPN-14477] - Concurrent Spring session access results in lost session attributes https://issues.redhat.com/browse/ISPN-14479[ISPN-14479] - SQL Cache store initiation fails on Sql type CHAR https://issues.redhat.com/browse/ISPN-14491[ISPN-14491] - Adding entries with putAll does not add metadata version - following replaceWithVersion will end with a timout https://issues.redhat.com/browse/ISPN-14510[ISPN-14510] - org.infinispan.server.cli.CliIT.testCliInteractive failure https://issues.redhat.com/browse/ISPN-14511[ISPN-14511] - RestOperations.testCounter[HTTP_20] failure https://issues.redhat.com/browse/ISPN-14512[ISPN-14512] - Fix *-jakarta modules https://issues.redhat.com/browse/ISPN-14516[ISPN-14516] - Wrong versions in spring-boot-3-tests module https://issues.redhat.com/browse/ISPN-14527[ISPN-14527] - Meta model may not reflect some schema changes https://issues.redhat.com/browse/ISPN-14535[ISPN-14535] - GetCounterNameOperation can fail replay https://issues.redhat.com/browse/ISPN-14540[ISPN-14540] - [Docs]Fix JSON example for Off-heap storage https://issues.redhat.com/browse/ISPN-14542[ISPN-14542] - AsyncStore needs to use SecurityAction when retrieving ComponentRegistry https://issues.redhat.com/browse/ISPN-14543[ISPN-14543] - Build resource filtering corrupts binary files https://issues.redhat.com/browse/ISPN-14544[ISPN-14544] - RESP endpoint cache shouldn't require no expiration configured https://issues.redhat.com/browse/ISPN-14545[ISPN-14545] - SIFS Compactor does not properly shut down but the index thinks it is okay https://issues.redhat.com/browse/ISPN-14569[ISPN-14569] - Protocol parser throws a NPE if all branches of a switch statement or if/else contain a throw clause https://issues.redhat.com/browse/ISPN-14573[ISPN-14573] - AbstractAuthorization.testRestServerNodeReport is failing https://issues.redhat.com/browse/ISPN-14574[ISPN-14574] - [CLI] ClassCastException with get clusters -s option https://issues.redhat.com/browse/ISPN-14578[ISPN-14578] - We should never be using CompletableFuture.completionStage https://issues.redhat.com/browse/ISPN-14579[ISPN-14579] - Various RESP commands are requesting wrong size for buffer https://issues.redhat.com/browse/ISPN-14580[ISPN-14580] - We should use voidPromise for all context writes that don't use a future https://issues.redhat.com/browse/ISPN-14583[ISPN-14583] - RESP endpoint should bundle flush calls to allow pipelining https://issues.redhat.com/browse/ISPN-14589[ISPN-14589] - JdbcStringBasedCacheStorePassivation.testFailoverWithPassivation failures https://issues.redhat.com/browse/ISPN-14683[ISPN-14683] - NPE in configuration reader if resolver is null https://issues.redhat.com/browse/ISPN-14685[ISPN-14685] - EncodingConfiguration matching is too strict https://issues.redhat.com/browse/ISPN-14687[ISPN-14687] - Detect circular references on marshalling https://issues.redhat.com/browse/ISPN-14691[ISPN-14691] - Fix Authorization error in Actuator Metrics Binding https://issues.redhat.com/browse/ISPN-14730[ISPN-14730] - Exclude completely the non jakarta commons dependency https://issues.redhat.com/browse/ISPN-14732[ISPN-14732] - ClasspathURLStreamHandlerProvider should throw FileNotFoundException if it cannot find a resource https://issues.redhat.com/browse/ISPN-14733[ISPN-14733] - Make quarkus modules inherit from Infinispan parent https://issues.redhat.com/browse/ISPN-14737[ISPN-14737] - SoftIndexFileStore Index can become corrupted https://issues.redhat.com/browse/ISPN-14738[ISPN-14738] - RESP endpoint commands don't require previous value https://issues.redhat.com/browse/ISPN-14739[ISPN-14739] - OffHeapConcurrentMap shouldn't require reading previous value on put https://issues.redhat.com/browse/ISPN-14744[ISPN-14744] - RemoteCacheManagerAdmin docs should mention supported config formats https://issues.redhat.com/browse/ISPN-14753[ISPN-14753] - Prevent SoftIndexFileStore Compactor from running multiple times https://issues.redhat.com/browse/ISPN-14755[ISPN-14755] - Empty authorization roles serialized as JSON cannot be parsed https://issues.redhat.com/browse/ISPN-14759[ISPN-14759] - SoftIndexFileStore Index can lag behind LogAppender under heavy load https://issues.redhat.com/browse/ISPN-14763[ISPN-14763] - Users unable to configure StoreMigrator marshaller allow-list via properties https://issues.redhat.com/browse/ISPN-14767[ISPN-14767] - CLI table printer breaks when values have line breaks TASK https://issues.redhat.com/browse/ISPN-11701[ISPN-11701] - Add store migration capabilities to the CLI https://issues.redhat.com/browse/ISPN-14263[ISPN-14263] - Restrict most JGroupsTransport INFO logging when using a ForkChannel. https://issues.redhat.com/browse/ISPN-14287[ISPN-14287] - Remove extended-statistics module https://issues.redhat.com/browse/ISPN-14288[ISPN-14288] - Remove kryo and protostuff marshallers https://issues.redhat.com/browse/ISPN-14375[ISPN-14375] - Remove all uses of SecurityManager/AccessControlContext https://issues.redhat.com/browse/ISPN-14414[ISPN-14414] - REST API retrieve caches in initializing state https://issues.redhat.com/browse/ISPN-14424[ISPN-14424] - [Docs] Fix errors in the REST guide https://issues.redhat.com/browse/ISPN-14426[ISPN-14426] - Disable tracing propagation on HotRod client using a system property https://issues.redhat.com/browse/ISPN-14492[ISPN-14492] - Build Infinispan with JDK 17 https://issues.redhat.com/browse/ISPN-14541[ISPN-14541] - [Docs] Use Java serialization or JBoss Marshalling with cyclic objects https://issues.redhat.com/browse/ISPN-14575[ISPN-14575] - Remove properties attribute from indexing configuration https://issues.redhat.com/browse/ISPN-14591[ISPN-14591] - Add exception to the BlockHound for the registering of a proto file https://issues.redhat.com/browse/ISPN-14705[ISPN-14705] - Expose indexing failures statistics using Infinispan indexing failure handler https://issues.redhat.com/browse/ISPN-14713[ISPN-14713] - Include new api dependency in Spring Boot 3 modules https://issues.redhat.com/browse/ISPN-14742[ISPN-14742] - Remove GeronimoTransactionManager https://issues.redhat.com/browse/ISPN-14756[ISPN-14756] - Remove JCache support https://issues.redhat.com/browse/ISPN-14769[ISPN-14769] - Replace completedExceptionFuture with failedFuture https://issues.redhat.com/browse/ISPN-14771[ISPN-14771] - Change PrivateMetadata in OffHeap to use a flag for presence instead of 4 bytes for length https://issues.redhat.com/browse/ISPN-14786[ISPN-14786] - Remove Wildfly modules https://issues.redhat.com/browse/ISPN-14787[ISPN-14787] - Remove Spring5 and Spring Boot 2 support https://issues.redhat.com/browse/ISPN-14789[ISPN-14789] - Fix port number in exam,ples of property files in SB starter docs https://issues.redhat.com/browse/ISPN-14792[ISPN-14792] - Remove Security Integration tests with WildFly COMPONENT UPGRADE https://issues.redhat.com/browse/ISPN-14270[ISPN-14270] - Update to Spring Boot 3 https://issues.redhat.com/browse/ISPN-14320[ISPN-14320] - Update Patternfly to 2022.14 Release https://issues.redhat.com/browse/ISPN-14342[ISPN-14342] - Apache SSHD 2.9.2 https://issues.redhat.com/browse/ISPN-14343[ISPN-14343] - Jackson 2.14.1 https://issues.redhat.com/browse/ISPN-14346[ISPN-14346] - Upgrade JGroups to 5.2.10.Final https://issues.redhat.com/browse/ISPN-14365[ISPN-14365] - JBoss Marshalling 2.1.1 https://issues.redhat.com/browse/ISPN-14372[ISPN-14372] - Update Patternfly to 2022.15 Release https://issues.redhat.com/browse/ISPN-14401[ISPN-14401] - Protostream 4.6.0.Final https://issues.redhat.com/browse/ISPN-14437[ISPN-14437] - Update XStream to 1.4.20 to fix CVEs https://issues.redhat.com/browse/ISPN-14442[ISPN-14442] - Update to 2.19.0 log4j https://issues.redhat.com/browse/ISPN-14444[ISPN-14444] - Update Patternfly to 2022.16 Release https://issues.redhat.com/browse/ISPN-14448[ISPN-14448] - Surefire 3.0.0-M8 https://issues.redhat.com/browse/ISPN-14462[ISPN-14462] - Upgrade assertj-core to 3.24.1 https://issues.redhat.com/browse/ISPN-14476[ISPN-14476] - Bump JGroups to 5.2.12.Final https://issues.redhat.com/browse/ISPN-14513[ISPN-14513] - Upgrade Narayana to 5.13.1.Final https://issues.redhat.com/browse/ISPN-14523[ISPN-14523] - Log4j 2.20.0 https://issues.redhat.com/browse/ISPN-14525[ISPN-14525] - Elytron 2.1.0.Final https://issues.redhat.com/browse/ISPN-14550[ISPN-14550] - Upgrade Patternfly Dependencies to Release 2023.01 (2023-02-02) https://issues.redhat.com/browse/ISPN-14553[ISPN-14553] - Spring and Spring Boot dependencies https://issues.redhat.com/browse/ISPN-14681[ISPN-14681] - Surefire 3.0.0 https://issues.redhat.com/browse/ISPN-14694[ISPN-14694] - Upgrade Patternfly Dependencies to Release 2023.02 (2023-03-24) https://issues.redhat.com/browse/ISPN-14711[ISPN-14711] - Updates latests SB 3 and 2 https://issues.redhat.com/browse/ISPN-14734[ISPN-14734] - Quarkus 3.0.0.CR2 https://issues.redhat.com/browse/ISPN-14745[ISPN-14745] - Narayana 6.0.0.Final https://issues.redhat.com/browse/ISPN-14746[ISPN-14746] - Fabric8 kubernetes-client 6.5.1 https://issues.redhat.com/browse/ISPN-14747[ISPN-14747] - jboss-threads 3.5.0.Final https://issues.redhat.com/browse/ISPN-14764[ISPN-14764] - Upgrade to plexus-utils 3.5.1 Enhancement https://issues.redhat.com/browse/ISPN-12106[ISPN-12106] - Add a refresh button in the cache detail https://issues.redhat.com/browse/ISPN-12223[ISPN-12223] - Confusing behaviour in case of joining nodes if a partition is DEGRADED https://issues.redhat.com/browse/ISPN-12484[ISPN-12484] - Explicit Locks should throw AvailabilityException during ClusterPartition instead of Timeouts https://issues.redhat.com/browse/ISPN-14092[ISPN-14092] - Cache Configuration Wizard Direct Link https://issues.redhat.com/browse/ISPN-14142[ISPN-14142] - Transport: add option to skip flow control https://issues.redhat.com/browse/ISPN-14204[ISPN-14204] - Standardize NYC and LON for XSite tests https://issues.redhat.com/browse/ISPN-14205[ISPN-14205] - InfinispanGenericContainer::getNetworkIpAddress fail fast if container is not running https://issues.redhat.com/browse/ISPN-14213[ISPN-14213] - [Docs]: Add a statement why we provide no performance numbers https://issues.redhat.com/browse/ISPN-14223[ISPN-14223] - Create Redis cache on first access https://issues.redhat.com/browse/ISPN-14244[ISPN-14244] - Don't pretty print XML/JSON by default https://issues.redhat.com/browse/ISPN-14246[ISPN-14246] - Query Statistics Tooltip https://issues.redhat.com/browse/ISPN-14247[ISPN-14247] - Drop snakeyaml dependency https://issues.redhat.com/browse/ISPN-14259[ISPN-14259] - Support benchmark CLI as a batch command https://issues.redhat.com/browse/ISPN-14322[ISPN-14322] - Number of owners is 2 by default https://issues.redhat.com/browse/ISPN-14327[ISPN-14327] - Overlays should be able to replace endpoint configuration https://issues.redhat.com/browse/ISPN-14341[ISPN-14341] - Allow injecting a MeterRegistry instance into Infinispan https://issues.redhat.com/browse/ISPN-14374[ISPN-14374] - Env variable for max_site_masters https://issues.redhat.com/browse/ISPN-14394[ISPN-14394] - Cache Configuration Wizard indexing startup mode https://issues.redhat.com/browse/ISPN-14415[ISPN-14415] - Expose REST endpoint to compare two cache configurations https://issues.redhat.com/browse/ISPN-14423[ISPN-14423] - Improve configuration parser error reporting https://issues.redhat.com/browse/ISPN-14451[ISPN-14451] - Set Hot Rod protocol version to AUTO via properties https://issues.redhat.com/browse/ISPN-14456[ISPN-14456] - Validation for delta (counter) https://issues.redhat.com/browse/ISPN-14467[ISPN-14467] - Suppressed exceptions should be sent over the wire https://issues.redhat.com/browse/ISPN-14472[ISPN-14472] - Maven Shade 3.4.1 https://issues.redhat.com/browse/ISPN-14473[ISPN-14473] - Improve REST API error reporting https://issues.redhat.com/browse/ISPN-14474[ISPN-14474] - Add a transcoder for 'application/x-www-form-urlencoded' https://issues.redhat.com/browse/ISPN-14482[ISPN-14482] - Provide a single executor for all caches to execute indexing commands https://issues.redhat.com/browse/ISPN-14490[ISPN-14490] - Add blocking scheduled tasks to BlockingManager https://issues.redhat.com/browse/ISPN-14507[ISPN-14507] - [docs] REST updates https://issues.redhat.com/browse/ISPN-14517[ISPN-14517] - Generate test certificates from code https://issues.redhat.com/browse/ISPN-14528[ISPN-14528] - Configuration conversion should support templates https://issues.redhat.com/browse/ISPN-14552[ISPN-14552] - Statistics reset REST API https://issues.redhat.com/browse/ISPN-14570[ISPN-14570] - Protocol Parser should allow for code to be provided before the decode is invoked https://issues.redhat.com/browse/ISPN-14585[ISPN-14585] - Convert RESP endpoint to use parser generator https://issues.redhat.com/browse/ISPN-14680[ISPN-14680] - Reuse image in Server testsuite https://issues.redhat.com/browse/ISPN-14689[ISPN-14689] - Handle RESP SET optional arguments https://issues.redhat.com/browse/ISPN-14690[ISPN-14690] - Rework virtual thread detection and make it optional https://issues.redhat.com/browse/ISPN-14720[ISPN-14720] - RESP endpoint should be able to parse commands as enum https://issues.redhat.com/browse/ISPN-14722[ISPN-14722] - Expose auto/manual indexing mode https://issues.redhat.com/browse/ISPN-14723[ISPN-14723] - Allow to configure index sharding https://issues.redhat.com/browse/ISPN-14724[ISPN-14724] - Create a simple DSL to build Protocol Buffers schema https://issues.redhat.com/browse/ISPN-14735[ISPN-14735] - Move to JakartaEE packages https://issues.redhat.com/browse/ISPN-14761[ISPN-14761] - Add marshalling info in the entries tab for not protostream https://issues.redhat.com/browse/ISPN-14765[ISPN-14765] - Java serialization to JSON transcoder https://issues.redhat.com/browse/ISPN-14784[ISPN-14784] - Build with JDK 21 Get them from our .]</content><dc:creator>Tristan Tarrant</dc:creator></entry></feed>
