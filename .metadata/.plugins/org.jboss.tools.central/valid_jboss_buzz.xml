<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Optimize container images for NGINX and Apache HTTPd</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" /><author><name>Honza Horak</name></author><id>40488572-2eb5-4d73-bbf3-95f8210de674</id><updated>2023-04-25T07:00:00Z</updated><published>2023-04-25T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers/"&gt;Container&lt;/a&gt; image size matters. Let’s look at an experiment that reduced Apache HTTP and NGINX servers to micro container images. This article walks through the process we used to achieve the final result, plus how many megabytes (MB) this approach saved.&lt;/p&gt; &lt;p&gt;For this experiment, we used Fedora RPMs, but a similar approach should work in other operating systems or container images, as you see in the list of available images (there is an Apache HTTP server image that uses CentOS Stream 8 and 9 RPMs).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A shortcut:&lt;/strong&gt; If you are only interested in the result and want to try out the micro variant of Apache HTTP or NGINX server container images, check out the images in the following registries.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/httpd-24-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/nginx-122-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c8s&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c9s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The benefits of smaller container images&lt;/h2&gt; &lt;p&gt;First, let’s explain a bit more about the story behind those micro containers.&lt;/p&gt; &lt;p&gt;Container images include everything that a specific application needs except a &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel. That's the spirit of container technology. Here, our focus is on web servers, so in addition to the Apache HTTPd and NGINX server daemons, the container needs to include also libraries that those daemons use, necessary userspace components, etc.&lt;/p&gt; &lt;p&gt;Even in the year 2023 when Internet speeds are tremendous, the size of such containers is important. For example, it matters in an environment where the Internet speed is still very limited (have you heard about &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; running on satellites somewhere far in space?). It can help make the user experience more delightful (waiting dozens of seconds is not fun) or limit potential attack vectors because of unnecessary pieces of software in the container image that could work without them.&lt;/p&gt; &lt;p&gt;These are just a few reasons why developers want to make the container image as small as practically possible. Now let's review the steps we took to reduce the web server container image size to a minimum. Our experiments used Apache HTTP server 2.4 and NGINX server 1.22.&lt;/p&gt; &lt;h2&gt;Choosing binaries&lt;/h2&gt; &lt;p&gt;For these tech preview container images, we decided to use a Fedora 36 base image and therefore take RPMs from Fedora repositories. There are different attempts to make the container image small by compiling just the necessary pieces directly from the source, which results in a small image, but that’s not always a good idea.&lt;/p&gt; &lt;p&gt;Using packages from a distribution has a clear benefit—they are well-tested, maintained when there is a security issue, interact well with the rest of the operating system, and are proven to work well outside of the container, so we only need to focus on the container specifics.&lt;/p&gt; &lt;p&gt;You might think about removing files once they are installed as RPMs; this might make the image smaller, but it would be rather risky and a container image could crash in some corner cases when some files would be needed, despite it seemed not like that. If our goal is to create a container image good enough for production, we should follow a simple principle: to not remove files from RPMs, so RPM packages are installed fully or not at all.&lt;/p&gt; &lt;p&gt;However, let's first see what we started with. The container images users currently can use for the latest stable web servers are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image        | Compressed size | Uncompressed size | Apache HTTP Server 2.4 | 120 MB          | 376 MB            | Nginx 1.22             | 111 MB          | 348 MB            |&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Minimizing the container image&lt;/h2&gt; &lt;p&gt;The main trick is to use a two-phase building of the container image. That means that we use a parent image only for installing RPMs to an empty directory and then use only the content of this directly as a final result. This way we not only get rid of the package installer (DNF) but also the RPM database and RPM tooling itself. We end up with only the web server RPMs and their direct and indirect dependencies.&lt;/p&gt; &lt;p&gt;This change alone already makes a big difference in size, but it also means installing additional software into such an image is not easy. Extending such an image would either mean copying files directly to the image, or the image would need to be rebuilt from scratch. That’s an acceptable disadvantage because, for many use cases, users do not need to extend images with web servers.&lt;/p&gt; &lt;h3&gt;Analyzing dependencies&lt;/h3&gt; &lt;p&gt;The next step was looking closely at what we actually have in the container image. For example, we see &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; and all of its dependencies. That makes sense when we install the web servers outside of the container image, but in the container? It's likely not needed. So, we worked with Apache HTTPd and NGINX server maintainers, who helped us to get rid of the systemd dependency by installing only &lt;code&gt;httpd-core&lt;/code&gt; and &lt;code&gt;nginx-core&lt;/code&gt; packages. We also avoided installing the Perl module in the case of NGINX, because it pulled in a lot of additional MBs in form of the Perl interpreter and several base libraries.&lt;/p&gt; &lt;p&gt;These changes again helped to squeeze the size significantly. We didn't stop there, though. We analyzed other packages and saw that we installed &lt;code&gt;nss_wrapper&lt;/code&gt; that pulled in the Perl interpreter as well. We also installed &lt;code&gt;gettext&lt;/code&gt; package in order to have &lt;code&gt;envsubst&lt;/code&gt; utility available (for expanding Bash variables in configuration files, as environment variables are common ways to configure container images). In both cases, we worked with the package maintainers, and they allowed us to use only minimal required parts of their tools so we could only install &lt;code&gt;nss_wrapper-libs&lt;/code&gt; and &lt;code&gt;envsubst&lt;/code&gt; packages, which removed additional MBs.&lt;/p&gt; &lt;h3&gt;What we kept in the image&lt;/h3&gt; &lt;p&gt;What we didn't get rid of were several Bash scripts that help the container when starting (starting the daemon, handling the configuration, etc.). These scripts do not take more than a few kilobytes (kB) anyway, so we didn’t touch those.&lt;/p&gt; &lt;p&gt;There are also a couple of other packages that we installed explicitly to make the container images work reasonably (&lt;code&gt;coreutils-single&lt;/code&gt;, &lt;code&gt;glibc-minimal-langpack&lt;/code&gt;), but those were already made as minimal as possible.&lt;/p&gt; &lt;h2&gt;Using the micro web server images&lt;/h2&gt; &lt;p&gt;The container images we worked with are designed to be used either directly via the container command-line interface (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools/"&gt;Podman&lt;/a&gt; or Docker) in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, but they were specifically designed to work well in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Read more about specific usage in the README files available in the GitHub repositories:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/blob/master/2.4-micro/root/usr/share/container-scripts/httpd/README.md"&gt;httpd-container&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/blob/master/1.22-micro/README.md"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The final result&lt;/h2&gt; &lt;p&gt;Did we succeed? Except for the Perl module in the case of the NGINX container image, the tests we have for the images passed fine for the micro container images as well. So, the main use cases should work fine and the micro images should still be pretty useful.&lt;/p&gt; &lt;p&gt;Now we can see how big the micro images are after all those changes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image              | Compressed size | Uncompressed size | Apache HTTP Server 2.4 micro | 16 MB (13%)     | 46 MB (12%)       | Nginx 1.22 micro             | 23 MB (21%)     | 63 MB (18%)       |&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In summary, we were able to decrease to approximately one-fifth of the original size, so the images will be downloaded five times faster and consume less than one-fifth of space.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The price for such a great difference is not large; the most important feature we lose is the ability to install additional software (due to the missing RPM and DNF). If your use case is to serve static content, then micro HTTPd and NGINX images should do the work without trouble. If your use case is beyond this and you want to serve something complicated or install further RPMs, then the original web server images might be a better choice for you. Or you can create your own micro image, based on the principles explained in this article.&lt;/p&gt; &lt;p&gt;Enjoy the micro web servers, and don't forget to let us know what you think by visiting the GitHub projects below. You can also leave a comment here if you simply like this approach and the images work for you.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/issues"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/issues"&gt;httpd-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Looking for more? Explore &lt;a href="https://developers.redhat.com/topics/containers/all"&gt;other container tutorials&lt;/a&gt; from Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" title="Optimize container images for NGINX and Apache HTTPd"&gt;Optimize container images for NGINX and Apache HTTPd&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Honza Horak</dc:creator><dc:date>2023-04-25T07:00:00Z</dc:date></entry><entry><title>How to debug OpenShift operators on a live cluster using dlv</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/24/how-debug-openshift-operators-live-cluster-using-dlv" /><author><name>Swarup Ghosh</name></author><id>1c9b4d81-6214-4d43-99f4-82e55760ed12</id><updated>2023-04-24T18:00:00Z</updated><published>2023-04-24T18:00:00Z</published><summary type="html">&lt;p&gt;Debugging operators can be tricky, especially if the operator needs to be debugged on a live cluster, which is useful for developing &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster operators. Remotely running delve debugger inside the &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; helps in this case. This article is about debugging operators live in an OpenShift cluster on the fly by rebuilding the operator container image and using &lt;code&gt;go dlv&lt;/code&gt; remotely through the &lt;code&gt;oc&lt;/code&gt; port-forward.&lt;/p&gt; &lt;h2&gt;About cluster operators and Delve debugger&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/operators/"&gt;Kubernetes operators&lt;/a&gt; are used to manage the lifecycle of applications within a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. The operator pattern is aimed at simplifying installation, management, and configuration of applications and services. OpenShift is an operator-first platform with its fundamental architecture strongly rooted to various operators. In the OpenShift world, operators help to manage the lifecycle of the running cluster as well as different applications that run on top of it. With each OpenShift cluster installation, there comes a set of default operators known as &lt;a href="https://docs.openshift.com/container-platform/4.12/operators/operator-reference.html" target="_blank"&gt;cluster operators&lt;/a&gt; which help to manage different aspects of the OpenShift cluster. An OpenShift cluster marks cluster creation as complete once all the cluster operators running in the cluster can reach a healthy running state.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/openshift/cluster-version-operator" target="_blank"&gt;Cluster Version Operator&lt;/a&gt; (CVO) is one of the important cluster operators that reconciles the resources within the cluster to match them to their desired state while ensuring that other cluster operators remain healthy. Each cluster operator manages specific area of the cluster’s functionality and these operator deployments observe a few set of args in their respective deployments manifests as per the configuration set by the cluster apart from other necessary values.&lt;/p&gt; &lt;p&gt;For the purpose of this example, we will use the &lt;a href="https://github.com/openshift/cluster-kube-apiserver-operator/" target="_blank"&gt;cluster-kube-apiserver-operator&lt;/a&gt; running on an OpenShift cluster and live debug the running operator remotely on a VS Code setup using go dlv debugger.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/go-delve/delve" target="_blank"&gt;Delve&lt;/a&gt; is one of the most widely used debuggers used for &lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;. It has the option to allow debugging a go binary remotely through a connected tcp port with the help of which developers can get debug access to the operator binary running inside the actual cluster.&lt;/p&gt; &lt;h2&gt;Debugging tutorial steps&lt;/h2&gt; &lt;p&gt;The following tutorial is aimed at allowing developers to debug operators running on the cluster.&lt;/p&gt; &lt;p&gt;The first step to modifying any cluster operator running on OpenShift is to disable the CVO this would help prevent the cluster operator deployment manifests to be tweaked without having it be reconciled to to the default image. With the kubeconfig of the running cluster and via &lt;code&gt;oc&lt;/code&gt; command, the following command would disable CVO completely.&lt;/p&gt; &lt;pre&gt; &lt;code class="bash language-bash"&gt;$ oc scale --replicas=0 deploy/cluster-version-operator -n openshift-cluster-version &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, if the cluster operator itself allows the user to set it to an unmanaged state through the ClusterVersion object, for the kube-api-server operator it would be as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash hljs"&gt;$ oc patch clusterversion/version --&lt;span class="hljs-built_in"&gt;type&lt;/span&gt;=&lt;span class="hljs-string"&gt;'merge'&lt;/span&gt; -p &lt;span class="hljs-string"&gt;"&lt;span class="hljs-subst"&gt;$(cat &lt;&lt;- EOF spec: overrides: - group: apps kind: Deployment name: kube-apiserver-operator namespace: openshift-kube-apiserver-operator unmanaged: true EOF )&lt;/span&gt;"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Either methods should work except that the first method completely disables CVO while the second method is specific to allowing deployment changes to kube-api-server operator only. It is noteworthy to mention that these steps are not required if you plan to use this tutorial to debug operators which are not OpenShift cluster operators. In that case, you can start past this point.&lt;/p&gt; &lt;p&gt;The deployment for kube-api-server operator can be displayed as follows:&lt;/p&gt; &lt;pre class="part" data-endline="65" data-position="3499" data-startline="36"&gt; &lt;code class="bash hljs"&gt;$ oc get deployment/kube-apiserver-operator -o yaml -n openshift-kube-apiserver-operator --- name: kube-apiserver-operator namespace: openshift-kube-apiserver-operator ownerReferences: - apiVersion: config.openshift.io/v1 kind: ClusterVersion name: version uid: 4b0f3c33-ade3-4e67-832f-169f8e297639 --- spec: --- spec: automountServiceAccountToken: &lt;span class="hljs-literal"&gt;false&lt;/span&gt; containers: - args: - --config=/var/run/configmaps/config/config.yaml &lt;span class="hljs-built_in"&gt;command&lt;/span&gt;: - cluster-kube-apiserver-operator - operator &lt;span class="hljs-built_in"&gt;env&lt;/span&gt;: - name: IMAGE value: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0314d2c0df2cf572cf8cfd13212c04dff8ef684f1cdbb93e22027c86852f1954 - name: OPERATOR_IMAGE value: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:2766c4f3330423b642ad82eaa5df66a5a46893594a685fd0562a6460a2719187 - name: OPERAND_IMAGE_VERSION value: 1.25.2 --- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As one can observe for the proper functioning of this operator, there are various container args plus environment variables required to be set as a part of the container spec. While debugging operators, developers usually start off by running the Golang binaries locally but it can get cumbersome to set these args and environment variables manually. To avoid this, we can rebuild the operator’s image with a debugger-friendly binary, push it to a registry, and use it in the deployment manifest of the operator.&lt;/p&gt; &lt;p&gt;In the case of kube-apiserver-operator, one can obtain the source of the operator by cloning it from &lt;a href="https://github.com/openshift/cluster-kube-apiserver-operator" target="_blank"&gt;the GitHub repository&lt;/a&gt;&lt;span data-position="5171" data-size="154"&gt;, and this step should be similar for any other operator or the developer should have the source files ready on their local at the time of reading this.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Launch a VS Code editor from the local folder containing the source files. Make the following changes to the Dockerfile of the operator. &lt;/p&gt; &lt;p&gt;We create a copy of the Dockerfile and ensure that Go build of the binaries are built with the gcflags as &lt;code&gt;“all=-N -l”&lt;/code&gt; before. It can either be passed at command line using &lt;code&gt;go build -gcflags="all=-N -l"&lt;/code&gt; or by setting environment variable &lt;code&gt;GCFLAGS&lt;/code&gt;. This should be set as a builder stage of the Dockerfile where binaries are compiled from source. For kube-apiserver-operator, the environment variable was set.&lt;/p&gt; &lt;pre class="part" data-endline="84" data-position="5916" data-startline="75"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... AS builder &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; go install -mod=&lt;span class="hljs-built_in"&gt;readonly&lt;/span&gt; github.com/go-delve/delve/cmd/dlv@latest&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; . .&lt;/span&gt; &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GO_PACKAGE github.com/openshift/cluster-kube-apiserver-operator &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GCFLAGS &lt;span class="hljs-string"&gt;"all=-N -l"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; make build --warn-undefined-variables&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same dlv binary needs to be copied over to the final image as well, using the following command:&lt;/p&gt; &lt;pre class="part" data-endline="92" data-position="6256" data-startline="88"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; --from=builder /go/bin/dlv /usr/bin/&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This would ensure that at the time of running the binary in the container as a part of the operator deployment, we can run it using &lt;code&gt;dlv&lt;/code&gt; and bind the debug stub inside the container to a port that can later be port-forwarded.&lt;/p&gt; &lt;p&gt;The final Dockerfile would be as follows:&lt;/p&gt; &lt;pre class="part" data-endline="109" data-position="6613" data-startline="97"&gt; &lt;code class="dockerfile hljs"&gt;&lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... AS builder &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; go install -mod=&lt;span class="hljs-built_in"&gt;readonly&lt;/span&gt; github.com/go-delve/delve/cmd/dlv@latest&lt;/span&gt; &lt;span class="hljs-keyword"&gt;WORKDIR&lt;/span&gt;&lt;span class="language-bash"&gt; /go/src/github.com/openshift/cluster-kube-apiserver-operator&lt;/span&gt; &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; . .&lt;/span&gt; &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GO_PACKAGE github.com/openshift/cluster-kube-apiserver-operator &lt;span class="hljs-keyword"&gt;ENV&lt;/span&gt; GCFLAGS &lt;span class="hljs-string"&gt;"all=-N -l"&lt;/span&gt; &lt;span class="hljs-keyword"&gt;RUN&lt;/span&gt;&lt;span class="language-bash"&gt; make build --warn-undefined-variables&lt;/span&gt; &lt;span class="hljs-keyword"&gt;FROM&lt;/span&gt; ... &lt;span class="hljs-keyword"&gt;COPY&lt;/span&gt;&lt;span class="language-bash"&gt; --from=builder /go/bin/dlv /usr/bin/&lt;/span&gt; ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same can be done for any other operator including ones which are not cluster operators. The only necessity is to build the Go binary with &lt;code&gt;-l&lt;/code&gt; and &lt;code&gt;-N&lt;/code&gt; to ensure that the linker keeps the symbols for helping to debug later. The size of the debug binary and hence the debug image could be more than the stripped binary we ship in production operators.&lt;/p&gt; &lt;p&gt;After obtaining the newly modified Dockerfile at &lt;code&gt;Dockerfile.debug&lt;/code&gt;, we can build and push it to the registry using:&lt;/p&gt; &lt;pre class="part" data-endline="129" data-position="7468" data-startline="114"&gt; &lt;code class="bash hljs"&gt;$ podman build -t quay.io/&lt;USERNAME&gt;/&lt;REPO&gt;:&lt;ANY_TAG&gt; -f Dockerfile.debug . [1/2] STEP 1/7: ... --- [2/2] COMMIT quay.io/swghosh/cluster-kube-apiserver-operator:debug --&gt; b18f722bd49 Successfully tagged quay.io/swghosh/cluster-kube-apiserver-operator:debug b18f722bd49ad82b8763917800eb0481ef0135b6b1f619973a6fb7c144a09cef $ podman push quay.io/&lt;USERNAME&gt;/&lt;REPO&gt;:&lt;ANY_TAG&gt; --- Copying blob 7c33fa50bff3 &lt;span class="hljs-keyword"&gt;done&lt;/span&gt; Copying config b18f722bd4 &lt;span class="hljs-keyword"&gt;done&lt;/span&gt; Writing manifest to image destination Storing signatures &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next step would be to patch the deployment of the running operator to use the this newly prepared image and alter the container args of the same to run using &lt;code&gt;dlv&lt;/code&gt;.&lt;/p&gt; &lt;pre class="part" data-endline="150" data-position="8151" data-startline="133"&gt; &lt;code class="bash hljs"&gt;$ oc edit deployment kube-apiserver-operator -n openshift-kube-apiserver-operator &lt;span class="hljs-comment"&gt;# Change the spec.template.containers[0].args,command to use dlv&lt;/span&gt; - args: - --listen=:40000 - --headless=&lt;span class="hljs-literal"&gt;true&lt;/span&gt; - --api-version=2 - --accept-multiclient - &lt;span class="hljs-built_in"&gt;exec&lt;/span&gt; - /usr/bin/cluster-kube-apiserver-operator - -- - operator - --config=/var/run/configmaps/config/config.yaml &lt;span class="hljs-built_in"&gt;command&lt;/span&gt;: - /usr/bin/dlv &lt;span class="hljs-comment"&gt;# Change the spec.template.containers[0].image&lt;/span&gt; image: quay.io/swghosh/cluster-kube-apiserver-operator:debug &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The dlv binary needs to be run in the container which would execute the built Golang binary of the operator. For any operator, the patch would finally to run the command: &lt;code&gt;/usr/bin/dlv --listen=:40000 --headless=true --api-version=2 --accept-multiclient exec /usr/bin/&lt;operator_binary&gt; -- &lt;other_operator_args&gt;&lt;/code&gt;. Headless and listen arguments for dlv are required to enable the dlv debugger to run as a stub in headless mode and bind it to a container port which we can access later. Once edited, we can save the deployment and close the editor for the new operator pod to take effect.&lt;/p&gt; &lt;p&gt;Verify that the new operator pod is running after the manifest change as follows:&lt;/p&gt; &lt;pre class="part" data-endline="160" data-position="9410" data-startline="156"&gt; &lt;code class="bash language-bash"&gt;$ oc get pods -n openshift-kube-apiserver-operator NAME READY STATUS RESTARTS AGE kube-apiserver-operator-86c5fc45cd-rr695 1/1 Running 0 29s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the pod is in running state, port-forward 40000 port from the container which is the dlv debug port (used before as a part of &lt;code&gt;--listen&lt;/code&gt;). This would enable the traffic at localhost:40000 to be forwarded to 40000 port bound to dlv process inside the container. Re-run the command if the connection for the port-forward times out.&lt;/p&gt; &lt;pre class="part" data-endline="168" data-position="9962" data-startline="164"&gt; &lt;code class="bash language-bash"&gt;$ oc port-forward pod/kube-apiserver-operator-65bd9656cc-jdvjr 40000 -n openshift-kube-apiserver-operator Forwarding from 127.0.0.1:40000 -&gt; 40000 Forwarding from [::1]:40000 -&gt; 40000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, this operator can be debugged from VS Code or using dlv from the command line and connecting to localhost:40000. To debug it using VS Code, select the remote attach debugger option in &lt;code&gt;Run &gt; Add Configuration &gt; Go: Connect to Server&lt;/code&gt; (as shown in Figure 1). This action will allow remote Go debugging.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_2.png?itok=Y5pzyB2G" width="600" height="215" alt="The VScode debug target window." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: In VS Code, this debug target is selected "Go: Connect to Server" to allow remote Go debugging.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, connect to the localhost (as shown in Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2_5.png?itok=wsv1D5SL" width="600" height="215" alt="Setting the VS code debug target to remote host." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2. In VS Code, set the debug target "localhost" to the remote host.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then, set the port to 40000 (as shown in Figure 3).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3_7.png?itok=M5dQ9d-q" width="600" height="257" alt="The VS code debug window for setting the remote port." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3. In the VS Code window, set the debug port to 40000.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Finally, the &lt;code&gt;launch.json&lt;/code&gt; in the &lt;code&gt;.vscode&lt;/code&gt; directory of the local source directory would contain something similar to what is shown in Figure 4. The contents of &lt;code&gt;launch.json&lt;/code&gt; will be auto-generated with the necessary configurations provided by the remote host and port set in the previous steps.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4_4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4_4.png?itok=rpnmBKjF" width="600" height="257" alt="Contents of the launch.json in the VScode window for remote Go debugging." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4. The contents of the launch.json after setting the debug host and port should be similar.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After the launch.json is setup with the necessary details, you can start the &lt;code&gt;Debug &gt; Connect to server&lt;/code&gt; target (as shown in Figure 5). Start live debugging of the Go binary running through delve inside the cluster.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/5_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/5_6.png?itok=MfMUQNGk" width="600" height="121" alt="Shows where to start live degugging in the VS code window from the debug explorer." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5. From the debug explorer in VS code, select the "Connect to server" debug target to start live debugging.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now you can use breakpoints, watch, check the current call stack, and do much more with your operator all while it is running live in the cluster.&lt;/p&gt; &lt;h2&gt;Running remote delve simplifies debugging operators&lt;/h2&gt; &lt;p&gt;We have illustrated how to simplify debugging operators by live debugging using dlv remotely. You can say goodbye to eerie print statements. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/24/how-debug-openshift-operators-live-cluster-using-dlv" title="How to debug OpenShift operators on a live cluster using dlv"&gt;How to debug OpenShift operators on a live cluster using dlv&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Swarup Ghosh</dc:creator><dc:date>2023-04-24T18:00:00Z</dc:date></entry><entry><title type="html">How to Trace requests with RestEasy</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-trace-requests-with-resteasy/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-trace-requests-with-resteasy/</id><updated>2023-04-20T11:30:45Z</updated><content type="html">When working with RESTful web services, it is often useful to be able to trace requests to better understand what is happening. Fortunately, this is relatively easy to do with WildFly and Resteasy. In this article, we will look at how to configure tracing for REST requests using an enhancement available in WildFly 28. There ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to build RHEL images for edge deployments</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/20/how-build-rhel-images-edge-deployments" /><author><name>Chris Santiago</name></author><id>26ea5a25-7501-4211-a188-9e6117aafd7e</id><updated>2023-04-20T07:00:00Z</updated><published>2023-04-20T07:00:00Z</published><summary type="html">&lt;p&gt;As &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt; infrastructure scales outside the data center into remote locations, small-factor devices such as IoT, POS, and sensors that have &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; images, need a way to be updated at scale.&lt;/p&gt; &lt;p&gt;The rpm-ostree core premise is that by default, the updates should base on a whole base image that is created and tested offline, and once ready deployed everywhere into the remote locations, overriding the previous image and lowering the risks of patching at scale.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-03-03_08-55-18.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-03-03_08-55-18.png?itok=soYJAaEV" width="533" height="354" alt="An illustration of the image builder." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The image builder.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We know many people are working in secure edge environments and need the ability to create and lifecycle manage operating systems. In order to help with this we have created a way to build, host and manage these images using the &lt;a href="https://developers.redhat.com/topics/ansible-automation-applications-and-services"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article will cover the &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/collections_using.html"&gt;Ansible collection&lt;/a&gt; for management of the&lt;a href="https://www.osbuild.org/documentation/#composer"&gt; osbuild composer&lt;/a&gt; to build&lt;a href="https://rpm-ostree.readthedocs.io/en/latest/"&gt; rpm-ostree&lt;/a&gt; based images for Fedora, &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;, and CentOS Stream. This collection has roles to build an osbuild server, an apache httpd server to host images, and a role to build installer images and rpm-ostree updates.&lt;/p&gt; &lt;h2&gt;2 roles of the infra.osbuild collection&lt;/h2&gt; &lt;p&gt;There are two roles that are part of the infra.osbuild collection:&lt;/p&gt; &lt;h3&gt;1. The infra.osbuild.setup_server role&lt;/h3&gt; &lt;p&gt;The setup_server role checks to see what type of OS the remote system is running, ostree-based or non ostree-based. A remote system running an OS based on ostree will need to have packages already installed via a previous commit or with the initial install to continue. Non-ostree based hosts will have all the necessary packages installed.&lt;/p&gt; &lt;p&gt;At the same time the setup_server role also ensures all necessary services are enabled and started. Lastly it adds support for rpm custom repositories for adding custom packages to images.&lt;/p&gt; &lt;h3&gt;2. The infra.osbuild.builder role&lt;/h3&gt; &lt;p&gt;This builder role creates a blueprint based on information provided by the playbook variables such as packages, user info, and compose type. A rpm-ostree repository is initialized for the blueprint name to handle commits and upgrades. The builder role creates an image based on the previously created blueprint. Lastly it creates a kickstart file which supports an optional auto registration to be used on a system.&lt;/p&gt; &lt;h2&gt;How to build images for edge deployment&lt;/h2&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To test this collection you will need either a RHEL, CentOS Stream, or Fedora system.&lt;/p&gt; &lt;p&gt;Install the infra.osbuild collection using the &lt;a href="https://docs.ansible.com/ansible/latest/galaxy/user_guide.html#installing-collections"&gt;ansible-galaxy&lt;/a&gt; command as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-galaxy collection install git+https://github.com/redhat-cop/infra.osbuild&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once installed we will make an empty directory to store the example playbooks and inventory file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir osbuild_example&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an &lt;a href="https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html"&gt;inventory file&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;touch inventory.ini&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inside the inventory file, create a group named &lt;code&gt;all&lt;/code&gt; with the remote systems IP address underneath.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;[all] &lt;Host IP&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we have an inventory file and a remote system to point to. Let’s take a look into the setup_role. As explained above, this role simply sets up all the necessary packages and services to get osbuild up and running. If you plan on using a custom rpm repository to add custom packages that you would like to make available to osbuild then we need to add some configuration otherwise we can use the role as is.&lt;/p&gt; &lt;p&gt;Create a playbook named &lt;code&gt;osbuild_setup_server.yaml&lt;/code&gt; with the sample playbook as its contents as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;--- - name: Run osbuild_server role hosts: all become: true tasks: - name: Run the role ansible.builtin.import_role: name: infra.osbuild.setup_server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For adding a custom rpm repository, we can pass a list to the &lt;code&gt;setup_server_custom_repos&lt;/code&gt; role variable. Each list entry is a&lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html"&gt; YAML dictionary&lt;/a&gt; type and has the following attributes:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;code&gt;repo_name&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;base_url&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;type&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;check_ssl&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;check_gpg&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;rhsm&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;code&gt;state&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If you wanted support for custom rpm repositories your playbook should something like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;--- - name: Run osbuild_server role hosts: all become: true vars: setup_server_custom_repos: - name: EPEL Everything base_url: "https://dl.fedoraproject.org/pub/epel/{{ hostvars[inventory_hostname].ansible_distribution_major_version }}/Everything/x86_64/" type: yum-baseurl check_ssl: true check_gpg: true state: present - name: My company custom repo base_url: "https://repo.example.com/company_repo/x86_64/" type: yum-baseurl tasks: - name: Run the role ansible.builtin.import_role: name: infra.osbuild.setup_server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we have the osbuild_setup_server file completed, we can run the playbook using this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook -i inventory.ini –ask-become –ask-pass playbooks/osbuild_setup_server.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We will run the playbook with &lt;code&gt;–ask-become&lt;/code&gt; and &lt;code&gt;–ask-pass&lt;/code&gt; flags to provide basic authentication, or if you want to set up proper authentication with ssh keys and proper user sudo management.&lt;/p&gt; &lt;p&gt;Once the playbook has finished running then the osbuild server is ready for us to start building images.&lt;/p&gt; &lt;p&gt;As explained  in the infra.osbuild.builder section, there are variables that are needed to create a blueprint. You can refer to the &lt;a href="https://github.com/redhat-cop/infra.osbuild/tree/main/roles/builder"&gt;full list and their explanations&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let’s create another playbook called &lt;code&gt;osbuild_builder.yaml&lt;/code&gt; with the sample playbook as its contents as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;--- - name: Run osbuild_builder role become: true hosts: all vars: builder_compose_type: edge-commit builder_blueprint_name: mybuild builder_pub_key: ~/.ssh/id_rsa.pub builder_compose_pkgs: - vim-enhanced - httpd - ansible-core - tmux builder_compose_customizations: user: name: "testuser" description: "test user" password: "testpassword" key: "{{ builder_pub_key }}" groups: '["users", "wheel"]' tasks: - name: Run the role ansible.builtin.import_role: name: infra.osbuild.builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you would like to have your image automatically register with Ansible Automation Platform, add &lt;code&gt;builder_aap_url&lt;/code&gt;, &lt;code&gt;builder_set_hostname&lt;/code&gt;, &lt;code&gt;builder_aap_ks_user&lt;/code&gt; and &lt;code&gt;builder_aap_ks_password&lt;/code&gt; underneath the vars section in the &lt;code&gt;osbuild_builder.yaml&lt;/code&gt; playbook.&lt;/p&gt; &lt;p&gt;Once you have finished writing your playbook run the &lt;code&gt;osbuild_builder.yml&lt;/code&gt; playbook to begin building an image.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook -i inventory.ini –ask-become –ask-pass playbooks/osbuild_builder.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After the playbook finishes, log in into your remote system &lt;code&gt;http://&lt;ip_addr&gt;/&lt;blueprint_name&gt;/&lt;/code&gt; to see the hosted repo and kickstart file that can be used to provision a new system.&lt;/p&gt; &lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;In summary, infra.osbuild is an easy-to-use solution for creating customizable images. Ansible validated content for infrastructure osbuild collection, will let you automate the provisioning and configuration of the required osbuild components and build a RHEL image for your edge deployments.&lt;/p&gt; &lt;p&gt;If you want to learn more about edge &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt;, here are a few suggestions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Explore resources at &lt;a href="https://www.ansible.com/use-cases/edge"&gt;Using Red Hat Ansible Automation Platform for edge computing&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download the &lt;a href="https://developers.redhat.com/e-books/automation-at-the-edge"&gt;Automation at the edge e-book&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;For additional use cases such as industrial protocol integration, read the article, &lt;a href="https://developers.redhat.com/articles/2023/01/10/automate-devices-using-ansible-cip"&gt;How to automate devices using the Ansible CIP collection&lt;/a&gt;. &lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/20/how-build-rhel-images-edge-deployments" title="How to build RHEL images for edge deployments"&gt;How to build RHEL images for edge deployments&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Chris Santiago</dc:creator><dc:date>2023-04-20T07:00:00Z</dc:date></entry><entry><title type="html">WildFly 28 is released!</title><link rel="alternate" href="https://wildfly.org//news/2023/04/20/WildFly28-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2023/04/20/WildFly28-Released/</id><updated>2023-04-20T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 28.0.0.Final releases are available for download at . NEW AND NOTABLE Observability The biggest changes in WildFly 28 relate to the observability space. * The micrometer subsystem has been , bringing support. As part of this work, we’ve added support for . The micrometer subsystem was first introduced in WildFly Preview in WildFly 27. Note that the subsystem has been updated from what was in WildFly Preview 27 to switch to pushing metric data via OTLP to a remote collector, instead of supporting polling of data on the WildFly server’s management interface. (Server and JVM metrics can still be pulled from the management endpoint if the is configured.) * We’ve also added support for via a . * We’ve removed support for MicroProfile Metrics, except for a that’s been kept to facilitate configuration migration. MicroProfile Metrics users are encouraged to use the new micrometer subsystem. * We’ve removed support for MicroProfile OpenTracing, except for a that’s been kept to facilitate configuration migration. MicroProfile OpenTracing users are encouraged to use the new microprofile-telemetry subsystem, or the opentelemetry subsystem upon which it is based. MicroProfile Besides the changes in the observability space noted above, there are a couple of important changes in WildFly 28’s MicroProfile support: * We’ve for via new microprofile-lra-coordinator and microprofile-lra-participant subsystems. * Except for MicroProfile Metrics and OpenTracing, which have been removed, we’ve updated our support for the other MicroProfile Platform specifications to the versions. Because we no longer support MicroProfile Metrics, WildFly 28 cannot claim to be a compatible implementation of the MicroProfile 6.0 specification. However, WildFly’s MicroProfile support includes implementations of the following specifications in our "full" (e.g. standalone-full.xml) and "default" (e.g standalone.xml) configurations as well as our "microprofile" configurations (e.g. standalone-microprofile.xml): MicroProfile Technology WildFly Full/Default Configurations WildFly MicroProfile Configuration MicroProfile Config 3.0 X X MicroProfile Fault Tolerance 4.0  —  X MicroProfile Health 4.0  —  X MicroProfile JWT Authentication 2.1 X X MicroProfile LRA 2.0  —  X MicroProfile OpenAPI 3.1  —  X MicroProfile Open Telemetry 1.0  —  X MicroProfile Reactive Messaging 3.0  —   —  MicroProfile Reactive Streams Operators 3.0  —   —  MicroProfile Rest Client 3.0 X X Provisioning * We’ve added a new to make it easy to provision a server based on the new introduced in EE 10. * Related to this we’ve introduced new and Galleon layers. These layers allow a more tailored configuration compared to the existing ee and web-server layers. Also, separate from WildFly itself, to help users in their migration from Jakarta EE 8 to EE 10 we’ve introduced a separate that provides a new Galleon feature pack. The wildfly-deployment-transformer-feature-pack allows you to integrate into a standard WildFly installation the EE 8 to EE 9 deployment transformation functionality that we’ve since its first release. See the for documentation on how to use this new feature pack. Quickstarts * Eduardo Martins and the teams working on server provisioning and cloud have done a significant enhancement to the WildFly quickstarts to . * We’ve also added a that . Other Treats * The server kernel team has added support for . is a nice alternative to using CLI scripts to tailor a stock configuration for a particular environment, as there is no need start a CLI process to apply the customization. This makes it well suited to workflows like . * The clustering team has added support for . * The clustering and web teams have added support for . * The RESTEasy team has added support for . * The messaging-activemq subsystem now supports . * When you use OIDC, the security team has added support for . * The web team has added for Undertow listeners. * We’ve updated Hibernate ORM from the ORM 6.1 release to 6.2.1. JAKARTA EE 10 SUPPORT WildFly 28 is a compatible implementation of the EE 10 as well as the and the new . WildFly is EE 10 compatible when running on both Java SE 11 and Java SE 17. Evidence supporting our certification is available in the repository on GitHub: * Jakarta EE 10 Full Platform * * * Jakarta EE 10 Web Profile * * * Jakarta EE 10 Core Profile * * JAVA SE SUPPORT Our recommendation is that you run WildFly on the most recent long-term support Java SE release, i.e. on SE 17 for WildFly 28. While we do do some testing of WildFly on JDK 20, we do considerably more testing of WildFly itself on the LTS JDKs, and we make no attempt to ensure the projects producing the various libraries we integrate are testing their libraries on anything other than JDK 11 or 17. WildFly 28 also is heavily tested and runs well on Java 11. We plan to continue to support Java 11 at least through WildFly 29, and likely beyond. We do, however, anticipate removing support for SE 17 sometime in the next 12 to 18 months. While we recommend using an LTS JDK release, I do believe WildFly runs well on JDK 20. By runs well, I mean the main WildFly testsuite runs with no more than a few failures in areas not expected to be commonly used. We want developers who are trying to evaluate what a newer JVM means for their applications to be able to look to WildFly as a useful development platform. Please note that WildFly runs on Java 11 and later in classpath mode. KNOWN ISSUES SPRING AND RESTEASY SPRING In WildFly 27, pending the final release of Spring 6, RESTEasy Spring support was removed from standard WildFly, and was only provided with WildFly Preview. With WildFly 28 we have reintroduced RESTEasy Spring support to standard WildFly. However, we’ve learned of a in WildFly 28 that will prevent Spring deployments, including those using RESTEasy Spring, from working. Until this is resolved in WildFly 28.0.1, users can work around this issue by to their deployment that declares a dependency on the org.jboss.vfs module. RELEASE NOTES The full release notes for the release are in the . Issues fixed in the underlying and releases are listed in the WildFly Core JIRA. Please try it out and give us your feedback, while we get to work on WildFly 29! Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>OpenJDK 8u372 to feature cgroup v2 support</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/19/openjdk-8u372-feature-cgroup-v2-support" /><author><name>Severin Gehwolf</name></author><id>ed0cf383-c604-47bf-b79e-5edc5248c27f</id><updated>2023-04-19T07:00:00Z</updated><published>2023-04-19T07:00:00Z</published><summary type="html">&lt;p&gt;The control group (cgroup) pseudo filesystem is the key feature enabling resource quotas on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;containers&lt;/a&gt; deployed on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. The cgroups filesystem is a &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; kernel feature and comes in one of three forms, depending on the hosts' configuration:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;cgroup v2, or unified hierarchy&lt;/li&gt; &lt;li&gt;cgroup v1, or legacy hierarchy&lt;/li&gt; &lt;li&gt;hybrid (basically cgroup v1, but some system services use cgroup v2).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The state of the art is cgroup v2. With the releases of &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.12&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux 9&lt;/a&gt;, which both feature cgroup v2, it becomes increasingly likely that OpenJDK 8 running in containers runs on a cgroup v2-enabled Linux kernel. Cgroup v1, the current predominant configuration, will become increasingly less frequent in practice as time moves forward.&lt;/p&gt; &lt;h2&gt;OpenJDK 8u372: cgroup v1 and v2 support&lt;/h2&gt; &lt;p&gt;With the release of OpenJDK 8u372 in April 2023, the cgroup v2 support patches present in later &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="d5069613-6b53-421b-9e45-bf8cf43625de" href="https://developers.redhat.com/articles/2022/04/19/java-17-whats-new-openjdks-container-awareness" title="Java 17: What’s new in OpenJDK's container awareness"&gt;JDK releases&lt;/a&gt; have been backported to OpenJDK 8. 30 patches in total have been backported so as to bring this feature to OpenJDK 8u. It was added to OpenJDK 8u, a very mature and stable JDK release, under the enhancement exception rule of adapting to new hardware and operating environments.&lt;/p&gt; &lt;p&gt;Version 8u372 and later will detect the cgroup version in use, v1 or v2, on the host system. Once it detects the version, it looks up the set resource limits via the pseudo filesystem hierarchy and will size its internal resources accordingly. This will ensure that OpenJDK 8 will comply to the set resource limits of containers on cgroup v2 systems, a feature OpenJDK users have grown accustomed to since it was first &lt;a href="https://bugs.openjdk.org/browse/JDK-8146115"&gt;brought to OpenJDK 8 with the 8u192&lt;/a&gt; release.&lt;/p&gt; &lt;h2&gt;How to see which cgroup version OpenJDK 8u detected&lt;/h2&gt; &lt;p&gt;One easy way to see which cgroup version, if any, OpenJDK 8u detected is by using the &lt;code&gt;-XshowSettings:system&lt;/code&gt; &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; launcher switch. Example:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[root@357fec96b37e /]# /opt/jdk8u372/bin/java -XshowSettings:system -version Operating System Metrics: Provider: cgroupv2 Effective CPU Count: 3 CPU Period: 100000us CPU Quota: 300000us CPU Shares: -1 List of Processors: N/A List of Effective Processors, 4 total: 0 1 2 3 List of Memory Nodes: N/A List of Available Memory Nodes, 1 total: 0 Memory Limit: 500.00M Memory Soft Limit: 0.00K Memory &amp; Swap Limit: 500.00M openjdk version "1.8.0_372-beta" OpenJDK Runtime Environment (Temurin)(build 1.8.0_372-beta-202303201451-b05) OpenJDK 64-Bit Server VM (Temurin)(build 25.372-b05, mixed mode)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another way is to use the &lt;code&gt;-XX:+UnlockDiagnosticVMOptions -XX:+PrintContainerInfo&lt;/code&gt; JVM switches in order to see the details of which files are being looked up internally:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[root@357fec96b37e /]# /opt/jdk8u372/bin/java -XX:+UnlockDiagnosticVMOptions -XX:+PrintContainerInfo -version OSContainer::init: Initializing Container Support Detected cgroups v2 unified hierarchy Path to /cpu.max is /sys/fs/cgroup//cpu.max Raw value for CPU quota is: 300000 CPU Quota is: 300000 Path to /cpu.max is /sys/fs/cgroup//cpu.max CPU Period is: 100000 Path to /cpu.weight is /sys/fs/cgroup//cpu.weight Raw value for CPU Shares is: 100 CPU Shares is: -1 CPU Quota count based on quota/period: 3 OSContainer::active_processor_count: 3 CgroupSubsystem::active_processor_count (cached): 3 total physical memory: 5033832448 Path to /memory.max is /sys/fs/cgroup//memory.max Raw value for memory limit is: 524288000 Memory Limit is: 524288000 total container memory: 524288000 total container memory: 524288000 CgroupSubsystem::active_processor_count (cached): 3 Path to /cpu.max is /sys/fs/cgroup//cpu.max Raw value for CPU quota is: 300000 CPU Quota is: 300000 Path to /cpu.max is /sys/fs/cgroup//cpu.max CPU Period is: 100000 Path to /cpu.weight is /sys/fs/cgroup//cpu.weight Raw value for CPU Shares is: 100 CPU Shares is: -1 CPU Quota count based on quota/period: 3 OSContainer::active_processor_count: 3 openjdk version "1.8.0_372-beta" OpenJDK Runtime Environment (Temurin)(build 1.8.0_372-beta-202303201451-b05) OpenJDK 64-Bit Server VM (Temurin)(build 25.372-b05, mixed mode)&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;OpenJDK 8u362 and older: cgroup v1 only&lt;/h2&gt; &lt;p&gt;Older releases of OpenJDK 8u will only be able to detect cgroup v1 systems. Security considerations aside, it's highly encouraged to upgrade to a later release if you are running your containers on recent cloud infrastructure such as OpenShift 4.12. For example, for a 8u362 build of OpenJDK on a cgroup's v2 system, it would look like as if the container detection failed (i.e., no metrics):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ./jdk8u362-b09/bin/java -XshowSettings:system -version Operating System Metrics: No metrics available for this platform openjdk version "1.8.0_362" OpenJDK Runtime Environment (Temurin)(build 1.8.0_362-b09) OpenJDK 64-Bit Server VM (Temurin)(build 25.362-b09, mixed mode) &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Onward to JDK 21&lt;/h2&gt; &lt;p&gt;While it's important to support new computing environments in older JDK releases, the next OpenJDK LTS release, OpenJDK 21, is around the corner and is scheduled to be released in September 2023. JDK 21 includes many &lt;a href="https://openjdk.org/jeps/387"&gt;more improvements&lt;/a&gt; for a better container and cloud experience. So if you are thinking of writing new Java applications, consider targeting JDK 21, and perhaps using &lt;a href="https://www.quarkus.io"&gt;Quarkus&lt;/a&gt; for the best cloud native experience!&lt;/p&gt; &lt;p&gt;Looking for more resources? &lt;a href="https://developers.redhat.com/java"&gt;Explore all things Java on Red Hat Developer.&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/19/openjdk-8u372-feature-cgroup-v2-support" title="OpenJDK 8u372 to feature cgroup v2 support"&gt;OpenJDK 8u372 to feature cgroup v2 support&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Severin Gehwolf</dc:creator><dc:date>2023-04-19T07:00:00Z</dc:date></entry><entry><title type="html">Using The RESTEasy Tracing Feature In WildFly</title><link rel="alternate" href="https://resteasy.dev/2023/04/19/tracing-feature-in-wildfly/" /><author><name /></author><id>https://resteasy.dev/2023/04/19/tracing-feature-in-wildfly/</id><updated>2023-04-19T00:00:00Z</updated><dc:creator /></entry><entry><title type="html">Kogito 1.36.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/04/kogito-1-36-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/04/kogito-1-36-0-released.html</id><updated>2023-04-18T13:12:34Z</updated><content type="html">We are glad to announce that the Kogito 1.36.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Added support for CloudEvents to Serverless Workflow Knative custom function * Implement action condition on Serverless Workflow * Job Service embedded Quarkus extension * Added Workflow executor. This allows  embedded execution of workflows in a standard JVM.  * Added first draft of Workflow definition fluent API. This allows programmatically defining workflows.  * Added support for OnOverflow annotation. This allows users to control event publisher overflow policy through application properties.  * Fixed bug that prevents using JQ expressions in arrays.   For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.27.0 artifacts are available at the . A detailed changelog for 1.36.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Implementing C++20 semaphores</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/18/implementing-c20-semaphores" /><author><name>Thomas Rodgers</name></author><id>afeb7533-a077-44a1-a7d3-be636d8d1cc2</id><updated>2023-04-18T07:00:00Z</updated><published>2023-04-18T07:00:00Z</published><summary type="html">&lt;p&gt;C++20 introduces &lt;code&gt;counting_semaphore&lt;/code&gt; and &lt;code&gt;binary_semaphore&lt;/code&gt;, which support blocking &lt;code&gt;acquire()&lt;/code&gt; and non-blocking &lt;code&gt;try_acquire()&lt;/code&gt; as well as timed &lt;code&gt;try_acquire_for()&lt;/code&gt; and &lt;code&gt;try_acquire_until()&lt;/code&gt;. On platforms that support &lt;code&gt;__platform_wait()&lt;/code&gt;/&lt;code&gt;__platform_notify()&lt;/code&gt;, we select an implementation strategy based on &lt;code&gt;atomic&lt;/code&gt;; otherwise, we attempt to use POSIX semaphores.&lt;/p&gt; &lt;p&gt;If you missed the previous article, read it here: &lt;a href="https://developers.redhat.com/articles/2022/12/06/implementing-c20-atomic-waiting-libstdc"&gt;Implementing C++20 atomic waiting in libstdc++&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Semaphores in C++&lt;/h2&gt; &lt;p&gt;Here's what the ISO C++ Standard has to say on the matter of semaphores:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;1 Class template counting_semaphore maintains an internal counter that is initialized when the semaphore is created. The counter is decremented when a thread acquires the semaphore, and is incremented when a thread releases the semaphore. If a thread tries to acquire the semaphore when the counter is zero, the thread will block until another thread increments the counter by releasing the semaphore.&lt;/p&gt; &lt;p&gt;2 least_max_value shall be non-negative; otherwise the program is ill-formed.&lt;/p&gt; &lt;p&gt;3 Concurrent invocations of the member functions of counting_semaphore, other than its destructor, do not introduce data races.&lt;/p&gt; &lt;h4&gt;void release(ptrdiff_t update = 1);&lt;/h4&gt; &lt;p&gt;8 Preconditions: update &gt;= 0 is true, and update &lt;= max() - counter is true.&lt;/p&gt; &lt;p&gt;9 Effects: Atomically execute counter += update. Then, unblocks any threads that are waiting for counter to be greater than zero.&lt;/p&gt; &lt;p&gt;10 Synchronization: Strongly happens before invocations of try_acquire that observe the result of the effects.&lt;/p&gt; &lt;p&gt;11 Throws: system_error when an exception is required (32.2.2).&lt;/p&gt; &lt;p&gt;12 Error conditions: Any of the error conditions allowed for mutex types (32.5.4.2).&lt;/p&gt; &lt;h4&gt;bool try_acquire() noexcept;&lt;/h4&gt; &lt;p&gt;13 Effects: Attempts to atomically decrement counter if it is positive, without blocking. If counter is not decremented, there is no effect and try_acquire immediately returns. An implementation may fail to decrement counter even if it is positive. [Note 1 : This spurious failure is normally uncommon, but allows interesting implementations based on a simple compare and exchange (Clause 31). — end note] An implementation should ensure that try_acquire does not consistently return false in the absence of contending semaphore operations.&lt;/p&gt; &lt;p&gt;14 Returns: true if counter was decremented, otherwise false.&lt;/p&gt; &lt;h4&gt;void acquire();&lt;/h4&gt; &lt;p&gt;15 Effects: Repeatedly performs the following steps, in order:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;(15.1) — Evaluates try_acquire. If the result is true, returns.&lt;.li&gt;&lt;/li&gt; &lt;li&gt;(15.2) — Blocks on *this until counter is greater than zero.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;16 Throws: system_error when an exception is required (32.2.2).&lt;/p&gt; &lt;p&gt;17 Error conditions: Any of the error conditions allowed for mutex types (32.5.4.2).&lt;/p&gt; &lt;h4&gt;template bool try_acquire_for(const chrono::duration&amp; rel_time); template bool try_acquire_until(const chrono::time_point&amp; abs_time);&lt;/h4&gt; &lt;p&gt;18 Effects: Repeatedly performs the following steps, in order:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;(18.1) — Evaluates try_acquire(). If the result is true, returns true.&lt;/li&gt; &lt;li&gt;(18.2) — Blocks on *this until counter is greater than zero or until the timeout expires. If it is unblocked by the timeout expiring, returns false. The timeout expires (32.2.4) when the current time is after abs_time (for try_acquire_until) or when at least rel_time has passed from the start of the function (for try_acquire_for).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;19 Throws: Timeout-related exceptions (32.2.4), or system_error when a non-timeout-related exception is required (32.2.2). 20 Error conditions: Any of the error conditions allowed for mutex types (32.5.4.2).&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;OK, so how to implement it?&lt;/h2&gt; &lt;p&gt;If we have POSIX semaphores available, the following type will be defined:&lt;/p&gt; &lt;pre&gt; struct __platform_semaphore { using __clock_t = chrono::system_clock; #ifdef SEM_VALUE_MAX static constexpr ptrdiff_t _S_max = SEM_VALUE_MAX; #else static constexpr ptrdiff_t _S_max = _POSIX_SEM_VALUE_MAX; #endif explicit __platform_semaphore(ptrdiff_t __count) noexcept { sem_init(&amp;_M_semaphore, 0, __count); } ~__platform_semaphore() { sem_destroy(&amp;_M_semaphore); } // ... }; &lt;/pre&gt; &lt;p&gt;And implements &lt;code&gt;acquire&lt;/code&gt;/​​​​​​&lt;code&gt;try_acquire&lt;/code&gt;/&lt;code&gt;release&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; struct __platform_semaphore { // ... void _M_acquire() noexcept { for (;;) { auto __err = sem_wait(&amp;_M_semaphore); if (__err &amp;&amp; (errno == EINTR)) continue; else if (__err) std::terminate(); else break; } } _GLIBCXX_ALWAYS_INLINE bool _M_try_acquire() noexcept { for (;;) { auto __err = sem_trywait(&amp;_M_semaphore); if (__err &amp;&amp; (errno == EINTR)) continue; else if (__err &amp;&amp; (errno == EAGAIN)) return false; else if (__err) std::terminate(); else break; } return true; } _GLIBCXX_ALWAYS_INLINE void _M_release(std::ptrdiff_t __update) noexcept { for(; __update != 0; --__update) { auto __err = sem_post(&amp;_M_semaphore); if (__err) std::terminate(); } } // ... }; &lt;/pre&gt; &lt;p&gt;For the timed wait operations, we have to worry about what the "basis" clock is and how to convert the user-supplied clock. For POSIX semaphores, this is handled as follows:&lt;/p&gt; &lt;pre&gt; struct __platform_semaphore { using __clock_t = chrono::system_clock; // ... bool _M_try_acquire_until_impl(const chrono::time_point&lt;__clock_t&gt;&amp; __atime) noexcept { auto __s = chrono::time_point_cast(__atime); auto __ns = chrono::duration_cast(__atime - __s); struct timespec __ts = { static_cast(__s.time_since_epoch().count()), static_cast(__ns.count()) }; for (;;) { if (auto __err = sem_timedwait(&amp;_M_semaphore, &amp;__ts)) { if (errno == EINTR) continue; else if (errno == ETIMEDOUT || errno == EINVAL) return false; else std::terminate(); } else break; } return true; } template bool _M_try_acquire_until(const chrono::time_point&lt;_Clock, _Duration&gt;&amp; __atime) noexcept { if constexpr (std::is_same_v&lt;__clock_t, _Clock&gt;) { return _M_try_acquire_until_impl(__atime); } else { const typename _Clock::time_point __c_entry = _Clock::now(); const auto __s_entry = __clock_t::now(); const auto __delta = __atime - __c_entry; const auto __s_atime = __s_entry + __delta; if (_M_try_acquire_until_impl(__s_atime)) return true; // We got a timeout when measured against __clock_t but // we need to check against the caller-supplied clock // to tell whether we should return a timeout. return (_Clock::now() &lt; __atime); } } }; &lt;/pre&gt; &lt;p&gt;If we have support for &lt;code&gt;atomic::wait&lt;/code&gt;/&lt;code&gt;notify_one&lt;/code&gt;/&lt;code&gt;all&lt;/code&gt; then the following type will be defined:&lt;/p&gt; &lt;pre&gt; struct __atomic_semaphore { static constexpr ptrdiff_t _S_max = __gnu_cxx::__int_traits::__max; explicit __atomic_semaphore(__detail::__platform_wait_t __count) noexcept : _M_counter(__count) { __glibcxx_assert(__count &gt;= 0 &amp;&amp; __count &lt;= _S_max); } static _GLIBCXX_ALWAYS_INLINE bool _S_do_try_acquire(__detail::__platform_wait_t* __counter) noexcept { auto __old = __atomic_impl::load(__counter, memory_order::acquire); if (__old == 0) return false; return __atomic_impl::compare_exchange_strong(__counter, __old, __old - 1, memory_order::acquire, memory_order::relaxed); } _GLIBCXX_ALWAYS_INLINE void _M_acquire() noexcept { auto const __pred = [this] {return _S_do_try_acquire(&amp;this-&gt;_M_counter);}; std::__atomic_wait_address(&amp;_M_counter, __pred); } bool _M_try_acquire() noexcept { auto const __pred = [this] { return _S_do_try_acquire(&amp;this-&gt;_M_counter); }; return std::__detail::__atomic_spin(__pred); } // ... }; &lt;/pre&gt; &lt;p&gt;This expects an implementation of &lt;code&gt;__atomic_wait_address&lt;/code&gt; that can accept a predicate; however, we have only defined &lt;code&gt;__atomic_wait_address_v&lt;/code&gt; to this point (see &lt;a href="https://developers.redhat.com/articles/2022/12/06/implementing-c20-atomic-waiting-libstdc"&gt;part 1&lt;/a&gt;).&lt;/p&gt; &lt;pre&gt; template&lt;typename _EntersWait&gt; struct __waiter { // ... template&lt;typename _Pred&gt; void _M_do_wait(_Pred __pred) noexcept { do { __platform_wait_t __val; if (__base_type::_M_do_spin(__pred, __val)) return; __base_type::_M_w._M_do_wait(__base_type::_M_addr, __val); } while (!__pred()); } }; template&lt;typename _Tp, typename _Pred&gt; void __atomic_wait_address(const _Tp* __addr, _Pred __pred) noexcept { __detail::__enters_wait __w(__addr); __w._M_do_wait(__pred); } &lt;/pre&gt; &lt;p&gt;There is a problem with this formulation, in that &lt;code&gt;__enters_wait&lt;/code&gt; will atomically increment and decrement a waiter count, which &lt;code&gt;__atomic_semaphore&lt;/code&gt; does not need, as it inherently tracks the presence of waiters. In the previous article, we introduced the notion of &lt;code&gt;__enters_wait&lt;/code&gt; and &lt;code&gt;__bare_wait&lt;/code&gt; types to be used by waiters and notifiers, respectively. We extend that notion also to support "bare" waiters.&lt;/p&gt; &lt;pre&gt; // This call is to be used by atomic types which track contention externally template void __atomic_wait_address_bare(const __detail::__platform_wait_t* __addr, _Pred __pred) noexcept { #ifdef _GLIBCXX_HAVE_PLATFORM_WAIT do { __detail::__platform_wait_t __val; if (__detail::__bare_wait::_S_do_spin(__addr, __pred, __val)) return; __detail::__platform_wait(__addr, __val); } while (!__pred()); #else // !_GLIBCXX_HAVE_PLATFORM_WAIT __detail::__bare_wait __w(__addr); __w._M_do_wait(__pred); #endif } &lt;/pre&gt; &lt;p&gt;We also need to introduce a corresponding "bare" &lt;code&gt;notify&lt;/code&gt;, which skips checks for waiters:&lt;/p&gt; &lt;pre&gt; struct __waiter_pool { // ... void _M_notify(const __platform_wait_t* __addr, bool __all, bool __bare) noexcept { if (!(__bare || _M_waiting())) return; #ifdef _GLIBCXX_HAVE_PLATFORM_WAIT __platform_notify(__addr, __all); #else if (__all) _M_cv.notify_all(); else _M_cv.notify_one(); #endif } }; &lt;/pre&gt; &lt;h3&gt;Supporting timed waits on __atomic_semaphore&lt;/h3&gt; &lt;p&gt;The C++ Standard has extensive support for durations and time points and specifies waiting operations in terms of those types:&lt;/p&gt; &lt;pre&gt; template&lt;class Rep, class Period&gt; bool try_acquire_for(const chrono::duration&lt;Rep, Period&gt;&amp; rel_time); template&lt;class Clock, class Duration&gt; bool try_acquire_until(const chrono::time_point&lt;Clock, Duration&gt;&amp; abs_time); &lt;/pre&gt; &lt;p&gt;However, &lt;code&gt;&lt;chrono&gt;&lt;/code&gt; is not supported in the freestanding (non-hosted) subset of the standard library, but &lt;code&gt;&lt;atomic&gt;&lt;/code&gt; is. This means we can't inject a dependency on the header or in its underlying &lt;code&gt;bits/atomic_base.h&lt;/code&gt; header. For this reason, all timed waiting functionality is split to &lt;code&gt;bits/atomic_timed_wait.h&lt;/code&gt;. We preferentially use &lt;code&gt;std::chrono::steady_clock&lt;/code&gt; as our basis clock on platforms that have futexes or &lt;code&gt;pthread_cond_clockwait&lt;/code&gt;. Otherwise, we fall back to the &lt;code&gt;system_clock&lt;/code&gt; and convert all user-supplied time points to this clock:&lt;/p&gt; &lt;pre&gt; #ifdef _GLIBCXX_HAVE_LINUX_FUTEX || _GLIBCXX_USE_PTHREAD_COND_CLOCKWAIT using __wait_clock_t = chrono::steady_clock; #else using __wait_clock_t = chrono::system_clock; #endif template&lt;typename _Clock, typename _Dur&gt; __wait_clock_t::time_point __to_wait_clock(const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { const typename _Clock::time_point __c_entry = _Clock::now(); const __wait_clock_t::time_point __w_entry = __wait_clock_t::now(); const auto __delta = __atime - __c_entry; using __w_dur = typename __wait_clock_t::duration; return __w_entry + chrono::ceil&lt;__w_dur&gt;(__delta); } template&lt;typename _Dur&gt; __wait_clock_t::time_point __to_wait_clock(const chrono::time_point&lt;__wait_clock_t, _Dur&gt;&amp; __atime) noexcept { using __w_dur = typename __wait_clock_t::duration; return chrono::ceil&lt;__w_dur&gt;(__atime); } &lt;/pre&gt; &lt;p&gt;If a platform supports an efficient &lt;code&gt;__platform_wait()&lt;/code&gt;, it is also expected to provide an efficient timed wait version of the same called &lt;code&gt;__platform_wait_until()&lt;/code&gt;. For Linux Futexes, we implement this as:&lt;/p&gt; &lt;pre&gt; // returns true if wait ended before timeout template&lt;typename _Dur&gt; bool __platform_wait_until_impl(const __platform_wait_t* __addr, __platform_wait_t __old, const chrono::time_point&lt;__wait_clock_t, _Dur&gt;&amp; __atime) noexcept { auto __s = chrono::time_point_cast&lt;chrono::seconds&gt;(__atime); auto __ns = chrono::duration_cast&lt;chrono::nanoseconds&gt;(__atime - __s); struct timespec __rt = { static_cast&lt;std::time_t&gt;(__s.time_since_epoch().count()), static_cast&lt;long&gt;(__ns.count()) }; auto __e = syscall (SYS_futex, __addr, static_cast&lt;int&gt;(__futex_wait_flags::__wait_bitset_private), __old, &amp;__rt, nullptr, static_cast&lt;int&gt;(__futex_wait_flags:: __bitset_match_any)); if (__e) { if ((errno != ETIMEDOUT) &amp;&amp; (errno != EINTR) &amp;&amp; (errno != EAGAIN)) __throw_system_error(errno); return true; } return false; } // returns true if wait ended before timeout template&lt;typename _Clock, typename _Dur&gt; bool __platform_wait_until(const __platform_wait_t* __addr, __platform_wait_t __old, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { if constexpr (is_same_v&lt;__wait_clock_t, _Clock&gt;) { return __platform_wait_until_impl(__addr, __old, __atime); } else { if (!__platform_wait_until_impl(__addr, __old, __to_wait_clock(__atime))) { // We got a timeout when measured against __clock_t but // we need to check against the caller-supplied clock // to tell whether we should return a timeout. if (_Clock::now() &lt; __atime) return true; } return false; } } &lt;/pre&gt; &lt;p&gt;As with &lt;code&gt;__platform_wait()&lt;/code&gt;, for platforms that do not provide &lt;code&gt;__platform_wait_until()&lt;/code&gt;, we fall back to using a mutex and condition variable and defining the following support functions:&lt;/p&gt; &lt;pre&gt; // Returns true if wait ended before timeout. // _Clock must be either steady_clock or system_clock. template&lt;typename _Clock, typename _Dur&gt; bool __cond_wait_until_impl(__condvar&amp; __cv, mutex&amp; __mx, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { static_assert(std::__is_one_of&lt;_Clock, chrono::steady_clock, chrono::system_clock&gt;::value); auto __s = chrono::time_point_cast&lt;chrono::seconds&gt;(__atime); auto __ns = chrono::duration_cast&lt;chrono::nanoseconds&gt;(__atime - __s); __gthread_time_t __ts = { static_cast&lt;std::time_t&gt;(__s.time_since_epoch().count()), static_cast&lt;long&lt;(__ns.count()) }; // if we have a pthreads implementation that supports CLOCK_MONOTONIC // and the caller's clock is steady_clock, use that #ifdef _GLIBCXX_USE_PTHREAD_COND_CLOCKWAIT if constexpr (is_same_v&lt;chrono::steady_clock, _Clock&gt;) __cv.wait_until(__mx, CLOCK_MONOTONIC, __ts); else #endif __cv.wait_until(__mx, __ts); return _Clock::now() &amp; __atime; } // returns true if wait ended before timeout template&lt;typename _Clock, typename _Dur&gt; bool __cond_wait_until(__condvar&amp; __cv, mutex&amp; __mx, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { #ifdef _GLIBCXX_USE_PTHREAD_COND_CLOCKWAIT if constexpr (is_same_v&lt;_Clock, chrono::steady_clock&gt;) return __detail::__cond_wait_until_impl(__cv, __mx, __atime); else #endif if constexpr (is_same_v&lt;_Clock, chrono::system_clock&gt;) return __detail::__cond_wait_until_impl(__cv, __mx, __atime); else { if (__cond_wait_until_impl(__cv, __mx, __to_wait_clock(__atime))) { // We got a timeout when measured against __clock_t but // we need to check against the caller-supplied clock // to tell whether we should return a timeout. if (_Clock::now() &lt; __atime) return true; } return false; } } &lt;/pre&gt; &lt;p&gt;The formulation of &lt;code&gt;__waiter_pool&lt;/code&gt; from &lt;a href="https://developers.redhat.com/articles/2022/12/06/implementing-c20-atomic-waiting-libstdc"&gt;part 1&lt;/a&gt; assumes that we are in the market for indefinitely blocking waits. However, in this case, we only care about timed waiting. The actual implementation in &lt;code&gt;bits/atomic_wait.h&lt;/code&gt; is:&lt;/p&gt; &lt;pre&gt; struct __waiter_pool_base { // ... alignas(_S_align) __platform_wait_t _M_wait = 0; #ifndef _GLIBCXX_HAVE_PLATFORM_WAIT mutex _M_mtx; #endif alignas(_S_align) __platform_wait_t _M_ver = 0; #ifndef _GLIBCXX_HAVE_PLATFORM_WAIT __condvar _M_cv; #endif void _M_enter_wait() noexcept; void _M_leave_wait() noexcept; bool _M_waiting() const noexcept; void _M_notify(const __platform_wait_t* __addr, bool __all, bool __bare) noexcept; static __waiter_pool_base&amp; _S_for(const void* __addr) noexcept; }; &lt;/pre&gt; &lt;p&gt;For indefinite atomic waits, the derived type &lt;code&gt;__waiter_pool&lt;/code&gt; is used:&lt;/p&gt; &lt;pre&gt; struct __waiter_pool : __waiter_pool_base { void _M_do_wait(const __platform_wait_t* __addr, __platform_wait_t __old) noexcept; }; &lt;/pre&gt; &lt;p&gt;For timed atomic waits, the derived type &lt;code&gt;__timed_waiter_pool&lt;/code&gt; is used:&lt;/p&gt; &lt;pre&gt; struct __timed_waiter_pool : __waiter_pool_base { // returns true if wait ended before timeout template bool _M_do_wait_until(__platform_wait_t* __addr, __platform_wait_t __old, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { #ifdef _GLIBCXX_HAVE_PLATFORM_TIMED_WAIT return __platform_wait_until(__addr, __old, __atime); #else __platform_wait_t __val; __atomic_load(__addr, &amp;__val, __ATOMIC_RELAXED); if (__val == __old) { lock_guard __l(_M_mtx); return __cond_wait_until(_M_cv, _M_mtx, __atime); } #endif // _GLIBCXX_HAVE_PLATFORM_TIMED_WAIT } }; &lt;/pre&gt; &lt;p&gt;Similarly, &lt;code&gt;__waiter&lt;/code&gt; is split into a &lt;code&gt;__waiter_base&lt;/code&gt; type:&lt;/p&gt; &lt;pre&gt; template&lt;typename _Tp&gt; struct __waiter_base { using __waiter_type = _Tp; __waiter_type&amp; _M_w; __platform_wait_t* _M_addr; template static __platform_wait_t* _S_wait_addr(const _Up* __a, __platform_wait_t* __b); static __waiter_type&amp; _S_for(const void* __addr) noexcept { static_assert(sizeof(__waiter_type) == sizeof(__waiter_pool_base)); auto&amp; res = __waiter_pool_base::_S_for(__addr); return reinterpret_cast&lt;__waiter_type&amp;&gt;(res); } template&lt;typename _Up&gt; explicit __waiter_base(const _Up* __addr) noexcept : _M_w(_S_for(__addr)) , _M_addr(_S_wait_addr(__addr, &amp;_M_w._M_ver)) { } void _M_notify(bool __all, bool __bare = false); template&lt;typename _Up, typename _ValFn, typename _Spin = __default_spin_policy&gt; static bool _S_do_spin_v(__platform_wait_t* __addr, const _Up&amp; __old, _ValFn __vfn, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); template&lt;typename _Up, typename _ValFn, typename _Spin = __default_spin_policy&gt; bool _M_do_spin_v(const _Up&amp; __old, _ValFn __vfn, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); template&lt;typename _Pred, typename _Spin = __default_spin_policy&gt; static bool _S_do_spin(const __platform_wait_t* __addr, _Pred __pred, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); template&lt;typename _Pred, typename _Spin = __default_spin_policy&gt; bool _M_do_spin(_Pred __pred, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); }; &lt;/pre&gt; &lt;p&gt;From which &lt;code&gt;__waiter&lt;/code&gt; is derived:&lt;/p&gt; &lt;pre&gt; template&lt;typename _EntersWait&gt; struct __waiter : __waiter_base&lt;__waiter_pool&gt; { using __base_type = __waiter_base&lt;__waiter_pool&gt;; template&lt;typename _Tp&gt; __waiter(const _Tp* __addr) noexcept; // ... template&lt;typename _Tp, typename _ValFn&gt; void _M_do_wait_v(_Tp __old, _ValFn __vfn); template&lt;typename _Pred&gt; void _M_do_wait(_Pred __pred) noexcept; }; using __enters_wait = __waiter&lt;std::true_type&gt;; using __bare_wait = __waiter&lt;std::false_type&gt;; &lt;/pre&gt; &lt;p&gt;&lt;code&gt;__timed_waiter&lt;/code&gt; is similarly derived:&lt;/p&gt; &lt;pre&gt; template&lt;typename _EntersWait&gt; struct __timed_waiter : __waiter_base&lt;__timed_waiter_pool&gt; { using __base_type = __waiter_base&lt;__timed_waiter_pool&gt;; template&lt;typename _Tp&gt; __timed_waiter(const _Tp* __addr) noexcept; // ... // returns true if wait ended before timeout template&lt;typename _Tp, typename _ValFn, typename _Clock, typename _Dur&gt; bool _M_do_wait_until_v(_Tp __old, _ValFn __vfn, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept; // returns true if wait ended before timeout template&lt;typename _Pred, typename _Clock, typename _Dur&gt; bool _M_do_wait_until(_Pred __pred, __platform_wait_t __val, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept; // returns true if wait ended before timeout template&lt;typename _Pred, typename _Clock, typename _Dur&gt; bool _M_do_wait_until(_Pred __pred, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept; template&lt;typename _Tp, typename _ValFn, typename _Rep, typename _Period&gt; bool _M_do_wait_for_v(_Tp __old, _ValFn __vfn, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept; template&lt;typename _Pred, typename _Rep, typename _Period&gt; bool _M_do_wait_for(_Pred __pred, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept; }; using __enters_timed_wait = __timed_waiter&lt;std::true_type&gt;; using __bare_timed_wait = __timed_waiter&lt;std::false_type&gt;; &lt;/pre&gt; &lt;p&gt;As with &lt;code&gt;__waiter&lt;/code&gt;, there are top-level &lt;code&gt;__atomic_wait_address_until&lt;/code&gt;/&lt;code&gt;for&lt;/code&gt; wrappers that &lt;code&gt;__atomic_semaphore&lt;/code&gt; calls into, all of which follow the general form of:&lt;/p&gt; &lt;pre&gt; // returns true if wait ended before timeout template&lt;typename _Tp, typename _ValFn, typename _Clock, typename _Dur&gt; bool __atomic_wait_address_until_v(const _Tp* __addr, _Tp&amp;&amp; __old, _ValFn&amp;&amp; __vfn, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { __detail::__enters_timed_wait __w{__addr}; return __w._M_do_wait_until_v(__old, __vfn, __atime); } template&lt;typename _Tp, typename _Pred, typename _Clock, typename _Dur&gt; bool __atomic_wait_address_until(const _Tp* __addr, _Pred __pred, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { __detail::__enters_timed_wait __w{__addr}; return __w._M_do_wait_until(__pred, __atime); } &lt;/pre&gt; &lt;p&gt;With versions for "bare" wait and the &lt;code&gt;_for_v/_&lt;/code&gt;for variants to wait for a supplied duration. The actual waiting entry points on &lt;code&gt;__timed_waiter&lt;/code&gt; are implemented as follows:&lt;/p&gt; &lt;pre&gt; struct __timed_waiter { // ... template&lt;typename _Tp, typename _ValFn, typename _Clock, typename _Dur&gt; bool _M_do_wait_until_v(_Tp __old, _ValFn __vfn, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { __platform_wait_t __val; if (_M_do_spin(__old, std::move(__vfn), __val, __timed_backoff_spin_policy(__atime))) return true; return __base_type::_M_w._M_do_wait_until(__base_type::_M_addr, __val, __atime); } template&lt;typename _Pred, typename _Clock, typename _Dur&gt; bool _M_do_wait_until(_Pred __pred, __platform_wait_t __val, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { for (auto __now = _Clock::now(); __now &lt; __atime; __now = _Clock::now()) { if (__base_type::_M_do_spin(__pred, __val, __timed_backoff_spin_policy(__atime, __now))) return true; if (__base_type::_M_w._M_do_wait_until(__base_type::_M_addr, __val, __atime) &amp;&amp; __pred()) return true; } return false; } }; &lt;/pre&gt; &lt;p&gt;Aside from the usual differences between the &lt;code&gt;_v&lt;/code&gt; and predicate forms of wait, the timed waits introduce a custom spin policy:&lt;/p&gt; &lt;pre&gt; struct __timed_backoff_spin_policy { __wait_clock_t::time_point _M_deadline; __wait_clock_t::time_point _M_t0; template&lt;typename _Clock, typename _Dur&gt; __timed_backoff_spin_policy(chrono::time_point&lt;_Clock, _Dur&gt; __deadline = _Clock::time_point::max(), chrono::time_point&lt;_Clock, _Dur&gt; __t0 = _Clock::now()) noexcept : _M_deadline(__to_wait_clock(__deadline)) , _M_t0(__to_wait_clock(__t0)) { } bool operator()() const noexcept { using namespace literals::chrono_literals; auto __now = __wait_clock_t::now(); if (_M_deadline &lt;= __now) return false; auto __elapsed = __now - _M_t0; if (__elapsed &gt; 128ms) { this_thread::sleep_for(64ms); } else if (__elapsed &gt; 64us) { this_thread::sleep_for(__elapsed / 2); } else if (__elapsed &gt; 4us) { __thread_yield(); } else return false; return true; } }; &lt;/pre&gt; &lt;p&gt;After the usual spin completes unsatisfied, this will begin sleeping the current thread as long as the deadline hasn't been reached. The relative &lt;code&gt;wait_for&lt;/code&gt; variants are implemented in terms of the absolute &lt;code&gt;wait_until&lt;/code&gt; members:&lt;/p&gt; &lt;pre&gt; struct __timed_waiter { // ... template&lt;typename _Tp, typename _ValFn, typename _Rep, typename _Period&gt; bool _M_do_wait_for_v(_Tp __old, _ValFn __vfn, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept { __platform_wait_t __val; if (_M_do_spin_v(__old, std::move(__vfn), __val)) return true; if (!__rtime.count()) return false; // no rtime supplied, and spin did not acquire auto __reltime = chrono::ceil&lt;__wait_clock_t::duration&gt;(__rtime); return __base_type::_M_w._M_do_wait_until(_base_type::_M_addr, __val, chrono::steady_clock::now() + __reltime); } template&lt;typename _Pred, typename _Rep, typename _Period&gt; bool _M_do_wait_for(_Pred __pred, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept { __platform_wait_t __val; if (__base_type::_M_do_spin(__pred, __val)) return true; if (!__rtime.count()) return false; // no rtime supplied, and spin did not acquire auto __reltime = chrono::ceil&lt;__wait_clock_t::duration&gt;(__rtime); return _M_do_wait_until(__pred, __val, chrono::steady_clock::now() + __reltime); } }; &lt;/pre&gt; &lt;h3&gt;The rest of __atomic_semaphore&lt;/h3&gt; &lt;p&gt;With support for atomic timed waits in place, we can define the remaining members of &lt;code&gt;__atomic_semaphore&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; struct __atomic_semaphore { // ... template&lt;typename _Clock, typename _Duration&gt; _GLIBCXX_ALWAYS_INLINE bool _M_try_acquire_until(const chrono::time_point&lt;_Clock, _Duration&gt;&amp; __atime) noexcept { auto const __pred = [this] { return _S_do_try_acquire(&amp;this-&gt;_M_counter); }; return __atomic_wait_address_until_bare(&amp;_M_counter, __pred, __atime); } template&lt;typename _Rep, typename _Period&gt; _GLIBCXX_ALWAYS_INLINE bool _M_try_acquire_for(const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept { auto const __pred = [this] { return _S_do_try_acquire(&amp;this-&gt;_M_counter); }; return __atomic_wait_address_for_bare(&amp;_M_counter, __pred, __rtime); } }; &lt;/pre&gt; &lt;p&gt;You might have observed that there are more &lt;code&gt;__atomic_wait_address_for&lt;/code&gt;/&lt;code&gt;until&lt;/code&gt; variations than are actually used by &lt;code&gt;__atomic_semaphore&lt;/code&gt;. C++26 will likely add timed versions of &lt;code&gt;wait()&lt;/code&gt; to &lt;code&gt;atomic&lt;/code&gt; as well as a version that accepts a predicate. The underlying support for these operations is already present but not yet exposed via the interface of &lt;code&gt;atomic,&lt;/code&gt; and the details are subject to change in a future version of GCC.&lt;/p&gt; &lt;h3&gt;The rest of &lt;semaphore&gt;&lt;/h3&gt; &lt;p&gt;The semaphore implementation to use is conditionally chosen based on the presence of the atomic wait feature-test macro:&lt;/p&gt; &lt;pre&gt; #if defined __cpp_lib_atomic_wait using __semaphore_impl = __atomic_semaphore; #elif _GLIBCXX_HAVE_POSIX_SEMAPHORE using __semaphore_impl = __platform_semaphore; #endif &lt;/pre&gt; &lt;p&gt;We then implement s&lt;code&gt;td::counting_semaphore&lt;/code&gt; in terms of &lt;code&gt;__semaphore_impl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; template&lt;ptrdiff_t __least_max_value = __semaphore_impl::_S_max&gt; class counting_semaphore { static_assert(__least_max_value &gt;= 0); static_assert(__least_max_value &lt;= __semaphore_impl::_S_max); __semaphore_impl _M_sem; public: explicit counting_semaphore(ptrdiff_t __desired) noexcept : _M_sem(__desired) { } ~counting_semaphore() = default; counting_semaphore(const counting_semaphore&amp;) = delete; counting_semaphore&amp; operator=(const counting_semaphore&amp;) = delete; static constexpr ptrdiff_t max() noexcept { return __least_max_value; } void release(ptrdiff_t __update = 1) noexcept(noexcept(_M_sem._M_release(1))) { _M_sem._M_release(__update); } void acquire() noexcept(noexcept(_M_sem._M_acquire())) { _M_sem._M_acquire(); } bool try_acquire() noexcept(noexcept(_M_sem._M_try_acquire())) { return _M_sem._M_try_acquire(); } template&lt;typename _Rep, typename _Period&gt; bool try_acquire_for(const std::chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) { return _M_sem._M_try_acquire_for(__rtime); } template&lt;typename _Clock, typename _Dur&gt; bool try_acquire_until(const std::chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { return _M_sem._M_try_acquire_until(__atime); } }; &lt;/pre&gt; &lt;p&gt;And &lt;code&gt;std::binary_semaphore&lt;/code&gt; is just a type alias:&lt;/p&gt; &lt;pre&gt; using binary_semaphore = std::counting_semaphore&lt;1&gt;; &lt;/pre&gt; &lt;h2&gt;Next time&lt;/h2&gt; &lt;p&gt;With all of the waiting detail and semaphores out of the way, the next installment will look at &lt;code&gt;&lt;latch&gt;&lt;/code&gt; and &lt;code&gt;&lt;barrier&gt;&lt;/code&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/18/implementing-c20-semaphores" title="Implementing C++20 semaphores"&gt;Implementing C++20 semaphores&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Thomas Rodgers</dc:creator><dc:date>2023-04-18T07:00:00Z</dc:date></entry><entry><title>My advice for transitioning to a clean architecture platform</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/17/my-advice-transitioning-clean-architecture-platform" /><author><name>Maarten Vandeperre, Kevin Dubois</name></author><id>ddcab0d1-0a01-4520-9b72-a081a2a66062</id><updated>2023-04-17T07:01:00Z</updated><published>2023-04-17T07:01:00Z</published><summary type="html">&lt;p&gt;Now that you have become an application development expert, it’s time to look at how clean architecture would look when we map it to the infrastructure side of an &lt;a href="https://developers.redhat.com/app-dev-platform"&gt;application platform&lt;/a&gt;. In case you're not familiar with clean architecture, read part one in this series, &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-building-maintainable-clean-architecture"&gt;My advice for building maintainable, clean architecture&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Preferred application platform architecture&lt;/h2&gt; &lt;p&gt;Before we get into the clean architecture mapping of our application platform, let’s describe how it should look (Figure 1).&lt;/p&gt; &lt;p&gt;We will have these four &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Personal data service&lt;/strong&gt;: Capturing personal information (i.e., email, name, phone number, age, gender, and address). Since this kind of data is relational in nature, we would opt for a relational database for this service.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Movies catalog service&lt;/strong&gt;: A service to capture all movies from which we have collected data (i.e., name, director, actors, release date, and categories). We prefer to have this data stored in a NoSQL database.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Movie tracking service&lt;/strong&gt;: A service to store tracking information about who watched a movie. Data is very limited (i.e., person ID, movie ID, timestamp, and if the movie was completed). We prefer to have this data stored in a NoSQL database.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Movie recommendation service&lt;/strong&gt;: A service that consumes data from the previous three microservices to provide recommendations based on viewer habits, age, gender, and broad location). Since this is connected data, we prefer to have this data stored in a graph database. Here too, we let go of DRY - data copied to the graph.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_1_service_outline.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_1_service_outline.jpeg?itok=xwmBanec" width="600" height="236" alt="An illustration of the microservices outline of preferred platform architecture" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The microservices outline of our preferred platform architecture.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now that we have this outline, let's discuss how to implement it.&lt;/p&gt; &lt;h2&gt;AWS single cloud vendor&lt;/h2&gt; &lt;p&gt;Within company XYZ, developers played with AWS in the past. The management and architects supported the choice for AWS, so the development team started the design of the application platform on AWS cloud. The list of requirements to design their cloud infrastructure is as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;The requirement of the person microservice is that it should run on a SQL database. Within AWS, there are various options, but the development team chooses AWS Aurora.&lt;/li&gt; &lt;li aria-level="1"&gt;The requirement of the movie microservice is that it should run on a NoSQL database. AWS only offers DynamoDB off-the-shelf, so they choose DynamoDB.&lt;/li&gt; &lt;li aria-level="1"&gt;The movie-tracking microservice has the same requirements as the movie service; hence they choose DynamoDB.&lt;/li&gt; &lt;li aria-level="1"&gt;The requirement of the movie recommendation service is that it should run on a graph database. Since AWS only offers Neptune off-the-shelf (at the time of writing), they choose Neptune. If they need machine learning models, the development team can opt for AWS Athena or a machine learning model running on the graph database.&lt;/li&gt; &lt;li aria-level="1"&gt;The development team thinks that &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift on AWS (ROSA)&lt;/a&gt; is too expensive, so they choose the DIY solution provided by EKS or ECS (both &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; implementations of AWS), not thinking or knowing about the risks going along with this decision.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now the development team has designed their cloud infrastructure for the cloud application platform (Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_2_general_design.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_2_general_design.jpeg?itok=zVVC7G-P" width="600" height="195" alt="An illustration of the design of the cloud architecture for the application platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The design of the cloud architecture for the application platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In the next section, they will get back to the management team and the architects.&lt;/p&gt; &lt;h2&gt;The evaluation of dependencies&lt;/h2&gt; &lt;p&gt;In this section, we’ll look at the AWS services as if they were code dependencies. Instead of libraries referenced from the code (see our &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-building-maintainable-clean-architecture"&gt;previous article&lt;/a&gt;), these dependencies will be cloud services referenced from the microservices. If we refer to clean architecture, the four microservices are the core-layer (i.e., the business logic that should stand the test of time) and the AWS services are the dependencies that should be easy to replace. We should not tightly couple (combine) with any specific AWS service (as interface layer, &lt;a href="https://developers.redhat.com/articles/2022/03/14/choose-best-camel-your-integration-ride-part-1"&gt;Camel&lt;/a&gt; can be used, but more on that in future articles). There is one exception. EKS or ECS is the hosting infrastructure and can be seen as the programming language within clean architecture.&lt;/p&gt; &lt;p&gt;The management team and the architects looked at the design of the application platform’s cloud architecture and had the following recommendations (Figure 3):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Usage of AWS Aurora:&lt;/strong&gt; That’s fine. It’s recommended by AWS. It’s a stable solution. This choice is accepted (acceptance rate: green).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Usage of DynamoDB&lt;/strong&gt;: This choice is not well received. The reason is that they bought or installed an on-premise bare-metal MongoDB cluster last year. In order to have some ROI, the management team and the architects prefer to use this cluster. If there is no other choice, DynamoDB can be accepted (acceptance rate: orange).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Usage of Neptune:&lt;/strong&gt; This is a no-go. Although it’s provided off-the-shelf by AWS, it’s not the best graph database on the market (at the time of writing). The management team and the architects would prefer Neo4J or TigerGraph (acceptance rate: red).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Usage of EKS or ECS:&lt;/strong&gt; Accepted by the management team and the architects, but this discussion is only won on the short-term pricing argument. They may have thought that OpenShift on AWS is too expensive, but they forgot the risks and the hidden costs that go along with a DIY solution.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_3_evaluation.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_3_evaluation.jpeg?itok=ZgMY1Ge9" width="600" height="195" alt="An illustration of the evaluation of the dependencies." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The evaluation of the dependencies.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Because not everything was accepted, the development team must go back to the drawing board.&lt;/p&gt; &lt;h2&gt;Preferred dependencies&lt;/h2&gt; &lt;p&gt;The development team, good listeners as they are, take the feedback from the management team and architects into account and start adapting their architecture. Their preferred dependencies are as follows (Figure 4):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;AWS Aurora was accepted. So AWS Aurora stays.&lt;/li&gt; &lt;li aria-level="1"&gt;Management was not too happy with the lack of ROI on their investment in the bare-metal MongoDB cluster. If they don’t want to have repercussions on future requested investments, it’s maybe a good idea to reuse that cluster, which is what they’ll do. AWS DynamoDB will be replaced by the on-premise bare-metal MongoDB cluster.&lt;/li&gt; &lt;li aria-level="1"&gt;Neptune was a no-go. The development team invests some time in reviewing graph databases. The winner in this exercise is Neo4J, off-the-shelf, offered by Google Cloud. AWS Neptune will be replaced by Google Cloud Neo4J.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_4_add_preferred_dependencies.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_4_add_preferred_dependencies.jpeg?itok=qpsizAp5" width="600" height="189" alt="An illustration adding the preferred dependencies to the architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Adding the recommended or preferred dependencies to the architecture.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;All of these changes should go relatively easy because the development team follows the principles of clean architecture. But it does not go easy. EKS or ECS has issues connecting to our on-premise MongoDB cluster and Google Cloud Neo4J (unless they spend quite some time and effort in fixing the network setup, which would even open the door for security flaws, as the development team is less trained in networking and operations).&lt;/p&gt; &lt;p&gt;The development team does not know how to fix the cloud (i.e., core layer) issue regarding the dependencies and asks the architects for input. This will be covered in the next section.&lt;/p&gt; &lt;p&gt;There is an analogy that can be made with &lt;a href="http://developers.redhat.com/topics/java"&gt;Java&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;. Imagine that person service is a Java service and that the on-premise MongoDB is a Python library. If the programming language (like EKS or ECS) is Java, you won’t be able to access the Python libraries from within the person service. If you change the programming language to JVM and then to GraalVM, it would allow Java code to access Python scripts. It then would allow that the movie and movie tracking service are Python scripts (i.e., allow using these libraries) and that they can be called from within a Java service. Changing the programming language from Java to GraalVM, opened the options to a broader spectrum of third-party libraries.&lt;/p&gt; &lt;h2&gt;Hybrid cloud and multicloud solutions to the rescue&lt;/h2&gt; &lt;p&gt;The development team consults the architects with the issue: “We have isolated our dependencies (i.e., cloud services), but have issues with organizing our core layer (i.e., the third-party independent business logic, the microservices).” The architects look to see where the issue originated.&lt;/p&gt; &lt;p&gt;Clean architecture is about keeping your options open. The development team did this, but they narrowed the options too much by limiting their programming language (i.e., the microservice hosting platform) to AWS only. What they really need to make full use of the options available in the cloud and on-premise, is a hybrid cloud and multicloud. With a hybrid cloud, we mean the combination of a part of the cloud located at a vendor and a part of the cloud located on-premise. With a multicloud, we mean that the cloud is located over multiple vendors.&lt;em&gt; &lt;/em&gt;When hybrid cloud and multicloud solutions are enabled as programming language, the development team will be able to use the preferred dependencies: AWS Aurora, on-premise MongoDB, and Google Cloud Neo4J.&lt;/p&gt; &lt;p&gt;This architecture will withstand the test of time as well. Whenever a new SaaS solution becomes available or a new cloud emerges, or new tooling is needed, it will be easy to plug it in the designed cloud architecture (Figure 5). This results in a more resilient platform that requires less maintenance and that fairly easily allows for future innovation.&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_5_hybrid_and_multi_cloud.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_5_hybrid_and_multi_cloud.jpeg?itok=YSx4igPs" width="600" height="201" alt="An illustration of the hybrid cloud and multicloud options." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Hybrid cloud and multicloud.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But how can they implement a hybrid cloud and multicloud solution?&lt;/p&gt; &lt;h2&gt;How to set up hybrid cloud and multicloud solutions&lt;/h2&gt; &lt;p&gt;How do you set up hybrid cloud or multicloud solutions? &lt;a href="www.redhat.com/en/resources/openshift-container-platform-datasheet"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; is the answer. It is a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; platform that has the ability to easily start building hybrid cloud and/or multicloud solutions.&lt;/p&gt; &lt;p&gt;Although we used OpenShift as a solution for the issues that come along with EKS, OpenShift is more than just a Kubernetes installation or implementation. OpenShift Container Platform is a full application container platform. Check with the OpenShift specialists about how to bring the hybrid cloud and multi-cloud service to your company. Get started with a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift Service on AWS (ROSA) trial&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-transitioning-clean-architecture-platform" title="My advice for transitioning to a clean architecture platform"&gt;My advice for transitioning to a clean architecture platform&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maarten Vandeperre, Kevin Dubois</dc:creator><dc:date>2023-04-17T07:01:00Z</dc:date></entry></feed>
